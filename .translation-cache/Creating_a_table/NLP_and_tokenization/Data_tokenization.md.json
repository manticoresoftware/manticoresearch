{
  "8037add88718a5a8190165da7bab2b2b95af5f1d6e53d603574954e28efb2a54": {
    "original": "# Data tokenization\n\nManticore doesn't store text exactly as it is for full-text searching. Instead, it breaks the text into words (called tokens) and builds several internal structures to enable fast full-text searches. These structures include a dictionary that helps quickly check if a word exists in the index. Other structures track which documents and fields contain the word, and even where exactly in the field it appears. These are all used during a search to find relevant results.\n\nThe process of splitting and handling text like this is called **tokenization**. Tokenization happens both when adding data to the index and when running a search. It works at both the character and word level.\n\n### Character-level tokenization\n\nAt the character level, only certain characters are allowed. This is controlled by the [`charset_table`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). Any other characters are replaced with a space (which is treated as a word separator). The `charset_table` also supports things like turning characters into lowercase or replacing one character with another. It can also define characters to be [ignored](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ignore_chars), [blended](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_chars), or treated as a [phrase boundary](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary).\n\n### Word-level tokenization\n\nAt the word level, the engine uses the [`min_word_len`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len) setting to decide the minimum word length (in characters) that should be indexed.\n\nManticore also supports matching words with different forms. For example, to treat \"car\" and \"cars\" as the same word, you can use [morphology processors](../../Creating_a_table/NLP_and_tokenization/Morphology.md#morphology).\n\nIf you want different words to be treated as the same—for example, \"USA\" and \"United States\" — you can define them using the [word forms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md) feature.\n\n### Handling common and noisy words\n\nVery common words (like \"the\", \"and\", \"is\") can slow down searches and increase index size. You can filter them out using [stop words](../../Creating_a_table/NLP_and_tokenization/Ignoring_stop-words.md#stopwords). This can make searches faster and the index smaller.\n\nA more advanced filtering method is [bigrams](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index), which creates special tokens by combining a common word with an uncommon one. This can significantly speed up phrase searches when common words are involved.\n\n### HTML content\n\nIf you're indexing HTML, it's usually best not to include the HTML tags in the index, since they add a lot of unnecessary content. You can use [HTML stripping](../../Creating_a_table/NLP_and_tokenization/Advanced_HTML_tokenization.md#Stripping-HTML-tags) to remove the tags, but still index certain tag attributes or skip specific elements entirely.\n\n### Token length limit\n\nKeep in mind that Manticore has a **maximum token length of 42 characters**. Any word longer than this will be **truncated**. This limit applies during both indexing and searching, so it's important to ensure your data and queries account for it.",
    "translations": {
      "chinese": "# 数据分词\n\nManticore 并不会将文本原样存储用于全文搜索。相反，它会将文本拆分成单词（称为词元）并构建多个内部结构以实现快速全文搜索。这些结构包括一个字典，用于快速检查单词是否存在于索引中。其他结构跟踪包含该单词的文档和字段，甚至精确到该单词在字段中的位置。这些都在搜索时用于找到相关结果。\n\n将文本这样拆分和处理的过程称为**分词**。分词在添加数据到索引和执行搜索时都会发生。它既在字符级别也在单词级别工作。\n\n### 字符级分词\n\n在字符级别，只有某些字符被允许。这由 [`charset_table`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 控制。任何其他字符都会被替换为空格（空格被视为单词分隔符）。`charset_table` 还支持将字符转换为小写或将一个字符替换为另一个字符。它还可以定义被[忽略](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ignore_chars)、[混合](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_chars)或作为[短语边界](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary)处理的字符。\n\n### 单词级分词\n\n在单词级别，引擎使用 [`min_word_len`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len) 设置来决定应索引的最小单词长度（以字符计）。\n\nManticore 还支持匹配不同形式的单词。例如，要将“car”和“cars”视为同一个单词，可以使用[形态学处理器](../../Creating_a_table/NLP_and_tokenization/Morphology.md#morphology)。\n\n如果你希望不同的单词被视为相同——例如，“USA”和“United States”——你可以使用[词形](../../Creating_a_table/NLP_and_tokenization/Wordforms.md)功能来定义它们。\n\n### 处理常见和噪声词\n\n非常常见的词（如“the”、“and”、“is”）会减慢搜索速度并增加索引大小。你可以使用[停用词](../../Creating_a_table/NLP_and_tokenization/Ignoring_stop-words.md#stopwords)将它们过滤掉。这可以使搜索更快，索引更小。\n\n更高级的过滤方法是[二元组](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index)，它通过将常见词与不常见词组合创建特殊词元。当涉及常见词的短语搜索时，这可以显著加快速度。\n\n### HTML 内容\n\n如果你正在索引 HTML，通常最好不要将 HTML 标签包含在索引中，因为它们会添加大量不必要的内容。你可以使用[HTML 去除](../../Creating_a_table/NLP_and_tokenization/Advanced_HTML_tokenization.md#Stripping-HTML-tags)来移除标签，但仍然可以索引某些标签属性或完全跳过特定元素。\n\n### 词元长度限制\n\n请记住，Manticore 有一个**最大词元长度为 42 个字符**的限制。任何超过此长度的单词都会被**截断**。此限制在索引和搜索时都适用，因此确保你的数据和查询考虑到这一点非常重要。",
      "russian": "# Токенизация данных\n\nManticore не хранит текст точно в том виде, в каком он есть, для полнотекстового поиска. Вместо этого он разбивает текст на слова (называемые токенами) и строит несколько внутренних структур для обеспечения быстрого полнотекстового поиска. Эти структуры включают словарь, который помогает быстро проверить, существует ли слово в индексе. Другие структуры отслеживают, в каких документах и полях содержится слово, и даже где именно в поле оно появляется. Все это используется во время поиска для нахождения релевантных результатов.\n\nПроцесс разбиения и обработки текста таким образом называется **токенизацией**. Токенизация происходит как при добавлении данных в индекс, так и при выполнении поиска. Она работает как на уровне символов, так и на уровне слов.\n\n### Токенизация на уровне символов\n\nНа уровне символов разрешены только определённые символы. Это контролируется с помощью [`charset_table`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). Все остальные символы заменяются пробелом (который рассматривается как разделитель слов). `charset_table` также поддерживает такие функции, как преобразование символов в нижний регистр или замена одного символа другим. Она также может определять символы, которые нужно [игнорировать](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ignore_chars), [объединять](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_chars) или рассматривать как [границу фразы](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary).\n\n### Токенизация на уровне слов\n\nНа уровне слов движок использует настройку [`min_word_len`](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len), чтобы определить минимальную длину слова (в символах), которое должно индексироваться.\n\nManticore также поддерживает сопоставление слов с разными формами. Например, чтобы рассматривать \"car\" и \"cars\" как одно и то же слово, можно использовать [морфологические процессоры](../../Creating_a_table/NLP_and_tokenization/Morphology.md#morphology).\n\nЕсли вы хотите, чтобы разные слова рассматривались как одно — например, \"USA\" и \"United States\" — вы можете определить их с помощью функции [форм слов](../../Creating_a_table/NLP_and_tokenization/Wordforms.md).\n\n### Обработка распространённых и шумных слов\n\nОчень распространённые слова (например, \"the\", \"and\", \"is\") могут замедлять поиск и увеличивать размер индекса. Вы можете отфильтровать их с помощью [стоп-слов](../../Creating_a_table/NLP_and_tokenization/Ignoring_stop-words.md#stopwords). Это может сделать поиск быстрее и уменьшить размер индекса.\n\nБолее продвинутый метод фильтрации — это [биграммы](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index), которые создают специальные токены, объединяя распространённое слово с редким. Это может значительно ускорить поиск фраз, когда в них участвуют распространённые слова.\n\n### HTML-контент\n\nЕсли вы индексируете HTML, обычно лучше не включать HTML-теги в индекс, так как они добавляют много ненужного содержимого. Вы можете использовать [удаление HTML-тегов](../../Creating_a_table/NLP_and_tokenization/Advanced_HTML_tokenization.md#Stripping-HTML-tags), чтобы убрать теги, но при этом индексировать определённые атрибуты тегов или полностью пропускать определённые элементы.\n\n### Ограничение длины токена\n\nИмейте в виду, что в Manticore существует **максимальная длина токена — 42 символа**. Любое слово длиннее этого будет **усечено**. Это ограничение действует как при индексировании, так и при поиске, поэтому важно учитывать его при подготовке данных и запросов."
    },
    "is_code_or_comment": false
  }
}
