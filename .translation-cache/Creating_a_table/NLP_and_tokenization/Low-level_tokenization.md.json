{
  "f6128b8ad28ec64f5f175a3dc2f3cdae9eb55eb968e99631c178a9e12c052932": {
    "original": "Regular expressions (regexps) used to filter the fields and queries. This directive is optional, multi-valued, and its default is an empty list of regular expressions. The regular expressions engine used by Manticore Search is Google's RE2, which is known for its speed and safety. For detailed information on the syntax supported by RE2, you can visit the [RE2 syntax guide](https://github.com/google/re2/wiki/Syntax).\n\nIn certain applications such as product search, there can be many ways to refer to a product, model, or property. For example, `iPhone 3gs` and `iPhone 3 gs` (or even `iPhone3 gs`) are very likely to refer to the same product. Another example could be different ways to express a laptop screen size, such as `13-inch`, `13 inch`, `13\"`, or `13in`.\n\nRegexps provide a mechanism to specify rules tailored to handle such cases. In the first example, you could possibly use a wordforms file to handle a handful of iPhone models, but in the second example, it's better to specify rules that would normalize \"13-inch\" and \"13in\" to something identical.\n\nRegular expressions listed in `regexp_filter` are applied in the order they are listed, at the earliest stage possible, before any other processing (including [exceptions](../../Creating_a_table/NLP_and_tokenization/Exceptions.md#exceptions)), even before tokenization. That is, regexps are applied to the raw source fields when indexing, and to the raw search query text when searching.\n\n<!-- request SQL -->\n\nCODE_BLOCK_223\n\n<!-- request JSON -->\n\nCODE_BLOCK_224\n\n<!-- request PHP -->\n\nCODE_BLOCK_225\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_226\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_227\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_228\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_229\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_230\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_231\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_232\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "正则表达式（regexps）用于过滤字段和查询。此指令是可选的，可多值，默认值为空的正则表达式列表。Manticore Search 使用的正则表达式引擎是 Google 的 RE2，以其速度和安全性著称。有关 RE2 支持的语法的详细信息，您可以访问 [RE2 语法指南](https://github.com/google/re2/wiki/Syntax)。\n\n在某些应用中，例如产品搜索，可能有多种方式来指代一个产品、型号或属性。例如，`iPhone 3gs` 和 `iPhone 3 gs`（甚至 `iPhone3 gs`）很可能指的是同一款产品。另一个例子是表达笔记本屏幕尺寸的不同方式，如 `13-inch`、`13 inch`、`13\"` 或 `13in`。\n\n正则表达式提供了一种机制，用于指定处理此类情况的规则。在第一个例子中，您可能会使用一个词形文件来处理少量的 iPhone 型号，但在第二个例子中，更好的是指定规则，将“13-inch”和“13in”规范化为相同的形式。\n\n列在 `regexp_filter` 中的正则表达式按列出顺序应用，在尽可能早的阶段应用，早于任何其他处理（包括[例外](../../Creating_a_table/NLP_and_tokenization/Exceptions.md#exceptions)），甚至早于分词。也就是说，正则表达式应用于索引时的原始源字段，以及搜索时的原始搜索查询文本。\n\n<!-- request SQL -->\n\nCODE_BLOCK_223\n\n<!-- request JSON -->\n\nCODE_BLOCK_224\n\n<!-- request PHP -->\n\nCODE_BLOCK_225\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_226\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_227\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_228\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_229\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_230\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_231\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_232\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "Регулярные выражения (regexps), используемые для фильтрации полей и запросов. Эта директива является необязательной, может принимать несколько значений, и по умолчанию представляет собой пустой список регулярных выражений. Движок регулярных выражений, используемый в Manticore Search, — это RE2 от Google, известный своей скоростью и безопасностью. Для подробной информации о синтаксисе, поддерживаемом RE2, вы можете посетить [руководство по синтаксису RE2](https://github.com/google/re2/wiki/Syntax).\n\nВ некоторых приложениях, таких как поиск товаров, может быть много способов обозначить продукт, модель или свойство. Например, `iPhone 3gs` и `iPhone 3 gs` (или даже `iPhone3 gs`) с большой вероятностью относятся к одному и тому же продукту. Другой пример — различные способы указания размера экрана ноутбука, такие как `13-inch`, `13 inch`, `13\"`, или `13in`.\n\nРегулярные выражения предоставляют механизм для задания правил, адаптированных для обработки таких случаев. В первом примере вы могли бы использовать файл wordforms для обработки нескольких моделей iPhone, но во втором примере лучше задать правила, которые нормализуют \"13-inch\" и \"13in\" к одному и тому же виду.\n\nРегулярные выражения, перечисленные в `regexp_filter`, применяются в том порядке, в котором они указаны, на самом раннем этапе, до любой другой обработки (включая [исключения](../../Creating_a_table/NLP_and_tokenization/Exceptions.md#exceptions)), даже до токенизации. То есть, регулярные выражения применяются к исходным полям при индексации и к исходному тексту поискового запроса при поиске.\n\n<!-- request SQL -->\n\nCODE_BLOCK_223\n\n<!-- request JSON -->\n\nCODE_BLOCK_224\n\n<!-- request PHP -->\n\nCODE_BLOCK_225\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_226\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_227\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_228\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_229\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_230\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_231\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_232\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  },
  "945f492bcc4d7a4877122c268ff1131a38ffe416be57274ccce14fbd092238af": {
    "original": "Some of the bigram indexing modes (see [bigram_index](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index)) require to define a list of frequent keywords. These are **not** to be confused with stop words. Stop words are completely eliminated when both indexing and searching. Frequent keywords are only used by bigrams to determine whether to index a current word pair or not.\n\n`bigram_freq_words` lets you define a list of such keywords.\n\n<!-- request SQL -->\n\nCODE_BLOCK_122\n\n<!-- request JSON -->\n\nCODE_BLOCK_123\n\n<!-- request PHP -->\n\nCODE_BLOCK_124\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_125\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_126\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_127\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_128\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_129\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_130\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_131\n\n<!-- end -->\n\n### dict\n\nCODE_BLOCK_132\n\n<!-- example dict -->\n\nThe type of keywords dictionary used is identified by one of two known values, 'crc' or 'keywords'. This is optional, with 'keywords' as the default.\n\nUsing the keywords dictionary mode (dict=keywords) can significantly decrease the indexing burden and enable substring searches on extensive collections. This mode can be utilized for both plain and RT tables.\n\nCRC dictionaries do not store the original keyword text in the index. Instead, they replace keywords with a control sum value (computed using FNV64) during both searching and indexing processes. This value is used internally within the index. This approach has two disadvantages:\n\n* Firstly, there's a risk of control sum collisions between different keywords pairs. This risk grows in proportion to the number of unique keywords in the index. Nonetheless, this concern is minor as the probability of a single FNV64 collision in a dictionary of 1 billion entries is roughly 1 in 16, or 6.25 percent. Most dictionaries will have far fewer than a billion keywords given that a typical spoken human language has between 1 and 10 million word forms.\n\n* Secondly, and more crucially, it's not straightforward to perform substring searches with control sums. Manticore addressed this issue by pre-indexing all possible substrings as separate keywords (see [min_prefix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_prefix_len), [min_infix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_infix_len) directives). This method even has an added advantage of matching substrings in the fastest way possible. Yet, pre-indexing all substrings significantly increases the index size (often by factors of 3-10x or more) and subsequently affects the indexing time, making substring searches on large indexes rather impractical.\n\nThe keywords dictionary resolves both of these issues. It stores keywords in the index and performs search-time wildcard expansion. For instance, a search for a `test*` prefix could internally expand to a 'test|tests|testing' query based on the dictionary's contents. This expansion process is entirely invisible to the application, with the exception that the separate per-keyword statistics for all the matched keywords are now also reported.\n\nFor substring (infix) searches, extended wildcards can be used. Special characters such as `?` and `%` are compatible with substring (infix) search (e.g., `t?st*`, `run%`, `*abc*`). Note that the [wildcards operators](../../Searching/Full_text_matching/Operators.md#Wildcard-operators) and the [REGEX](../../Searching/Full_text_matching/Operators.md#REGEX-operator) only function with `dict=keywords`.\n\nIndexing with a keywords dictionary is approximately 1.1x to 1.3x slower than regular, non-substring indexing - yet significantly faster than substring indexing (either prefix or infix). The index size should only be slightly larger than that of the standard non-substring table, with a total difference of 1..10% percent. The time it takes for regular keyword searching should be nearly the same or identical across all three index types discussed (CRC non-substring, CRC substring, keywords). Substring searching time can significantly fluctuate based on how many actual keywords match the given substring (i.e., how many keywords the search term expands into). The maximum number of matched keywords is limited by the [expansion_limit](../../Server_settings/Searchd.md#expansion_limit) directive.\n\nIn summary, keywords and CRC dictionaries offer two different trade-off decisions for substring searching. You can opt to either sacrifice indexing time and index size to achieve the fastest worst-case searches (CRC dictionary), or minimally impact indexing time but sacrifice worst-case searching time when the prefix expands into a high number of keywords (keywords dictionary).\n\n<!-- request SQL -->\n\nCODE_BLOCK_133\n\n<!-- request JSON -->\n\nCODE_BLOCK_134\n\n<!-- request PHP -->\n\nCODE_BLOCK_135\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_136\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_137\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_138\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_139\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_140\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_141\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_142\n\n<!-- end -->\n\n### embedded_limit\n\nCODE_BLOCK_143\n\n<!-- example embedded_limit -->\n\nEmbedded exceptions, wordforms, or stop words file size limit. Optional, default is 16K.\n\nWhen you create a table the above mentioned files can be either saved externally along with the table or embedded directly into the table. Files sized under `embedded_limit` get stored into the table. For bigger files, only the file names are stored. This also simplifies moving table files to a different machine; you may get by just copying a single file.",
    "translations": {
      "chinese": "一些二元索引模式（参见 [bigram_index](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index)）需要定义一个频繁关键词列表。这些**不**应与停用词混淆。停用词在索引和搜索时会被完全剔除。频繁关键词仅被二元组用来判断是否索引当前的词对。\n\n`bigram_freq_words` 允许你定义这样一个关键词列表。\n\n<!-- request SQL -->\n\nCODE_BLOCK_122\n\n<!-- request JSON -->\n\nCODE_BLOCK_123\n\n<!-- request PHP -->\n\nCODE_BLOCK_124\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_125\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_126\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_127\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_128\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_129\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_130\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_131\n\n<!-- end -->\n\n### dict\n\nCODE_BLOCK_132\n\n<!-- example dict -->\n\n所使用的关键词字典类型由两个已知值之一标识，'crc' 或 'keywords'。这是可选的，默认值为 'keywords'。\n\n使用关键词字典模式（dict=keywords）可以显著减少索引负担，并在大规模集合上启用子串搜索。此模式可用于普通表和 RT 表。\n\nCRC 字典不会在索引中存储原始关键词文本。相反，它们在搜索和索引过程中用一个控制和校验值（使用 FNV64 计算）替代关键词。该值在索引内部使用。这种方法有两个缺点：\n\n* 首先，不同关键词对之间存在控制和校验值冲突的风险。该风险随着索引中唯一关键词数量的增加而增加。尽管如此，这个问题较小，因为在一个包含 10 亿条目的字典中，单个 FNV64 冲突的概率大约是 1/16，即 6.25%。大多数字典的关键词数量远少于 10 亿，因为典型的口语人类语言词形数量在 1 到 1000 万之间。\n\n* 其次，更重要的是，使用控制和校验值进行子串搜索并不简单。Manticore 通过预先索引所有可能的子串作为独立关键词来解决此问题（参见 [min_prefix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_prefix_len)，[min_infix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_infix_len) 指令）。这种方法还有一个额外优点，即以最快的方式匹配子串。然而，预索引所有子串会显著增加索引大小（通常增加 3-10 倍或更多），并随后影响索引时间，使得在大型索引上进行子串搜索变得不太实用。\n\n关键词字典解决了这两个问题。它在索引中存储关键词，并在搜索时执行通配符扩展。例如，搜索 `test*` 前缀时，内部可能扩展为基于字典内容的 'test|tests|testing' 查询。此扩展过程对应用程序完全透明，唯一的区别是所有匹配关键词的单独统计数据也会被报告。\n\n对于子串（中缀）搜索，可以使用扩展通配符。特殊字符如 `?` 和 `%` 与子串（中缀）搜索兼容（例如，`t?st*`，`run%`，`*abc*`）。请注意，[通配符操作符](../../Searching/Full_text_matching/Operators.md#Wildcard-operators) 和 [REGEX](../../Searching/Full_text_matching/Operators.md#REGEX-operator) 仅在 `dict=keywords` 时有效。\n\n使用关键词字典索引的速度大约比常规非子串索引慢 1.1 到 1.3 倍，但明显快于子串索引（无论是前缀还是中缀）。索引大小应仅比标准非子串表略大，总差异在 1% 到 10% 之间。常规关键词搜索的时间在这三种索引类型（CRC 非子串、CRC 子串、关键词）中应几乎相同或相等。子串搜索时间会根据匹配给定子串的实际关键词数量（即搜索词扩展成多少关键词）显著波动。匹配关键词的最大数量受 [expansion_limit](../../Server_settings/Searchd.md#expansion_limit) 指令限制。\n\n总之，关键词和 CRC 字典为子串搜索提供了两种不同的权衡选择。你可以选择牺牲索引时间和索引大小以获得最快的最坏情况搜索（CRC 字典），或者最小化索引时间影响，但在前缀扩展成大量关键词时牺牲最坏情况搜索时间（关键词字典）。\n\n<!-- request SQL -->\n\nCODE_BLOCK_133\n\n<!-- request JSON -->\n\nCODE_BLOCK_134\n\n<!-- request PHP -->\n\nCODE_BLOCK_135\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_136\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_137\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_138\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_139\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_140\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_141\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_142\n\n<!-- end -->\n\n### embedded_limit\n\nCODE_BLOCK_143\n\n<!-- example embedded_limit -->\n\n嵌入式例外、词形或停用词文件大小限制。可选，默认值为 16K。\n\n当你创建表时，上述文件可以与表一起外部保存，或者直接嵌入到表中。大小低于 `embedded_limit` 的文件会存储到表中。对于较大的文件，仅存储文件名。这也简化了将表文件移动到另一台机器的过程；你可能只需复制一个文件即可。",
      "russian": "Некоторые режимы индексации биграмм (см. [bigram_index](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index)) требуют определения списка часто встречающихся ключевых слов. Их **не следует** путать со стоп-словами. Стоп-слова полностью исключаются как при индексации, так и при поиске. Часто встречающиеся ключевые слова используются биграммами только для определения, индексировать ли текущую пару слов или нет.\n\n`bigram_freq_words` позволяет определить такой список ключевых слов.\n\n<!-- request SQL -->\n\nCODE_BLOCK_122\n\n<!-- request JSON -->\n\nCODE_BLOCK_123\n\n<!-- request PHP -->\n\nCODE_BLOCK_124\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_125\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_126\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_127\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_128\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_129\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_130\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_131\n\n<!-- end -->\n\n### dict\n\nCODE_BLOCK_132\n\n<!-- example dict -->\n\nТип словаря ключевых слов определяется одним из двух известных значений: 'crc' или 'keywords'. Это необязательно, по умолчанию используется 'keywords'.\n\nИспользование режима словаря ключевых слов (dict=keywords) может значительно снизить нагрузку на индексацию и обеспечить поиск подстрок в больших коллекциях. Этот режим можно использовать как для обычных, так и для RT таблиц.\n\nCRC-словари не хранят исходный текст ключевого слова в индексе. Вместо этого они заменяют ключевые слова контрольной суммой (вычисляемой с помощью FNV64) как при поиске, так и при индексации. Это значение используется внутри индекса. Такой подход имеет два недостатка:\n\n* Во-первых, существует риск коллизий контрольных сумм между разными парами ключевых слов. Этот риск растет пропорционально количеству уникальных ключевых слов в индексе. Тем не менее, это незначительная проблема, так как вероятность одной коллизии FNV64 в словаре из 1 миллиарда записей примерно 1 к 16, или 6,25 процента. Большинство словарей будут содержать гораздо меньше миллиарда ключевых слов, учитывая, что типичный разговорный язык содержит от 1 до 10 миллионов словоформ.\n\n* Во-вторых, и что более важно, с контрольными суммами сложно выполнять поиск подстрок. Manticore решил эту проблему путем предварительной индексации всех возможных подстрок как отдельных ключевых слов (см. директивы [min_prefix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_prefix_len), [min_infix_len](../../Creating_a_table/NLP_and_tokenization/Wildcard_searching_settings.md#min_infix_len)). Этот метод даже имеет дополнительное преимущество — максимально быстрый поиск подстрок. Однако предварительная индексация всех подстрок значительно увеличивает размер индекса (часто в 3-10 раз и более) и, соответственно, время индексации, что делает поиск подстрок в больших индексах довольно непрактичным.\n\nСловарь ключевых слов решает обе эти проблемы. Он хранит ключевые слова в индексе и выполняет расширение подстановочных знаков во время поиска. Например, поиск по префиксу `test*` может внутренне расширяться в запрос 'test|tests|testing' на основе содержимого словаря. Этот процесс расширения полностью прозрачен для приложения, за исключением того, что теперь также предоставляется отдельная статистика по каждому совпавшему ключевому слову.\n\nДля поиска подстрок (инфиксного поиска) можно использовать расширенные подстановочные знаки. Специальные символы, такие как `?` и `%`, совместимы с поиском подстрок (например, `t?st*`, `run%`, `*abc*`). Обратите внимание, что [операторы подстановочных знаков](../../Searching/Full_text_matching/Operators.md#Wildcard-operators) и [оператор REGEX](../../Searching/Full_text_matching/Operators.md#REGEX-operator) работают только с `dict=keywords`.\n\nИндексация с использованием словаря ключевых слов примерно в 1.1-1.3 раза медленнее, чем обычная индексация без подстрок, но значительно быстрее, чем индексация подстрок (префиксная или инфиксная). Размер индекса должен быть лишь немного больше, чем у стандартной таблицы без подстрок, с общей разницей в 1..10%. Время обычного поиска по ключевым словам должно быть почти одинаковым или идентичным для всех трех типов индексов (CRC без подстрок, CRC с подстроками, keywords). Время поиска подстрок может значительно варьироваться в зависимости от того, сколько ключевых слов соответствует заданной подстроке (то есть во сколько ключевых слов расширяется поисковый термин). Максимальное количество совпавших ключевых слов ограничено директивой [expansion_limit](../../Server_settings/Searchd.md#expansion_limit).\n\nВ итоге, словари keywords и CRC предлагают два разных компромисса для поиска подстрок. Вы можете либо пожертвовать временем индексации и размером индекса ради максимально быстрого худшего случая поиска (CRC словарь), либо минимально повлиять на время индексации, но пожертвовать временем худшего случая поиска, когда префикс расширяется в большое количество ключевых слов (keywords словарь).\n\n<!-- request SQL -->\n\nCODE_BLOCK_133\n\n<!-- request JSON -->\n\nCODE_BLOCK_134\n\n<!-- request PHP -->\n\nCODE_BLOCK_135\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_136\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_137\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_138\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_139\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_140\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_141\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_142\n\n<!-- end -->\n\n### embedded_limit\n\nCODE_BLOCK_143\n\n<!-- example embedded_limit -->\n\nЛимит размера встроенных исключений, словоформ или стоп-слов. Необязательно, по умолчанию 16K.\n\nПри создании таблицы вышеуказанные файлы могут быть либо сохранены внешне вместе с таблицей, либо встроены непосредственно в таблицу. Файлы размером меньше `embedded_limit` сохраняются в таблице. Для больших файлов сохраняются только имена файлов. Это также упрощает перенос файлов таблицы на другой компьютер; зачастую достаточно просто скопировать один файл."
    },
    "is_code_or_comment": false
  },
  "2970b6b875d43371d727a8f48073dcc32b7952876358c02a1ef0caeab236f762": {
    "original": "With smaller files, such embedding reduces the number of the external files on which the table depends, and helps maintenance. But at the same time it makes no sense to embed a 100 MB wordforms dictionary into a tiny delta table. So there needs to be a size threshold, and `embedded_limit` is that threshold.\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_144\n\n<!-- end -->\n\n### global_idf\n\nCODE_BLOCK_145\n\n<!-- example global_idf -->\n\nThe path to a file with global (cluster-wide) keyword IDFs. Optional, default is empty (use local IDFs).\n\nOn a multi-table cluster, per-keyword frequencies are quite likely to differ across different tables. That means that when the ranking function uses TF-IDF based values, such as BM25 family of factors, the results might be ranked slightly differently depending on what cluster node they reside.\n\nThe easiest way to fix that issue is to create and utilize a global frequency dictionary, or a global IDF file for short. This directive lets you specify the location of that file. It is suggested (but not required) to use an .idf extension. When the IDF file is specified for a given table *and* [OPTION global_idf](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#global_idf) is set to 1, the engine will use the keyword frequencies and collection documents counts from the global_idf file, rather than just the local table. That way, IDFs and the values that depend on them will stay consistent across the cluster.\n\nIDF files can be shared across multiple tables. Only a single copy of an IDF file will be loaded by `searchd`, even when many tables refer to that file. Should the contents of an IDF file change, the new contents can be loaded with a SIGHUP.\n\nYou can build an .idf file using [indextool](../../Miscellaneous_tools.md#indextool) utility, by dumping dictionaries using `--dumpdict dict.txt --stats` switch first, then converting those to .idf format using `--buildidf`, then merging all the .idf files across cluster using `--mergeidf`.\n\n<!-- request SQL -->\n\nCODE_BLOCK_146\n\n<!-- request JSON -->\n\nCODE_BLOCK_147\n\n<!-- request PHP -->\n\nCODE_BLOCK_148\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_149\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_150\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_151\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_152\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_153\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_154\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_155\n\n<!-- end -->\n\n### hitless_words\n\nCODE_BLOCK_156\n\n<!-- example hitless_words -->\n\nHitless words list. Optional, allowed values are 'all', or a list file name.\n\nBy default, Manticore full-text index stores not only a list of matching documents for every given keyword, but also a list of its in-document positions (known as hitlist). Hitlists enables phrase, proximity, strict order and other advanced types of searching, as well as phrase proximity ranking. However, hitlists for specific frequent keywords (that can not be stopped for some reason despite being frequent) can get huge and thus slow to process while querying. Also, in some cases we might only care about boolean keyword matching, and never need position-based searching operators (such as phrase matching) nor phrase ranking.\n\n`hitless_words` lets you create indexes that either do not have positional information (hitlists) at all, or skip it for specific keywords.\n\nHitless index will generally use less space than the respective regular full-text index (about 1.5x can be expected). Both indexing and searching should be faster, at a cost of missing positional query and ranking support.\n\nIf used in positional queries (e.g. phrase queries) the hitless words are taken out from them and used as operand without a position.  For example if \"hello\" and \"world\" are hitless and \"simon\" and \"says\" are not hitless, the phrase query  `\"simon says hello world\"` will be converted to `(\"simon says\" & hello & world)`, matching \"hello\" and \"world\" anywhere in the document and \"simon says\" as an exact phrase.\n\nA positional query than contains only hitless words will result in an empty phrase node, therefore the entire query will return an empty result and a warning. If the whole dictionary is hitless (using `all`) only boolean matching can be used on the respective index.\n\n<!-- request SQL -->\n\nCODE_BLOCK_157\n\n<!-- request JSON -->\n\nCODE_BLOCK_158\n\n<!-- request PHP -->\n\nCODE_BLOCK_159\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_160\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_161\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_162\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_163\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_164\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_165\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_166\n\n<!-- end -->\n\n### index_field_lengths\n\nCODE_BLOCK_167\n\n<!-- example index_field_lengths -->\n\nEnables computing and storing of field lengths (both per-document and average per-index values) into the full-text index. Optional, default is 0 (do not compute and store).\n\nWhen `index_field_lengths` is set to 1 Manticore will:\n\n* create a respective length attribute for every full-text field, sharing the same name but with `__len` suffix\n\n* compute a field length (counted in keywords) for every document and store in to a respective attribute\n\n* compute the per-index averages. The lengths attributes will have a special TOKENCOUNT type, but their values are in fact regular 32-bit integers, and their values are generally accessible.",
    "translations": {
      "chinese": "对于较小的文件，这种嵌入减少了表所依赖的外部文件数量，有助于维护。但同时，将一个100 MB的词形字典嵌入到一个微小的增量表中是没有意义的。因此需要有一个大小阈值，而`embedded_limit`就是这个阈值。\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_144\n\n<!-- end -->\n\n### global_idf\n\nCODE_BLOCK_145\n\n<!-- example global_idf -->\n\n全局（集群范围）关键词IDF文件的路径。可选，默认为空（使用本地IDF）。\n\n在多表集群中，每个关键词的频率很可能在不同的表之间有所不同。这意味着当排名函数使用基于TF-IDF的值（如BM25系列因子）时，结果可能会根据它们所在的集群节点而略有不同。\n\n解决该问题的最简单方法是创建并使用全局频率字典，简称全局IDF文件。该指令允许您指定该文件的位置。建议（但不强制）使用.idf扩展名。当为给定表指定了IDF文件*且* [OPTION global_idf](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#global_idf) 设置为1时，引擎将使用global_idf文件中的关键词频率和集合文档计数，而不仅仅是本地表。这样，IDF及其依赖的值将在整个集群中保持一致。\n\nIDF文件可以被多个表共享。即使许多表引用该文件，`searchd`也只会加载IDF文件的单个副本。如果IDF文件的内容发生变化，可以通过SIGHUP加载新内容。\n\n您可以使用[indextool](../../Miscellaneous_tools.md#indextool)工具构建.idf文件，先使用`--dumpdict dict.txt --stats`开关导出字典，然后使用`--buildidf`将其转换为.idf格式，最后使用`--mergeidf`合并整个集群的所有.idf文件。\n\n<!-- request SQL -->\n\nCODE_BLOCK_146\n\n<!-- request JSON -->\n\nCODE_BLOCK_147\n\n<!-- request PHP -->\n\nCODE_BLOCK_148\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_149\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_150\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_151\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_152\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_153\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_154\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_155\n\n<!-- end -->\n\n### hitless_words\n\nCODE_BLOCK_156\n\n<!-- example hitless_words -->\n\n无命中词列表。可选，允许的值为'all'，或一个列表文件名。\n\n默认情况下，Manticore全文索引不仅存储每个关键词匹配的文档列表，还存储其在文档中的位置列表（称为hitlist）。Hitlist支持短语、邻近、严格顺序及其他高级搜索类型，以及短语邻近排名。然而，对于某些频繁关键词（尽管频繁但无法被停止词过滤），hitlist可能非常庞大，导致查询时处理缓慢。此外，在某些情况下，我们可能只关心布尔关键词匹配，根本不需要基于位置的搜索操作符（如短语匹配）或短语排名。\n\n`hitless_words`允许您创建完全没有位置信息（hitlist）的索引，或对特定关键词跳过位置信息。\n\n无命中索引通常比相应的常规全文索引占用更少空间（大约可节省1.5倍）。索引和搜索速度都应更快，但代价是缺少位置查询和排名支持。\n\n如果在位置查询（例如短语查询）中使用，无命中词会被从查询中剔除，并作为无位置的操作数使用。例如，如果“hello”和“world”是无命中词，而“simon”和“says”不是无命中词，则短语查询 `\"simon says hello world\"` 会被转换为 `(\"simon says\" & hello & world)`，匹配文档中任意位置的“hello”和“world”，以及作为精确短语的“simon says”。\n\n仅包含无命中词的位置信息查询将导致空短语节点，因此整个查询将返回空结果并发出警告。如果整个字典都是无命中词（使用`all`），则只能在相应索引上使用布尔匹配。\n\n<!-- request SQL -->\n\nCODE_BLOCK_157\n\n<!-- request JSON -->\n\nCODE_BLOCK_158\n\n<!-- request PHP -->\n\nCODE_BLOCK_159\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_160\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_161\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_162\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_163\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_164\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_165\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_166\n\n<!-- end -->\n\n### index_field_lengths\n\nCODE_BLOCK_167\n\n<!-- example index_field_lengths -->\n\n启用计算和存储字段长度（包括每文档和每索引的平均值）到全文索引中。可选，默认值为0（不计算和存储）。\n\n当`index_field_lengths`设置为1时，Manticore将：\n\n* 为每个全文字段创建相应的长度属性，名称相同但带有`__len`后缀\n\n* 计算每个文档的字段长度（以关键词计数）并存储到相应属性中\n\n* 计算每个索引的平均值。长度属性将具有特殊的TOKENCOUNT类型，但其值实际上是常规的32位整数，且其值通常是可访问的。",
      "russian": "С меньшими файлами такое встраивание уменьшает количество внешних файлов, от которых зависит таблица, и облегчает обслуживание. Но в то же время нет смысла встраивать словарь wordforms размером 100 МБ в крошечную дельта-таблицу. Поэтому необходим порог по размеру, и `embedded_limit` — это тот самый порог.\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_144\n\n<!-- end -->\n\n### global_idf\n\nCODE_BLOCK_145\n\n<!-- example global_idf -->\n\nПуть к файлу с глобальными (по всему кластеру) IDF ключевых слов. Необязательно, по умолчанию пусто (использовать локальные IDF).\n\nВ многотабличном кластере частоты ключевых слов, скорее всего, будут отличаться в разных таблицах. Это означает, что когда функция ранжирования использует значения на основе TF-IDF, такие как факторы семейства BM25, результаты могут ранжироваться немного по-разному в зависимости от того, на каком узле кластера они находятся.\n\nСамый простой способ исправить эту проблему — создать и использовать глобальный словарь частот, или сокращённо глобальный IDF-файл. Эта директива позволяет указать расположение этого файла. Рекомендуется (но не обязательно) использовать расширение .idf. Когда для данной таблицы указан IDF-файл *и* [OPTION global_idf](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#global_idf) установлен в 1, движок будет использовать частоты ключевых слов и количество документов коллекции из файла global_idf, а не только из локальной таблицы. Таким образом, IDF и значения, зависящие от них, будут оставаться согласованными по всему кластеру.\n\nIDF-файлы могут использоваться несколькими таблицами. Только одна копия IDF-файла будет загружена `searchd`, даже если многие таблицы ссылаются на этот файл. Если содержимое IDF-файла изменится, новые данные можно загрузить с помощью SIGHUP.\n\nВы можете создать .idf файл с помощью утилиты [indextool](../../Miscellaneous_tools.md#indextool), сначала выгрузив словари с помощью переключателя `--dumpdict dict.txt --stats`, затем преобразовав их в формат .idf с помощью `--buildidf`, а затем объединив все .idf файлы по всему кластеру с помощью `--mergeidf`.\n\n<!-- request SQL -->\n\nCODE_BLOCK_146\n\n<!-- request JSON -->\n\nCODE_BLOCK_147\n\n<!-- request PHP -->\n\nCODE_BLOCK_148\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_149\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_150\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_151\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_152\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_153\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_154\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_155\n\n<!-- end -->\n\n### hitless_words\n\nCODE_BLOCK_156\n\n<!-- example hitless_words -->\n\nСписок слов без позиций (hitless words). Необязательно, допустимые значения — 'all' или имя файла со списком.\n\nПо умолчанию полнотекстовый индекс Manticore хранит не только список документов, соответствующих каждому ключевому слову, но и список его позиций в документе (известный как hitlist). Hitlist позволяет выполнять поиск по фразам, близости, строгому порядку и другие продвинутые типы поиска, а также ранжирование по близости фраз. Однако hitlist для некоторых часто встречающихся ключевых слов (которые по каким-то причинам нельзя исключить, несмотря на их частоту) может стать очень большим и, следовательно, замедлять обработку запросов. Кроме того, в некоторых случаях нам может быть важен только булевый поиск по ключевым словам, и позиционные операторы поиска (например, поиск по фразам) и ранжирование по фразам не нужны.\n\n`hitless_words` позволяет создавать индексы, которые либо вообще не содержат позиционной информации (hitlist), либо пропускают её для определённых ключевых слов.\n\nИндекс без позиций обычно занимает меньше места, чем соответствующий обычный полнотекстовый индекс (ожидается примерно в 1.5 раза меньше). Индексация и поиск должны быть быстрее, но при этом отсутствует поддержка позиционных запросов и ранжирования.\n\nЕсли такие слова используются в позиционных запросах (например, в запросах по фразам), то слова без позиций исключаются из них и используются как операнды без позиции. Например, если \"hello\" и \"world\" — слова без позиций, а \"simon\" и \"says\" — с позициями, то фразовый запрос `\"simon says hello world\"` будет преобразован в `(\"simon says\" & hello & world)`, что означает поиск \"hello\" и \"world\" в любом месте документа и точной фразы \"simon says\".\n\nПозиционный запрос, содержащий только слова без позиций, приведёт к пустому узлу фразы, поэтому весь запрос вернёт пустой результат и предупреждение. Если весь словарь без позиций (используется `all`), то на соответствующем индексе можно использовать только булевый поиск.\n\n<!-- request SQL -->\n\nCODE_BLOCK_157\n\n<!-- request JSON -->\n\nCODE_BLOCK_158\n\n<!-- request PHP -->\n\nCODE_BLOCK_159\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_160\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_161\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_162\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_163\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_164\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_165\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_166\n\n<!-- end -->\n\n### index_field_lengths\n\nCODE_BLOCK_167\n\n<!-- example index_field_lengths -->\n\nВключает вычисление и сохранение длин полей (как для каждого документа, так и средних значений по индексу) в полнотекстовом индексе. Необязательно, по умолчанию 0 (не вычислять и не сохранять).\n\nКогда `index_field_lengths` установлен в 1, Manticore будет:\n\n* создавать соответствующий атрибут длины для каждого полнотекстового поля с тем же именем, но с суффиксом `__len`\n\n* вычислять длину поля (в количестве ключевых слов) для каждого документа и сохранять в соответствующий атрибут\n\n* вычислять средние значения по индексу. Атрибуты длины будут иметь специальный тип TOKENCOUNT, но их значения на самом деле являются обычными 32-битными целыми числами, и к ним обычно можно получить доступ."
    },
    "is_code_or_comment": false
  },
  "01ab3c14d1fb2bac87adf547c87a430bff437a5e2beb37dfcc2a08ce23021ae0": {
    "original": "CODE_BLOCK_77\n\n<!-- end -->\n\n### ngram_chars\n\nCODE_BLOCK_78\n\n<!-- example ngram_chars -->\n\nN-gram characters list. Optional, default is empty.\n\nTo be used in conjunction with in [ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len), this list defines characters, sequences of which are subject to N-gram extraction. Words comprised of other characters will not be affected by N-gram indexing feature. The value format is identical to [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). N-gram characters cannot appear in the [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table).\n\n<!-- request SQL -->\n\nCODE_BLOCK_79\n\n<!-- request JSON -->\n\nCODE_BLOCK_80\n\n<!-- request PHP -->\n\nCODE_BLOCK_81\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_82\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_83\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_84\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_85\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_86\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_87\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_88\n\n<!-- end -->\n\n<!-- example ngram_chars 2 -->\n\nAlso you can use an alias for our default N-gram table as in the example. It should be sufficient in most cases.\n\n<!-- request SQL -->\n\nCODE_BLOCK_89\n\n<!-- request JSON -->\n\nCODE_BLOCK_90\n\n<!-- request PHP -->\n\nCODE_BLOCK_91\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_92\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_93\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_94\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_95\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_96\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_97\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_98\n\n<!-- end -->\n\n### ignore_chars\n\nCODE_BLOCK_99\n\n<!-- example ignore_chars -->\n\nIgnored characters list. Optional, default is empty.\n\nUseful in cases when some characters, such as soft hyphenation mark (U+00AD), should be not just treated as separators but rather fully ignored. For example, if '-' is simply not in the charset_table, \"abc-def\" text will be indexed as \"abc\" and \"def\" keywords. On the contrary, if '-' is added to ignore_chars list, the same text will be indexed as a single \"abcdef\" keyword.\n\nThe syntax is the same as for [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table), but it's only allowed to declare characters, and not allowed to map them. Also, the ignored characters must not be present in charset_table.\n\n<!-- request SQL -->\n\nCODE_BLOCK_100\n\n<!-- request JSON -->\n\nCODE_BLOCK_101\n\n<!-- request PHP -->\n\nCODE_BLOCK_102\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_103\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_104\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_105\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_106\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_107\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_108\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_109\n\n<!-- end -->\n\n### bigram_index\n\nCODE_BLOCK_110\n\n<!-- example bigram_index -->\n\nBigram indexing mode. Optional, default is none.\n\nBigram indexing is a feature to accelerate phrase searches. When indexing, it stores a document list for either all or some of the adjacent words pairs into the index. Such a list can then be used at searching time to significantly accelerate phrase or sub-phrase matching.\n\n`bigram_index` controls the selection of specific word pairs. The known modes are:\n\n* `all`, index every single word pair\n\n* `first_freq`, only index word pairs where the *first* word is in a list of frequent words (see [bigram_freq_words](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_freq_words)). For example, with `bigram_freq_words = the, in, i, a`, indexing \"alone in the dark\" text will result in \"in the\" and \"the dark\" pairs being stored as bigrams, because they begin with a frequent keyword (either \"in\" or \"the\" respectively), but \"alone in\" would **not** be indexed, because \"in\" is a *second* word in that pair.\n\n* `both_freq`, only index word pairs where both words are frequent. Continuing with the same example, in this mode indexing \"alone in the dark\" would only store \"in the\" (the very worst of them all from searching perspective) as a bigram, but none of the other word pairs.\n\nFor most use cases, `both_freq` would be the best mode, but your mileage may vary.\n\nIt's important to note that `bigram_index` works only at the tokenization level and doesn't account for transformations like `morphology`, `wordforms` or `stopwords`. This means the tokens it creates are very straightforward, which makes searching phrases more exact and strict. While this can improve the accuracy of phrase matching, it also makes the system less able to recognize different forms of words or variations in how words appear.\n\n<!-- request SQL -->\n\nCODE_BLOCK_111\n\n<!-- request JSON -->\n\nCODE_BLOCK_112\n\n<!-- request PHP -->\n\nCODE_BLOCK_113\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_114\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_115\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_116\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_117\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_118\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_119\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_120\n\n<!-- end -->\n\n### bigram_freq_words\n\nCODE_BLOCK_121\n\n<!-- example bigram_freq_words -->\n\nA list of keywords considered \"frequent\" when indexing bigrams. Optional, default is empty.",
    "translations": {
      "chinese": "CODE_BLOCK_77\n\n<!-- end -->\n\n### ngram_chars\n\nCODE_BLOCK_78\n\n<!-- example ngram_chars -->\n\nN-gram 字符列表。可选，默认为空。\n\n与 [ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len) 配合使用时，此列表定义了哪些字符序列会被用于 N-gram 提取。由其他字符组成的单词不会受到 N-gram 索引功能的影响。值的格式与 [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 相同。N-gram 字符不能出现在 [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 中。\n\n<!-- request SQL -->\n\nCODE_BLOCK_79\n\n<!-- request JSON -->\n\nCODE_BLOCK_80\n\n<!-- request PHP -->\n\nCODE_BLOCK_81\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_82\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_83\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_84\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_85\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_86\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_87\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_88\n\n<!-- end -->\n\n<!-- example ngram_chars 2 -->\n\n你也可以像示例中那样使用我们默认的 N-gram 表的别名。在大多数情况下，这应该足够了。\n\n<!-- request SQL -->\n\nCODE_BLOCK_89\n\n<!-- request JSON -->\n\nCODE_BLOCK_90\n\n<!-- request PHP -->\n\nCODE_BLOCK_91\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_92\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_93\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_94\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_95\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_96\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_97\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_98\n\n<!-- end -->\n\n### ignore_chars\n\nCODE_BLOCK_99\n\n<!-- example ignore_chars -->\n\n忽略字符列表。可选，默认为空。\n\n当某些字符（如软连字符 U+00AD）不仅应被视为分隔符，而应完全忽略时，此功能非常有用。例如，如果 '-' 不在 charset_table 中，文本 \"abc-def\" 将被索引为 \"abc\" 和 \"def\" 两个关键词。相反，如果 '-' 被添加到 ignore_chars 列表中，同样的文本将被索引为单个关键词 \"abcdef\"。\n\n语法与 [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 相同，但只允许声明字符，不允许映射它们。此外，被忽略的字符不能出现在 charset_table 中。\n\n<!-- request SQL -->\n\nCODE_BLOCK_100\n\n<!-- request JSON -->\n\nCODE_BLOCK_101\n\n<!-- request PHP -->\n\nCODE_BLOCK_102\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_103\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_104\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_105\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_106\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_107\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_108\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_109\n\n<!-- end -->\n\n### bigram_index\n\nCODE_BLOCK_110\n\n<!-- example bigram_index -->\n\n二元组索引模式。可选，默认无。\n\n二元组索引是一种加速短语搜索的功能。索引时，它会将所有或部分相邻词对的文档列表存储到索引中。该列表随后可在搜索时用来显著加快短语或子短语匹配。\n\n`bigram_index` 控制特定词对的选择。已知的模式有：\n\n* `all`，索引每一个词对\n\n* `first_freq`，仅索引第一个词是频繁词列表中的词的词对（见 [bigram_freq_words](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_freq_words)）。例如，设置 `bigram_freq_words = the, in, i, a` 时，索引文本 \"alone in the dark\" 会存储 \"in the\" 和 \"the dark\" 这两个二元组，因为它们以频繁词（\"in\" 或 \"the\"）开头，但 \"alone in\" 不会被索引，因为 \"in\" 是该词对的第二个词。\n\n* `both_freq`，仅索引两个词都是频繁词的词对。以同样的例子，使用此模式索引 \"alone in the dark\" 只会存储 \"in the\"（从搜索角度看是最差的情况），其他词对不会被存储。\n\n对于大多数用例，`both_freq` 是最佳模式，但具体情况可能有所不同。\n\n需要注意的是，`bigram_index` 仅在分词层面工作，不考虑诸如 `morphology`、`wordforms` 或 `stopwords` 等转换。这意味着它创建的词元非常直接，使短语搜索更精确和严格。虽然这可以提高短语匹配的准确性，但也降低了系统识别词形变化或词语变体的能力。\n\n<!-- request SQL -->\n\nCODE_BLOCK_111\n\n<!-- request JSON -->\n\nCODE_BLOCK_112\n\n<!-- request PHP -->\n\nCODE_BLOCK_113\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_114\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_115\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_116\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_117\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_118\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_119\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_120\n\n<!-- end -->\n\n### bigram_freq_words\n\nCODE_BLOCK_121\n\n<!-- example bigram_freq_words -->\n\n在索引二元组时被视为“频繁”的关键词列表。可选，默认为空。",
      "russian": "CODE_BLOCK_77\n\n<!-- end -->\n\n### ngram_chars\n\nCODE_BLOCK_78\n\n<!-- example ngram_chars -->\n\nСписок символов для N-грамм. Необязательно, по умолчанию пустой.\n\nИспользуется вместе с параметром [ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len), этот список определяет символы, последовательности которых подлежат извлечению N-грамм. Слова, состоящие из других символов, не будут затронуты функцией индексации N-грамм. Формат значения идентичен [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). Символы N-грамм не могут присутствовать в [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table).\n\n<!-- request SQL -->\n\nCODE_BLOCK_79\n\n<!-- request JSON -->\n\nCODE_BLOCK_80\n\n<!-- request PHP -->\n\nCODE_BLOCK_81\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_82\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_83\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_84\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_85\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_86\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_87\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_88\n\n<!-- end -->\n\n<!-- example ngram_chars 2 -->\n\nТакже вы можете использовать псевдоним для нашей стандартной таблицы N-грамм, как в примере. Это будет достаточно в большинстве случаев.\n\n<!-- request SQL -->\n\nCODE_BLOCK_89\n\n<!-- request JSON -->\n\nCODE_BLOCK_90\n\n<!-- request PHP -->\n\nCODE_BLOCK_91\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_92\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_93\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_94\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_95\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_96\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_97\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_98\n\n<!-- end -->\n\n### ignore_chars\n\nCODE_BLOCK_99\n\n<!-- example ignore_chars -->\n\nСписок игнорируемых символов. Необязательно, по умолчанию пустой.\n\nПолезно в случаях, когда некоторые символы, такие как мягкий перенос (U+00AD), должны не просто рассматриваться как разделители, а полностью игнорироваться. Например, если '-' просто отсутствует в charset_table, текст \"abc-def\" будет индексироваться как ключевые слова \"abc\" и \"def\". Напротив, если '-' добавлен в список ignore_chars, тот же текст будет индексироваться как одно ключевое слово \"abcdef\".\n\nСинтаксис такой же, как для [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table), но разрешено объявлять только символы, без их отображения. Также игнорируемые символы не должны присутствовать в charset_table.\n\n<!-- request SQL -->\n\nCODE_BLOCK_100\n\n<!-- request JSON -->\n\nCODE_BLOCK_101\n\n<!-- request PHP -->\n\nCODE_BLOCK_102\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_103\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_104\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_105\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_106\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_107\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_108\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_109\n\n<!-- end -->\n\n### bigram_index\n\nCODE_BLOCK_110\n\n<!-- example bigram_index -->\n\nРежим индексации биграмм. Необязательно, по умолчанию отсутствует.\n\nИндексация биграмм — это функция для ускорения поиска фраз. При индексации в индекс сохраняется список документов для всех или некоторых пар соседних слов. Этот список затем может использоваться во время поиска для значительного ускорения сопоставления фраз или подфраз.\n\n`bigram_index` управляет выбором конкретных пар слов. Известные режимы:\n\n* `all` — индексировать каждую пару слов\n\n* `first_freq` — индексировать только пары слов, где *первое* слово находится в списке частотных слов (см. [bigram_freq_words](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_freq_words)). Например, при `bigram_freq_words = the, in, i, a` индексация текста \"alone in the dark\" сохранит пары \"in the\" и \"the dark\" как биграммы, потому что они начинаются с частого слова (\"in\" или \"the\"), но \"alone in\" не будет индексироваться, так как \"in\" — второе слово в паре.\n\n* `both_freq` — индексировать только пары слов, где оба слова частые. Продолжая пример, в этом режиме при индексации \"alone in the dark\" будет сохранена только пара \"in the\" (самая плохая с точки зрения поиска), остальные пары не будут индексироваться.\n\nДля большинства случаев `both_freq` будет лучшим режимом, но результаты могут варьироваться.\n\nВажно отметить, что `bigram_index` работает только на уровне токенизации и не учитывает преобразования, такие как `morphology`, `wordforms` или `stopwords`. Это означает, что создаваемые токены очень просты, что делает поиск фраз более точным и строгим. Хотя это может улучшить точность сопоставления фраз, система становится менее способной распознавать разные формы слов или вариации их написания.\n\n<!-- request SQL -->\n\nCODE_BLOCK_111\n\n<!-- request JSON -->\n\nCODE_BLOCK_112\n\n<!-- request PHP -->\n\nCODE_BLOCK_113\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_114\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_115\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_116\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_117\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_118\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_119\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_120\n\n<!-- end -->\n\n### bigram_freq_words\n\nCODE_BLOCK_121\n\n<!-- example bigram_freq_words -->\n\nСписок ключевых слов, считающихся \"частыми\" при индексации биграмм. Необязательно, по умолчанию пустой."
    },
    "is_code_or_comment": false
  },
  "d1239fd78c1f927af3d4a74104f4f868d49454793577ffb91e07bbf3ff9201c5": {
    "original": "[BM25A()](../../Functions/Searching_and_ranking_functions.md#BM25A%28%29) and [BM25F()](../../Functions/Searching_and_ranking_functions.md#BM25F%28%29) functions in the expression ranker are based on these lengths and require `index_field_lengths` to be enabled. Historically, Manticore used a simplified, stripped-down variant of BM25 that, unlike the complete function, did **not** account for document length. There's also support for both a complete variant of BM25, and its extension towards multiple fields, called BM25F. They require per-document length and per-field lengths, respectively. Hence the additional directive.\n\n<!-- request SQL -->\n\nCODE_BLOCK_168\n\n<!-- request JSON -->\n\nCODE_BLOCK_169\n\n<!-- request PHP -->\n\nCODE_BLOCK_170\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_171\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_172\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_173\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_174\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_175\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_176\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_177\n\n<!-- end -->\n\n### index_token_filter\n\nCODE_BLOCK_178\n\n<!-- example index_token_filter -->\n\nIndex-time token filter for full-text indexing. Optional, default is empty.\n\nThe index_token_filter directive specifies an optional index-time token filter for full-text indexing. This directive is used to create a custom tokenizer that makes tokens according to custom rules. The filter is created by the indexer on indexing source data into a plain table or by an RT table on processing `INSERT` or `REPLACE` statements. The plugins are defined using the format `library name:plugin name:optional string of settings`. For example, `my_lib.so:custom_blend:chars=@#&`.\n\n<!-- request SQL -->\n\nCODE_BLOCK_179\n\n<!-- request JSON -->\n\nCODE_BLOCK_180\n\n<!-- request PHP -->\n\nCODE_BLOCK_181\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_182\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_183\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_184\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_185\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_186\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_187\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_188\n\n<!-- end -->\n\n### overshort_step\n\nCODE_BLOCK_189\n\n<!-- example overshort_step -->\n\nPosition increment on overshort (less than [min_word_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len)) keywords. Optional, allowed values are 0 and 1, default is 1.\n\n<!-- request SQL -->\n\nCODE_BLOCK_190\n\n<!-- request JSON -->\n\nCODE_BLOCK_191\n\n<!-- request PHP -->\n\nCODE_BLOCK_192\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_193\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_194\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_195\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_196\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_197\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_198\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_199\n\n<!-- end -->\n\n### phrase_boundary\n\nCODE_BLOCK_200\n\n<!-- example phrase_boundary -->\n\nPhrase boundary characters list. Optional, default is empty.\n\nThis list controls what characters will be treated as phrase boundaries, in order to adjust word positions and enable phrase-level search emulation through proximity search. The syntax is similar to [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table), but mappings are not allowed and the boundary characters must not overlap with anything else.\n\nOn phrase boundary, additional word position increment (specified by [phrase_boundary_step](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary_step)) will be added to current word position. This enables phrase-level searching through proximity queries: words in different phrases will be guaranteed to be more than phrase_boundary_step distance away from each other; so proximity search within that distance will be equivalent to phrase-level search.\n\nPhrase boundary condition will be raised if and only if such character is followed by a separator; this is to avoid abbreviations such as S.T.A.L.K.E.R or URLs being treated as several phrases.\n\n<!-- request SQL -->\n\nCODE_BLOCK_201\n\n<!-- request JSON -->\n\nCODE_BLOCK_202\n\n<!-- request PHP -->\n\nCODE_BLOCK_203\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_204\n\n<!-- intro -->\n\n##### Pytho-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_205\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_206\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_207\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_208\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_209\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_210\n\n<!-- end -->\n\n### phrase_boundary_step\n\nCODE_BLOCK_211\n\n<!-- example phrase_boundary_step -->\n\nPhrase boundary word position increment. Optional, default is 0.\n\nOn phrase boundary, current word position will be additionally incremented by this number.\n\n<!-- request SQL -->\n\nCODE_BLOCK_212\n\n<!-- request JSON -->\n\nCODE_BLOCK_213\n\n<!-- request PHP -->\n\nCODE_BLOCK_214\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_215\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_216\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_217\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_218\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_219\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_220\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_221\n\n<!-- end -->\n\n### regexp_filter\n\nCODE_BLOCK_222\n\n<!-- example regexp_filter -->",
    "translations": {
      "chinese": "[BM25A()](../../Functions/Searching_and_ranking_functions.md#BM25A%28%29) 和 [BM25F()](../../Functions/Searching_and_ranking_functions.md#BM25F%28%29) 表达式排序器中的函数基于这些长度，并且需要启用 `index_field_lengths`。历史上，Manticore 使用了一个简化的、精简版的 BM25，该版本与完整函数不同，**不**考虑文档长度。现在也支持 BM25 的完整变体及其多字段扩展，称为 BM25F。它们分别需要每个文档长度和每个字段长度。因此需要额外的指令。\n\n<!-- request SQL -->\n\nCODE_BLOCK_168\n\n<!-- request JSON -->\n\nCODE_BLOCK_169\n\n<!-- request PHP -->\n\nCODE_BLOCK_170\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_171\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_172\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_173\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_174\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_175\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_176\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_177\n\n<!-- end -->\n\n### index_token_filter\n\nCODE_BLOCK_178\n\n<!-- example index_token_filter -->\n\n全文索引时的索引时令牌过滤器。可选，默认为空。\n\nindex_token_filter 指令指定全文索引时的可选索引时令牌过滤器。该指令用于创建一个根据自定义规则生成令牌的自定义分词器。该过滤器由索引器在将源数据索引到普通表时创建，或由 RT 表在处理 `INSERT` 或 `REPLACE` 语句时创建。插件使用格式 `library name:plugin name:optional string of settings` 定义。例如，`my_lib.so:custom_blend:chars=@#&`。\n\n<!-- request SQL -->\n\nCODE_BLOCK_179\n\n<!-- request JSON -->\n\nCODE_BLOCK_180\n\n<!-- request PHP -->\n\nCODE_BLOCK_181\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_182\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_183\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_184\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_185\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_186\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_187\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_188\n\n<!-- end -->\n\n### overshort_step\n\nCODE_BLOCK_189\n\n<!-- example overshort_step -->\n\n对过短（少于 [min_word_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len)）关键词的位置增量。可选，允许值为 0 和 1，默认是 1。\n\n<!-- request SQL -->\n\nCODE_BLOCK_190\n\n<!-- request JSON -->\n\nCODE_BLOCK_191\n\n<!-- request PHP -->\n\nCODE_BLOCK_192\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_193\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_194\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_195\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_196\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_197\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_198\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_199\n\n<!-- end -->\n\n### phrase_boundary\n\nCODE_BLOCK_200\n\n<!-- example phrase_boundary -->\n\n短语边界字符列表。可选，默认为空。\n\n该列表控制哪些字符将被视为短语边界，以调整词的位置并通过邻近搜索实现短语级搜索模拟。语法类似于 [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table)，但不允许映射，且边界字符不得与其他任何内容重叠。\n\n在短语边界处，将向当前词位置添加额外的词位置增量（由 [phrase_boundary_step](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary_step) 指定）。这使得通过邻近查询实现短语级搜索成为可能：不同短语中的词将保证相距超过 phrase_boundary_step；因此在该距离内的邻近搜索等同于短语级搜索。\n\n只有当该字符后面跟着分隔符时，才会触发短语边界条件；这是为了避免将缩写如 S.T.A.L.K.E.R 或 URL 视为多个短语。\n\n<!-- request SQL -->\n\nCODE_BLOCK_201\n\n<!-- request JSON -->\n\nCODE_BLOCK_202\n\n<!-- request PHP -->\n\nCODE_BLOCK_203\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_204\n\n<!-- intro -->\n\n##### Pytho-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_205\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_206\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_207\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_208\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_209\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_210\n\n<!-- end -->\n\n### phrase_boundary_step\n\nCODE_BLOCK_211\n\n<!-- example phrase_boundary_step -->\n\n短语边界词位置增量。可选，默认是 0。\n\n在短语边界处，当前词位置将额外增加此数值。\n\n<!-- request SQL -->\n\nCODE_BLOCK_212\n\n<!-- request JSON -->\n\nCODE_BLOCK_213\n\n<!-- request PHP -->\n\nCODE_BLOCK_214\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_215\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_216\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_217\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_218\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_219\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_220\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_221\n\n<!-- end -->\n\n### regexp_filter\n\nCODE_BLOCK_222\n\n<!-- example regexp_filter -->",
      "russian": "[BM25A()](../../Functions/Searching_and_ranking_functions.md#BM25A%28%29) и [BM25F()](../../Functions/Searching_and_ranking_functions.md#BM25F%28%29) функции в ранжировщике выражений основаны на этих длинах и требуют включения `index_field_lengths`. Исторически Manticore использовал упрощённый, урезанный вариант BM25, который, в отличие от полной функции, **не** учитывал длину документа. Также поддерживается как полный вариант BM25, так и его расширение для нескольких полей, называемое BM25F. Они требуют длины на документ и длины на поле соответственно. Отсюда и дополнительная директива.\n\n<!-- request SQL -->\n\nCODE_BLOCK_168\n\n<!-- request JSON -->\n\nCODE_BLOCK_169\n\n<!-- request PHP -->\n\nCODE_BLOCK_170\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_171\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_172\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_173\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_174\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_175\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_176\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_177\n\n<!-- end -->\n\n### index_token_filter\n\nCODE_BLOCK_178\n\n<!-- example index_token_filter -->\n\nФильтр токенов во время индексации для полнотекстового индексирования. Опционально, по умолчанию пусто.\n\nДиректива index_token_filter задаёт опциональный фильтр токенов во время индексации для полнотекстового индексирования. Эта директива используется для создания пользовательского токенизатора, который формирует токены согласно пользовательским правилам. Фильтр создаётся индексатором при индексации исходных данных в обычную таблицу или RT-таблицей при обработке операторов `INSERT` или `REPLACE`. Плагины определяются в формате `имя_библиотеки:имя_плагина:опциональная_строка_настроек`. Например, `my_lib.so:custom_blend:chars=@#&`.\n\n<!-- request SQL -->\n\nCODE_BLOCK_179\n\n<!-- request JSON -->\n\nCODE_BLOCK_180\n\n<!-- request PHP -->\n\nCODE_BLOCK_181\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_182\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_183\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_184\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_185\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_186\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_187\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_188\n\n<!-- end -->\n\n### overshort_step\n\nCODE_BLOCK_189\n\n<!-- example overshort_step -->\n\nИнкремент позиции для слишком коротких (меньше [min_word_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len)) ключевых слов. Опционально, допустимые значения 0 и 1, по умолчанию 1.\n\n<!-- request SQL -->\n\nCODE_BLOCK_190\n\n<!-- request JSON -->\n\nCODE_BLOCK_191\n\n<!-- request PHP -->\n\nCODE_BLOCK_192\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_193\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_194\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_195\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_196\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_197\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_198\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_199\n\n<!-- end -->\n\n### phrase_boundary\n\nCODE_BLOCK_200\n\n<!-- example phrase_boundary -->\n\nСписок символов границ фраз. Опционально, по умолчанию пусто.\n\nЭтот список контролирует, какие символы будут рассматриваться как границы фраз, чтобы корректировать позиции слов и включить эмуляцию поиска на уровне фраз через поиск по близости. Синтаксис похож на [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table), но отображения не разрешены, и символы границ не должны пересекаться с чем-либо ещё.\n\nНа границе фразы к текущей позиции слова будет добавлен дополнительный инкремент позиции слова (указанный в [phrase_boundary_step](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary_step)). Это позволяет выполнять поиск на уровне фраз через запросы по близости: слова из разных фраз гарантированно будут находиться на расстоянии больше phrase_boundary_step друг от друга; таким образом, поиск по близости в пределах этого расстояния будет эквивалентен поиску на уровне фраз.\n\nУсловие границы фразы будет срабатывать только если за таким символом следует разделитель; это сделано, чтобы избежать обработки сокращений типа S.T.A.L.K.E.R или URL как нескольких фраз.\n\n<!-- request SQL -->\n\nCODE_BLOCK_201\n\n<!-- request JSON -->\n\nCODE_BLOCK_202\n\n<!-- request PHP -->\n\nCODE_BLOCK_203\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_204\n\n<!-- intro -->\n\n##### Pytho-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_205\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_206\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_207\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_208\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_209\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_210\n\n<!-- end -->\n\n### phrase_boundary_step\n\nCODE_BLOCK_211\n\n<!-- example phrase_boundary_step -->\n\nИнкремент позиции слова на границе фразы. Опционально, по умолчанию 0.\n\nНа границе фразы текущая позиция слова будет дополнительно увеличена на это число.\n\n<!-- request SQL -->\n\nCODE_BLOCK_212\n\n<!-- request JSON -->\n\nCODE_BLOCK_213\n\n<!-- request PHP -->\n\nCODE_BLOCK_214\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_215\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_216\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_217\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_218\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_219\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_220\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_221\n\n<!-- end -->\n\n### regexp_filter\n\nCODE_BLOCK_222\n\n<!-- example regexp_filter -->"
    },
    "is_code_or_comment": false
  },
  "f6022538621573951a037ccb44d8bc4172a98a436f8b083b1058751ace38042e": {
    "original": "# Low-level tokenization\n\nWhen text is indexed in Manticore, it is split into words and case folding is done so that words like \"Abc\", \"ABC\", and \"abc\" are treated as the same word.\n\nTo perform these operations correctly, Manticore must know:\n\n* the encoding of the source text (which should always be UTF-8)\n\n* which characters are considered letters and which are not\n\n* which letters should be folded to other letters\n\nYou can configure these settings on a per-table basis using the  [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) option. charset_table specifies an array that maps letter characters to their case-folded versions (or any other characters that you prefer). Characters that are not present in the array are considered to be non-letters and will be treated as word separators during indexing or searching in this table.\n\nThe default character set is `non_cont`, which includes [most languages](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md).\n\nYou can also define text pattern replacement rules. For example, with the following rules:\n\nCODE_BLOCK_0\n\nThe text `RED TUBE 5\" LONG` would be indexed as `COLOR TUBE 5 INCH LONG`, and `PLANK 2\" x 4\"` would be indexed as `PLANK 2 INCH x 4 INCH`. These rules are applied in the specified order. The rules also apply to queries, so a search for `BLUE TUBE` would actually search for `COLOR TUBE`.\n\nYou can learn more about [regexp_filter here](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter).\n\n## Index configuration options\n\n### charset_table\n\nCODE_BLOCK_1\n\n<!-- example charset_table -->\n\n`charset_table` specifies an array that maps letter characters to their case-folded versions (or any other characters if you prefer). The default character set is `non_cont`, which includes most languages with [non-continuous](https://en.wikipedia.org/wiki/Scriptio_continua) scripts.\n\n`charset_table` is a workhorse of Manticore's tokenization process, which extracts keywords from document text or query text. It controls what characters are accepted as valid and how they should be transformed (e.g. whether case should be removed or not).\n\nBy default, every character maps to 0, which means that it is not considered a valid keyword and is treated as a separator. Once a character is mentioned in the table, it is mapped to another character (most frequently, either to itself or to a lowercase letter) and is treated as a valid keyword part.\n\ncharset_table uses a comma-separated list of mappings to declare characters as valid or to map them to other characters. Syntax shortcuts are available for mapping ranges of characters at once:\n\n* Single char mapping: `A->a`. Declares the source character 'A' as allowed within keywords and maps it to the destination character 'a' (but does not declare 'a' as allowed).\n\n* Range mapping: `A..Z->a..z`. Declares all characters in the source range as allowed and maps them to the destination range. Does not declare the destination range as allowed. Checks the lengths of both ranges.\n\n* Stray char mapping: `a`. Declares a character as allowed and maps it to itself. Equivalent to `a->a` single char mapping.\n\n* Stray range mapping: `a..z`.  Declares all characters in the range as allowed and maps them to themselves. Equivalent to `a..z->a..z` range mapping.\n\n* Checkerboard range mapping: `A..Z/2`. Maps every pair of characters to the second character. For instance, `A..Z/2` is equivalent to `A->B, B->B, C->D, D->D, ..., Y->Z, Z->Z`. This mapping shortcut is helpful for Unicode blocks where uppercase and lowercase letters go in an interleaved order.\n\nFor characters with codes from 0 to 32, and those in the range of 127 to 8-bit ASCII and Unicode characters, Manticore always treats them as separators. To avoid configuration file encoding issues, 8-bit ASCII characters and Unicode characters must be specified in `U+XXX` form, where `XXX` is a hexadecimal code point number. The minimal accepted Unicode character code is `U+0021`.\n\nIf the default mappings are insufficient for your needs, you can redefine the character mappings by specifying them again with another mapping. For example, if the built-in `non_cont` array includes characters `Ä` and `ä` and maps them both to the ASCII character `a`, you can redefine those characters by adding the Unicode code points for them, like this:\n\nCODE_BLOCK_2\n\nfor case sensitive search or\n\nCODE_BLOCK_3\n\nfor case insensitive search.\n\n<!-- request SQL -->\n\nCODE_BLOCK_4\n\n<!-- request JSON -->\n\nCODE_BLOCK_5\n\n<!-- request PHP -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_9\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_12\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n<!-- example charset_table 2 -->\n\nBesides definitions of characters and mappings, there are several built-in aliases that can be used. Current aliases are:\n\n* `chinese`\n\n* `cjk`\n\n* `cont`\n\n* `english`\n\n* `japanese`\n\n* `korean`\n\n* `non_cont` (`non_cjk`)\n\n* `russian`\n\n* `thai`\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- request JSON -->\n\nCODE_BLOCK_15\n\n<!-- request PHP -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_21\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_22\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_23\n\n<!-- end -->",
    "translations": {
      "chinese": "# 低级分词\n\n当文本在 Manticore 中被索引时，它会被拆分成单词，并进行大小写折叠，以便像 \"Abc\"、\"ABC\" 和 \"abc\" 这样的单词被视为相同的单词。\n\n为了正确执行这些操作，Manticore 必须知道：\n\n* 源文本的编码（应始终为 UTF-8）\n\n* 哪些字符被视为字母，哪些不是\n\n* 哪些字母应折叠为其他字母\n\n您可以使用 [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 选项按表配置这些设置。charset_table 指定一个数组，将字母字符映射到它们的大小写折叠版本（或您偏好的其他字符）。数组中未出现的字符被视为非字母，并在该表的索引或搜索过程中作为单词分隔符处理。\n\n默认字符集是 `non_cont`，它包括[大多数语言](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md)。\n\n您还可以定义文本模式替换规则。例如，使用以下规则：\n\nCODE_BLOCK_0\n\n文本 `RED TUBE 5\" LONG` 将被索引为 `COLOR TUBE 5 INCH LONG`，而 `PLANK 2\" x 4\"` 将被索引为 `PLANK 2 INCH x 4 INCH`。这些规则按指定顺序应用。规则也适用于查询，因此搜索 `BLUE TUBE` 实际上会搜索 `COLOR TUBE`。\n\n您可以在[这里了解更多关于 regexp_filter](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter)。\n\n## 索引配置选项\n\n### charset_table\n\nCODE_BLOCK_1\n\n<!-- example charset_table -->\n\n`charset_table` 指定一个数组，将字母字符映射到它们的大小写折叠版本（或您偏好的其他字符）。默认字符集是 `non_cont`，它包括大多数使用[非连续](https://en.wikipedia.org/wiki/Scriptio_continua)脚本的语言。\n\n`charset_table` 是 Manticore 分词过程的核心，它从文档文本或查询文本中提取关键词。它控制哪些字符被接受为有效字符以及它们应如何转换（例如是否去除大小写）。\n\n默认情况下，每个字符映射为 0，表示它不被视为有效关键词，且被视为分隔符。一旦字符在表中被提及，它就会被映射到另一个字符（通常是映射到自身或小写字母），并被视为有效的关键词部分。\n\ncharset_table 使用逗号分隔的映射列表来声明字符为有效或将它们映射到其他字符。语法快捷方式可用于一次映射一系列字符：\n\n* 单字符映射：`A->a`。声明源字符 'A' 在关键词中允许，并映射到目标字符 'a'（但不声明 'a' 为允许字符）。\n\n* 范围映射：`A..Z->a..z`。声明源范围内的所有字符为允许，并映射到目标范围。不会声明目标范围为允许。会检查两个范围的长度。\n\n* 单独字符映射：`a`。声明字符为允许，并映射到自身。等同于 `a->a` 单字符映射。\n\n* 单独范围映射：`a..z`。声明范围内所有字符为允许，并映射到自身。等同于 `a..z->a..z` 范围映射。\n\n* 棋盘范围映射：`A..Z/2`。将每对字符映射到第二个字符。例如，`A..Z/2` 等同于 `A->B, B->B, C->D, D->D, ..., Y->Z, Z->Z`。此映射快捷方式适用于大写和小写字母交错排列的 Unicode 块。\n\n对于代码从 0 到 32 的字符，以及 127 到 8 位 ASCII 和 Unicode 字符范围内的字符，Manticore 总是将它们视为分隔符。为避免配置文件编码问题，8 位 ASCII 字符和 Unicode 字符必须以 `U+XXX` 形式指定，其中 `XXX` 是十六进制代码点编号。接受的最小 Unicode 字符代码是 `U+0021`。\n\n如果默认映射不能满足您的需求，您可以通过再次指定映射来重新定义字符映射。例如，如果内置的 `non_cont` 数组包含字符 `Ä` 和 `ä` 并将它们都映射到 ASCII 字符 `a`，您可以通过添加它们的 Unicode 代码点来重新定义这些字符，如下所示：\n\nCODE_BLOCK_2\n\n用于区分大小写的搜索，或\n\nCODE_BLOCK_3\n\n用于不区分大小写的搜索。\n\n<!-- request SQL -->\n\nCODE_BLOCK_4\n\n<!-- request JSON -->\n\nCODE_BLOCK_5\n\n<!-- request PHP -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_9\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_12\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n<!-- example charset_table 2 -->\n\n除了字符和映射的定义外，还有几个内置别名可用。当前别名有：\n\n* `chinese`\n\n* `cjk`\n\n* `cont`\n\n* `english`\n\n* `japanese`\n\n* `korean`\n\n* `non_cont` (`non_cjk`)\n\n* `russian`\n\n* `thai`\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- request JSON -->\n\nCODE_BLOCK_15\n\n<!-- request PHP -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_21\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_22\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_23\n\n<!-- end -->",
      "russian": "# Низкоуровневая токенизация\n\nКогда текст индексируется в Manticore, он разбивается на слова, и выполняется преобразование регистра, чтобы такие слова, как \"Abc\", \"ABC\" и \"abc\", рассматривались как одно и то же слово.\n\nДля правильного выполнения этих операций Manticore должен знать:\n\n* кодировку исходного текста (которая всегда должна быть UTF-8)\n\n* какие символы считаются буквами, а какие нет\n\n* какие буквы должны преобразовываться в другие буквы\n\nВы можете настроить эти параметры для каждой таблицы отдельно, используя опцию [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). charset_table задаёт массив, который отображает буквенные символы в их версии с приведённым регистром (или в любые другие символы по вашему выбору). Символы, отсутствующие в массиве, считаются не буквами и будут рассматриваться как разделители слов при индексировании или поиске в этой таблице.\n\nПо умолчанию используется набор символов `non_cont`, который включает [большинство языков](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md).\n\nВы также можете определить правила замены текстовых шаблонов. Например, с помощью следующих правил:\n\nCODE_BLOCK_0\n\nТекст `RED TUBE 5\" LONG` будет индексироваться как `COLOR TUBE 5 INCH LONG`, а `PLANK 2\" x 4\"` будет индексироваться как `PLANK 2 INCH x 4 INCH`. Эти правила применяются в указанном порядке. Правила также применяются к запросам, поэтому поиск по `BLUE TUBE` фактически будет искать `COLOR TUBE`.\n\nПодробнее о [regexp_filter можно узнать здесь](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter).\n\n## Опции конфигурации индекса\n\n### charset_table\n\nCODE_BLOCK_1\n\n<!-- example charset_table -->\n\n`charset_table` задаёт массив, который отображает буквенные символы в их версии с приведённым регистром (или в любые другие символы, если вы предпочитаете). По умолчанию используется набор символов `non_cont`, который включает большинство языков с [непрерывными](https://en.wikipedia.org/wiki/Scriptio_continua) скриптами.\n\n`charset_table` является основным элементом процесса токенизации Manticore, который извлекает ключевые слова из текста документа или текста запроса. Он контролирует, какие символы считаются допустимыми и как они должны преобразовываться (например, нужно ли убирать регистр).\n\nПо умолчанию каждый символ отображается в 0, что означает, что он не считается допустимым ключевым словом и рассматривается как разделитель. Как только символ упоминается в таблице, он отображается в другой символ (чаще всего либо в себя, либо в строчную букву) и считается допустимой частью ключевого слова.\n\ncharset_table использует список отображений, разделённых запятыми, чтобы объявить символы допустимыми или отобразить их в другие символы. Доступны сокращения для отображения диапазонов символов сразу:\n\n* Отображение одного символа: `A->a`. Объявляет исходный символ 'A' допустимым в ключевых словах и отображает его в символ 'a' (но не объявляет 'a' допустимым).\n\n* Отображение диапазона: `A..Z->a..z`. Объявляет все символы в исходном диапазоне допустимыми и отображает их в символы в целевом диапазоне. Не объявляет целевой диапазон допустимым. Проверяет длины обоих диапазонов.\n\n* Отображение одиночного символа: `a`. Объявляет символ допустимым и отображает его в себя. Эквивалентно отображению одного символа `a->a`.\n\n* Отображение одиночного диапазона: `a..z`. Объявляет все символы в диапазоне допустимыми и отображает их в себя. Эквивалентно отображению диапазона `a..z->a..z`.\n\n* Отображение с шагом 2: `A..Z/2`. Отображает каждую пару символов во второй символ. Например, `A..Z/2` эквивалентно `A->B, B->B, C->D, D->D, ..., Y->Z, Z->Z`. Это сокращение полезно для блоков Unicode, где заглавные и строчные буквы идут в перемешку.\n\nДля символов с кодами от 0 до 32, а также для символов в диапазоне от 127 до 8-битных ASCII и Unicode, Manticore всегда рассматривает их как разделители. Чтобы избежать проблем с кодировкой в конфигурационных файлах, 8-битные ASCII символы и Unicode символы должны указываться в форме `U+XXX`, где `XXX` — это шестнадцатеричный номер кода символа. Минимально допустимый код символа Unicode — `U+0021`.\n\nЕсли стандартных отображений недостаточно, вы можете переопределить отображения символов, указав их заново с другим отображением. Например, если встроенный массив `non_cont` включает символы `Ä` и `ä` и отображает их обоих в ASCII символ `a`, вы можете переопределить эти символы, добавив их коды Unicode, например так:\n\nCODE_BLOCK_2\n\nдля поиска с учётом регистра или\n\nCODE_BLOCK_3\n\nдля поиска без учёта регистра.\n\n<!-- request SQL -->\n\nCODE_BLOCK_4\n\n<!-- request JSON -->\n\nCODE_BLOCK_5\n\n<!-- request PHP -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_9\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_12\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n<!-- example charset_table 2 -->\n\nПомимо определения символов и отображений, доступны несколько встроенных псевдонимов, которые можно использовать. Текущие псевдонимы:\n\n* `chinese`\n\n* `cjk`\n\n* `cont`\n\n* `english`\n\n* `japanese`\n\n* `korean`\n\n* `non_cont` (`non_cjk`)\n\n* `russian`\n\n* `thai`\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- request JSON -->\n\nCODE_BLOCK_15\n\n<!-- request PHP -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_21\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_22\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_23\n\n<!-- end -->"
    },
    "is_code_or_comment": false
  },
  "14c99301e224769211c6a5c3382b888804e6648af53dc4f9306c58fc4c78ea1c": {
    "original": "If you want to support different languages in your search, it can be a laborious task to define sets of valid characters and folding rules for all of them. We have simplified this for you by providing default charset tables, `non_cont` and `cont`, that cover languages with non-continuous and continuous (Chinese, Japanese, Korean, Thai) scripts, respectively. In most cases, these charsets should be sufficient for your needs.\n\nPlease note that the following languages are currently **not** supported:\n\n* Assamese\n\n* Bishnupriya\n\n* Buhid\n\n* Garo\n\n* Hmong\n\n* Ho\n\n* Komi\n\n* Large Flowery Miao\n\n* Maba\n\n* Maithili\n\n* Marathi\n\n* Mende\n\n* Mru\n\n* Myene\n\n* Ngambay\n\n* Odia\n\n* Santali\n\n* Sindhi\n\n* Sylheti\n\nAll other languages listed in the [Unicode languages\n\nlist](http://www.unicode.org/cldr/charts/latest/supplemental/languages_and_scripts.html/) are supported by default.\n\n<!-- example charset_table 3 -->\n\nTo work with both cont and non-cont languages, set the options in your configuration file as shown below (with an [exception](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md) for Chinese):\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- request JSON -->\n\nCODE_BLOCK_25\n\n<!-- request PHP -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_32\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\nIf you do not require support for continuous-script languages, you can simply exclude the [ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len) and [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars).\n\noptions. For more information on these options, refer to the corresponding documentation sections.\n\nTo map one character to multiple characters or vice versa, you can use [regexp_filter](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter) can be helpful.\n\n### blend_chars\n\nCODE_BLOCK_34\n\n<!-- example blend_chars -->\n\nBlended characters list. Optional, default is empty.\n\nBlended characters are indexed as both separators and valid characters. For example, when `&` is defined as a blended character and `AT&T` appears in an indexed document, three different keywords will be indexed, `at&t`, `at` and `t`.\n\nAdditionally, blended characters can influence indexing in such a way that keywords are indexed as if the blended characters were not typed at all. This behavior is particularly evident when `blend_mode = trim_all` is specified. For example, the phrase `some_thing` will be indexed as `some`, `something`, and `thing` with `blend_mode = trim_all`.\n\nCare should be taken when using blended characters as defining a character as blended means that it is no longer a separator.\n\n* Therefore, if you put a comma to the `blend_chars` and search for `dog,cat`, it will treat that as a single token `dog,cat`. If `dog,cat` was **not** indexed as `dog,cat`, but left as `dog cat` only, then it will not match.\n\n* Hence, this behavior should be controlled with the [blend_mode](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_mode) setting.\n\nPositions for tokens obtained by replacing blended characters with whitespace are assigned as usual, and regular keywords will be indexed as if there were no `blend_chars` specified at all. An additional token that mixes blended and non-blended characters will be put at the starting position. For instance, if `AT&T company` occurs in the very beginning of the text field, `at` will be given position 1, `t` position 2, `company` position 3, and `AT&T` will also be given position 1, blending with the opening regular keyword. As a result, queries for `AT&T` or just `AT` will match that document. A phrase query for `\"AT T\"` will also match, as well as a phrase query for `\"AT&T company\"`.\n\nBlended characters can overlap with special characters used in query syntax, such as `T-Mobile` or `@twitter`. Where possible, the query parser will handle the blended character as blended. For instance, if `hello @twitter` is within quotes (a phrase operator), the query parser will handle the `@` symbol as blended. However, if the `@` symbol was not within quotes, the character would be handled as an operator. Therefore, it is recommended to escape keywords.\n\nBlended characters can be remapped so that multiple different blended characters can be normalized into one base form. This is useful when indexing multiple alternative Unicode codepoints with equivalent glyphs.\n\n<!-- request SQL -->\n\nCODE_BLOCK_35\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- request PHP -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_42\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_43\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_44\n\n<!-- end -->\n\n### blend_mode\n\nCODE_BLOCK_45\n\n<!-- example blend_mode -->\n\nThe blended tokens indexing mode is enabled by the blend_mode directive.\n\nBy default, tokens that mix blended and non-blended characters get indexed entirely. For example, when both an at-sign and an exclamation are in `blend_chars`, the string `@dude!` will be indexed as two tokens: `@dude!` (with all the blended characters) and `dude` (without any). As a result, a query of `@dude` will **not** match it.\n\n`blend_mode` adds flexibility to this indexing behavior. It takes a comma-separated list of options, each of which specifies a token indexing variant.",
    "translations": {
      "chinese": "如果您想在搜索中支持不同的语言，定义所有语言的有效字符集和折叠规则可能是一项繁重的工作。我们为您简化了这一过程，提供了默认的字符集表 `non_cont` 和 `cont`，分别涵盖了非连续和连续（中文、日文、韩文、泰文）脚本的语言。在大多数情况下，这些字符集应足以满足您的需求。\n\n请注意，以下语言当前**不支持**：\n\n* 阿萨姆语\n\n* 比什努普里亚语\n\n* 布希德语\n\n* 加罗语\n\n* 苗族语\n\n* 霍语\n\n* 科米语\n\n* 大花苗语\n\n* 马巴语\n\n* 迈蒂利语\n\n* 马拉地语\n\n* 门德语\n\n* 姆鲁语\n\n* 米耶内语\n\n* 恩甘贝语\n\n* 奥里亚语\n\n* 桑塔利语\n\n* 信德语\n\n* 锡尔赫蒂语\n\n所有其他列在[Unicode语言列表](http://www.unicode.org/cldr/charts/latest/supplemental/languages_and_scripts.html/)中的语言默认均支持。\n\n<!-- example charset_table 3 -->\n\n要同时处理 cont 和 non-cont 语言，请在配置文件中按如下所示设置选项（中文有一个[例外](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md)）：\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- request JSON -->\n\nCODE_BLOCK_25\n\n<!-- request PHP -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_32\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\n如果您不需要支持连续脚本语言，可以简单地排除[ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len)和[ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars)选项。有关这些选项的更多信息，请参阅相应的文档部分。\n\n要将一个字符映射到多个字符或反之，您可以使用[regexp_filter](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter)。\n\n### blend_chars\n\nCODE_BLOCK_34\n\n<!-- example blend_chars -->\n\n混合字符列表。可选，默认为空。\n\n混合字符既被索引为分隔符，也被索引为有效字符。例如，当 `&` 被定义为混合字符且索引文档中出现 `AT&T` 时，将索引三个不同的关键词：`at&t`、`at` 和 `t`。\n\n此外，混合字符还可以影响索引，使得关键词的索引效果仿佛混合字符根本未被输入。这种行为在指定 `blend_mode = trim_all` 时尤为明显。例如，短语 `some_thing` 在 `blend_mode = trim_all` 下将被索引为 `some`、`something` 和 `thing`。\n\n使用混合字符时应谨慎，因为将字符定义为混合字符意味着它不再是分隔符。\n\n* 因此，如果您将逗号放入 `blend_chars` 并搜索 `dog,cat`，它将被视为单个标记 `dog,cat`。如果 `dog,cat` **未** 被索引为 `dog,cat`，而仅作为 `dog cat`，则不会匹配。\n\n* 因此，这种行为应通过[blend_mode](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_mode)设置进行控制。\n\n通过将混合字符替换为空格获得的标记位置按常规分配，常规关键词将被索引，仿佛未指定任何 `blend_chars`。一个混合了混合字符和非混合字符的额外标记将放置在起始位置。例如，如果文本字段开头出现 `AT&T company`，则 `at` 位置为1，`t` 位置为2，`company` 位置为3，同时 `AT&T` 也被赋予位置1，与开头的常规关键词混合。因此，查询 `AT&T` 或仅 `AT` 都会匹配该文档。短语查询 `\"AT T\"` 也会匹配，短语查询 `\"AT&T company\"` 亦然。\n\n混合字符可能与查询语法中使用的特殊字符重叠，例如 `T-Mobile` 或 `@twitter`。查询解析器会尽可能将混合字符作为混合字符处理。例如，如果 `hello @twitter` 在引号内（短语操作符），查询解析器会将 `@` 视为混合字符。但如果 `@` 不在引号内，则该字符会被视为操作符。因此，建议对关键词进行转义。\n\n混合字符可以重新映射，使多个不同的混合字符规范化为一个基本形式。这在索引多个具有等效字形的替代 Unicode 代码点时非常有用。\n\n<!-- request SQL -->\n\nCODE_BLOCK_35\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- request PHP -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_42\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_43\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_44\n\n<!-- end -->\n\n### blend_mode\n\nCODE_BLOCK_45\n\n<!-- example blend_mode -->\n\n混合标记索引模式由 blend_mode 指令启用。\n\n默认情况下，混合和非混合字符混合的标记会被完整索引。例如，当 `blend_chars` 中同时包含 at 符号和感叹号时，字符串 `@dude!` 会被索引为两个标记：`@dude!`（包含所有混合字符）和 `dude`（不含任何混合字符）。因此，查询 `@dude` **不会** 匹配它。\n\n`blend_mode` 为此索引行为增加了灵活性。它接受一个逗号分隔的选项列表，每个选项指定一种标记索引变体。",
      "russian": "Если вы хотите поддерживать разные языки в поиске, определение наборов допустимых символов и правил свёртки для всех из них может быть трудоёмкой задачей. Мы упростили это для вас, предоставив таблицы символов по умолчанию — `non_cont` и `cont`, которые охватывают языки с прерывистыми и непрерывными (китайский, японский, корейский, тайский) письменностями соответственно. В большинстве случаев этих наборов символов будет достаточно для ваших нужд.\n\nОбратите внимание, что следующие языки в настоящее время **не** поддерживаются:\n\n* Assamese\n\n* Bishnupriya\n\n* Buhid\n\n* Garo\n\n* Hmong\n\n* Ho\n\n* Komi\n\n* Large Flowery Miao\n\n* Maba\n\n* Maithili\n\n* Marathi\n\n* Mende\n\n* Mru\n\n* Myene\n\n* Ngambay\n\n* Odia\n\n* Santali\n\n* Sindhi\n\n* Sylheti\n\nВсе остальные языки, перечисленные в [списке языков Unicode](http://www.unicode.org/cldr/charts/latest/supplemental/languages_and_scripts.html/), поддерживаются по умолчанию.\n\n<!-- example charset_table 3 -->\n\nЧтобы работать как с языками с непрерывной, так и с прерывистой письменностью, установите параметры в вашем конфигурационном файле, как показано ниже (с [исключением](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md) для китайского):\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- request JSON -->\n\nCODE_BLOCK_25\n\n<!-- request PHP -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_32\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\nЕсли вам не нужна поддержка языков с непрерывной письменностью, вы можете просто исключить параметры [ngram_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_len) и [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars). Для получения дополнительной информации об этих параметрах обратитесь к соответствующим разделам документации.\n\nДля отображения одного символа в несколько символов или наоборот может быть полезен [regexp_filter](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#regexp_filter).\n\n### blend_chars\n\nCODE_BLOCK_34\n\n<!-- example blend_chars -->\n\nСписок смешанных символов. Необязательно, по умолчанию пусто.\n\nСмешанные символы индексируются как разделители и как допустимые символы. Например, если `&` определён как смешанный символ и в индексируемом документе встречается `AT&T`, будут проиндексированы три разных ключевых слова: `at&t`, `at` и `t`.\n\nКроме того, смешанные символы могут влиять на индексацию так, что ключевые слова индексируются так, как будто смешанные символы вообще не были введены. Такое поведение особенно заметно при указании `blend_mode = trim_all`. Например, фраза `some_thing` будет индексироваться как `some`, `something` и `thing` при `blend_mode = trim_all`.\n\nПри использовании смешанных символов следует быть осторожным, так как определение символа как смешанного означает, что он больше не является разделителем.\n\n* Поэтому, если вы добавите запятую в `blend_chars` и выполните поиск по `dog,cat`, это будет рассматриваться как один токен `dog,cat`. Если `dog,cat` **не** был проиндексирован как `dog,cat`, а остался как `dog cat`, совпадения не будет.\n\n* Следовательно, это поведение следует контролировать с помощью настройки [blend_mode](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_mode).\n\nПозиции для токенов, полученных заменой смешанных символов на пробелы, назначаются как обычно, и обычные ключевые слова индексируются так, как если бы `blend_chars` вообще не было. Дополнительный токен, смешивающий смешанные и несмешанные символы, будет помещён на начальную позицию. Например, если в самом начале текстового поля встречается `AT&T company`, `at` получит позицию 1, `t` — позицию 2, `company` — позицию 3, а `AT&T` также получит позицию 1, смешиваясь с открывающим обычным ключевым словом. В результате запросы `AT&T` или просто `AT` найдут этот документ. Фразовый запрос `\"AT T\"` также совпадёт, как и фразовый запрос `\"AT&T company\"`.\n\nСмешанные символы могут пересекаться со специальными символами, используемыми в синтаксисе запросов, такими как `T-Mobile` или `@twitter`. По возможности парсер запросов будет обрабатывать смешанный символ как смешанный. Например, если `hello @twitter` находится в кавычках (оператор фразы), парсер запросов обработает символ `@` как смешанный. Однако если символ `@` не в кавычках, он будет обработан как оператор. Поэтому рекомендуется экранировать ключевые слова.\n\nСмешанные символы могут быть переназначены так, чтобы несколько разных смешанных символов нормализовались в одну базовую форму. Это полезно при индексации нескольких альтернативных кодовых точек Unicode с эквивалентными глифами.\n\n<!-- request SQL -->\n\nCODE_BLOCK_35\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- request PHP -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_42\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_43\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_44\n\n<!-- end -->\n\n### blend_mode\n\nCODE_BLOCK_45\n\n<!-- example blend_mode -->\n\nРежим индексации смешанных токенов включается директивой blend_mode.\n\nПо умолчанию токены, смешивающие смешанные и несмешанные символы, индексируются полностью. Например, если в `blend_chars` включены символы @ и !, строка `@dude!` будет индексироваться как два токена: `@dude!` (со всеми смешанными символами) и `dude` (без них). В результате запрос `@dude` **не** найдёт совпадений.\n\n`blend_mode` добавляет гибкости этому поведению индексации. Он принимает список опций, разделённых запятыми, каждая из которых задаёт вариант индексации токенов."
    },
    "is_code_or_comment": false
  },
  "86a5bb34c86bea90c1da82edba9eea54824a8e1fbe0f3f55d2a7ac79f9fc472f": {
    "original": "If multiple options are specified, multiple variants of the same token will be indexed. Regular keywords (resulting from that token by replacing blended characters with a separator) are always indexed.\n\nThe options are:\n\n* `trim_none` - Index the entire token\n\n* `trim_head` - Trim heading blended characters, and index the resulting token\n\n* `trim_tail` - Trim trailing blended characters, and index the resulting token\n\n* `trim_both`- Trim both heading and trailing blended characters, and index the resulting token\n\n* `trim_all` - Trim heading, trailing, and middle blended characters, and index the resulting token\n\n* `skip_pure` - Do not index the token if it is purely blended, that is, consists of blended characters only\n\nUsing `blend_mode` with the example `@dude!` string above, the setting `blend_mode = trim_head, trim_tail` would result in two indexed tokens: `@dude` and `dude!`. Using `trim_both` would have no effect because trimming both blended characters results in `dude`, which is already indexed as a regular keyword. Indexing `@U.S.A.` with `trim_both` (and assuming that dot is blended two) would result in `U.S.A` being indexed. Lastly, `skip_pure` enables you to ignore sequences of blended characters only. For example, `one @@@ two` would be indexed as `one two`, and match that as a phrase. This is not the case by default because a fully blended token gets indexed and offsets the second keyword position.\n\nDefault behavior is to index the entire token, equivalent to `blend_mode = trim_none`.\n\nBe aware that using blend modes limits your search, even with the default mode `trim_none` if you assume `.` is a blended character:\n\n* `.dog.` will become `.dog. dog` during indexing\n\n* and you won't be able to find it by `dog.`.\n\nUsing more modes increases the chance your keyword will match something.\n\n<!-- request SQL -->\n\nCODE_BLOCK_46\n\n<!-- request JSON -->\n\nCODE_BLOCK_47\n\n<!-- request PHP -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_50\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_53\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_54\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_55\n\n<!-- end -->\n\n### min_word_len\n\nCODE_BLOCK_56\n\n<!-- example min_word_len -->\n\nmin_word_len is an optional index configuration option in Manticore that specifies the minimum indexed word length. The default value is 1, which means that everything is indexed.\n\nOnly those words that are not shorter than this minimum will be indexed. For example, if min_word_len is 4, then 'the' won't be indexed, but 'they' will be.\n\n<!-- request SQL -->\n\nCODE_BLOCK_57\n\n<!-- request JSON -->\n\nCODE_BLOCK_58\n\n<!-- request PHP -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_64\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_65\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_66\n\n<!-- end -->\n\n### ngram_len\n\nCODE_BLOCK_67\n\n<!-- example ngram_len -->\n\nN-gram lengths for N-gram indexing. Optional, default is 0 (disable n-gram indexing). Known values are 0 and 1.\n\nN-grams provide basic support for continuous-script languages in unsegmented texts. The issue with searching in languages using continuous scripts is the absence of clear separators between words. In some cases, you may not want to use dictionary-based segmentation, such as [the one available for Chinese](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md). In those instances, n-gram segmentation might also work well.\n\nWhen this feature is enabled, streams of such languages (or any other characters defined in [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars)) are indexed as N-grams. For example, if the incoming text is \"ABCDEF\" (where A to F represent some language characters) and ngram_len is 1, it will be indexed as if it were \"A B C D E F\". Only ngram_len=1 is currently supported. Only those characters that are listed in [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars) table will be split this way; others will not be affected.\n\nNote that if the search query is segmented, i.e. there are separators between individual words, then wrapping the words in quotes and using extended mode will result in proper matches being found even if the text was **not** segmented. For instance, assume that the original query is `BC DEF`. After wrapping in quotes on the application side, it should look like `\"BC\" \"DEF\"` (*with* quotes). This query will be passed to Manticore and internally split into 1-grams too, resulting in `\"B C\" \"D E F\"` query, still with quotes that are the phrase matching operator. And it will match the text even though there were no separators in the text.\n\nEven if the search query is not segmented, Manticore should still produce good results, thanks to phrase-based ranking: it will pull closer phrase matches (which in the case of N-gram words can mean closer multi-character word matches) to the top.\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->",
    "translations": {
      "chinese": "如果指定了多个选项，将会索引同一标记的多个变体。常规关键字（通过用分隔符替换混合字符从该标记生成的）始终会被索引。\n\n选项包括：\n\n* `trim_none` - 索引整个标记\n\n* `trim_head` - 修剪开头的混合字符，并索引结果标记\n\n* `trim_tail` - 修剪结尾的混合字符，并索引结果标记\n\n* `trim_both`- 修剪开头和结尾的混合字符，并索引结果标记\n\n* `trim_all` - 修剪开头、结尾和中间的混合字符，并索引结果标记\n\n* `skip_pure` - 如果标记纯粹由混合字符组成，则不索引该标记\n\n使用上述示例字符串 `@dude!` 的 `blend_mode`，设置 `blend_mode = trim_head, trim_tail` 会导致索引两个标记：`@dude` 和 `dude!`。使用 `trim_both` 不会有影响，因为修剪两端的混合字符后得到的 `dude` 已作为常规关键字被索引。使用 `trim_both` 索引 `@U.S.A.`（假设点是混合字符）会索引 `U.S.A`。最后，`skip_pure` 允许忽略仅由混合字符组成的序列。例如，`one @@@ two` 会被索引为 `one two`，并作为短语匹配。默认情况下不是这样，因为完全混合的标记会被索引并影响第二个关键字的位置。\n\n默认行为是索引整个标记，相当于 `blend_mode = trim_none`。\n\n请注意，即使使用默认模式 `trim_none`，如果假设 `.` 是混合字符，使用混合模式也会限制搜索：\n\n* `.dog.` 在索引时会变成 `.dog. dog`\n\n* 你将无法通过 `dog.` 找到它。\n\n使用更多模式会增加关键字匹配的可能性。\n\n<!-- request SQL -->\n\nCODE_BLOCK_46\n\n<!-- request JSON -->\n\nCODE_BLOCK_47\n\n<!-- request PHP -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_50\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_53\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_54\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_55\n\n<!-- end -->\n\n### min_word_len\n\nCODE_BLOCK_56\n\n<!-- example min_word_len -->\n\nmin_word_len 是 Manticore 中的一个可选索引配置选项，指定最小索引词长。默认值为 1，表示所有内容都会被索引。\n\n只有长度不小于该最小值的词才会被索引。例如，如果 min_word_len 是 4，则 'the' 不会被索引，但 'they' 会被索引。\n\n<!-- request SQL -->\n\nCODE_BLOCK_57\n\n<!-- request JSON -->\n\nCODE_BLOCK_58\n\n<!-- request PHP -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_64\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_65\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_66\n\n<!-- end -->\n\n### ngram_len\n\nCODE_BLOCK_67\n\n<!-- example ngram_len -->\n\nN-gram 长度，用于 N-gram 索引。可选，默认值为 0（禁用 N-gram 索引）。已知值为 0 和 1。\n\nN-gram 为连续书写语言的无分词文本提供基本支持。使用连续书写语言搜索的问题在于缺少明确的词间分隔符。在某些情况下，你可能不想使用基于词典的分词，比如[中文的分词](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md)。在这些情况下，N-gram 分词也可能效果良好。\n\n启用此功能时，这些语言的文本流（或任何在 [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars) 中定义的字符）将作为 N-gram 索引。例如，如果输入文本是 \"ABCDEF\"（其中 A 到 F 代表某种语言字符），且 ngram_len 是 1，则会被索引为 \"A B C D E F\"。目前仅支持 ngram_len=1。只有列在 [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars) 表中的字符会被这样拆分；其他字符不受影响。\n\n请注意，如果搜索查询已分词，即单词间有分隔符，则在应用端将单词用引号包裹并使用扩展模式，仍能正确匹配未分词的文本。例如，假设原始查询是 `BC DEF`。在应用端包裹引号后，应为 `\"BC\" \"DEF\"`（带引号）。该查询传递给 Manticore 后，内部也会拆分成 1-gram，变成 `\"B C\" \"D E F\"` 查询，仍带有作为短语匹配操作符的引号。即使文本中没有分隔符，也能匹配该文本。\n\n即使搜索查询未分词，Manticore 仍应产生良好结果，得益于基于短语的排名：它会将更接近的短语匹配（对于 N-gram 词来说，意味着更接近的多字符词匹配）排在前面。\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->",
      "russian": "Если указано несколько опций, будет проиндексировано несколько вариантов одного и того же токена. Обычные ключевые слова (полученные из этого токена путем замены смешанных символов на разделитель) всегда индексируются.\n\nОпции:\n\n* `trim_none` - Индексировать весь токен\n\n* `trim_head` - Обрезать начальные смешанные символы и индексировать полученный токен\n\n* `trim_tail` - Обрезать конечные смешанные символы и индексировать полученный токен\n\n* `trim_both` - Обрезать как начальные, так и конечные смешанные символы и индексировать полученный токен\n\n* `trim_all` - Обрезать начальные, конечные и средние смешанные символы и индексировать полученный токен\n\n* `skip_pure` - Не индексировать токен, если он состоит только из смешанных символов\n\nИспользуя `blend_mode` с примером строки `@dude!` выше, настройка `blend_mode = trim_head, trim_tail` приведет к индексации двух токенов: `@dude` и `dude!`. Использование `trim_both` не даст эффекта, так как обрезка обоих смешанных символов приведет к `dude`, который уже индексируется как обычное ключевое слово. Индексация `@U.S.A.` с `trim_both` (при условии, что точка считается смешанным символом) приведет к индексации `U.S.A`. Наконец, `skip_pure` позволяет игнорировать последовательности, состоящие только из смешанных символов. Например, `one @@@ two` будет индексироваться как `one two` и будет соответствовать этой фразе. По умолчанию это не так, потому что полностью смешанный токен индексируется и смещает позицию второго ключевого слова.\n\nПоведение по умолчанию — индексировать весь токен, что эквивалентно `blend_mode = trim_none`.\n\nУчтите, что использование режимов смешивания ограничивает поиск, даже при режиме по умолчанию `trim_none`, если считать `.` смешанным символом:\n\n* `.dog.` будет индексироваться как `.dog. dog`\n\n* и вы не сможете найти его по запросу `dog.`.\n\nИспользование большего количества режимов увеличивает вероятность совпадения вашего ключевого слова.\n\n<!-- request SQL -->\n\nCODE_BLOCK_46\n\n<!-- request JSON -->\n\nCODE_BLOCK_47\n\n<!-- request PHP -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_50\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_53\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_54\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_55\n\n<!-- end -->\n\n### min_word_len\n\nCODE_BLOCK_56\n\n<!-- example min_word_len -->\n\nmin_word_len — это необязательная опция конфигурации индекса в Manticore, которая задает минимальную длину индексируемого слова. Значение по умолчанию — 1, что означает, что индексируются все слова.\n\nИндексируются только те слова, длина которых не меньше этого минимума. Например, если min_word_len равен 4, то слово 'the' индексироваться не будет, а 'they' — будет.\n\n<!-- request SQL -->\n\nCODE_BLOCK_57\n\n<!-- request JSON -->\n\nCODE_BLOCK_58\n\n<!-- request PHP -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_64\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_65\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_66\n\n<!-- end -->\n\n### ngram_len\n\nCODE_BLOCK_67\n\n<!-- example ngram_len -->\n\nДлины N-грамм для индексирования N-граммами. Необязательно, значение по умолчанию 0 (отключить индексирование N-граммами). Известные значения — 0 и 1.\n\nN-граммы обеспечивают базовую поддержку языков с непрерывным письмом в неразмеченных текстах. Проблема поиска в языках с непрерывным письмом заключается в отсутствии четких разделителей между словами. В некоторых случаях вы можете не захотеть использовать сегментацию на основе словаря, например, [как для китайского языка](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md). В таких случаях сегментация N-граммами может работать хорошо.\n\nКогда эта функция включена, потоки таких языков (или любых других символов, определенных в [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars)) индексируются как N-граммы. Например, если входящий текст — \"ABCDEF\" (где A–F — символы какого-то языка), и ngram_len равен 1, он будет индексироваться как \"A B C D E F\". В настоящее время поддерживается только ngram_len=1. Только символы, перечисленные в таблице [ngram_chars](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ngram_chars), будут разбиваться таким образом; остальные не будут затронуты.\n\nОбратите внимание, что если поисковый запрос сегментирован, то есть между отдельными словами есть разделители, то оборачивание слов в кавычки и использование расширенного режима приведет к правильному поиску совпадений, даже если текст **не** был сегментирован. Например, предположим, что исходный запрос — `BC DEF`. После оборачивания в кавычки на стороне приложения он должен выглядеть как `\"BC\" \"DEF\"` (*с* кавычками). Этот запрос будет передан в Manticore и внутренне также разбит на 1-граммы, что даст запрос `\"B C\" \"D E F\"`, все еще с кавычками, которые являются оператором поиска фразы. И он найдет совпадение в тексте, даже если в тексте не было разделителей.\n\nДаже если поисковый запрос не сегментирован, Manticore должен выдавать хорошие результаты благодаря ранжированию на основе фраз: он поднимет выше совпадения фраз (что в случае N-грамм может означать более близкие совпадения многосимвольных слов).\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->"
    },
    "is_code_or_comment": false
  }
}
