{
  "54c2d88640a9aaa3941612d55b031221b09022a55bdcc521722671098e21f7d6": {
    "original": "<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_53\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_54\n\n<!-- end -->\n\n## jieba_mode\n\n<!-- example jieba_mode -->\n\nCODE_BLOCK_55\n\nJieba segmentation mode. Optional; the default is `accurate`.\n\nIn accurate mode, Jieba splits the sentence into the most precise words using dictionary matching. This mode focuses on precision, ensuring that the segmentation is as accurate as possible.\n\nIn full mode, Jieba tries to split the sentence into every possible word combination, aiming to include all potential words. This mode focuses on maximizing recall, meaning it identifies as many words as possible, even if some of them overlap or are less commonly used. It returns all the words found in its dictionary.\n\nIn search mode, Jieba breaks the text into both whole words and smaller parts, combining precise segmentation with extra detail by providing overlapping word fragments. This mode balances precision and recall, making it useful for search engines.\n\n`jieba_mode` should be used with `morphology = jieba_chinese`. See [Chinese, Japanese, Korean (CJK) and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_56\n\n<!-- request JSON -->\n\nCODE_BLOCK_57\n\n<!-- request PHP -->\n\nCODE_BLOCK_58\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_64\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_65\n\n<!-- end -->\n\n## jieba_user_dict_path\n\n<!-- example jieba_user_dict_path -->\n\nCODE_BLOCK_66\n\nPath to the Jieba user dictionary. Optional.\n\nJieba, a Chinese text segmentation library, uses dictionary files to assist with word segmentation. The format of these dictionary files is as follows: each line contains a word, split into three parts separated by spaces — the word itself, word frequency, and part of speech (POS) tag. The word frequency and POS tag are optional and can be omitted. The dictionary file must be UTF-8 encoded.\n\nExample:\n\nCODE_BLOCK_67\n\n`jieba_user_dict_path` should be used with `morphology = jieba_chinese`. For more details, see [Chinese, Japanese, Korean (CJK), and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_77\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_53\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_54\n\n<!-- end -->\n\n## jieba_mode\n\n<!-- example jieba_mode -->\n\nCODE_BLOCK_55\n\nJieba segmentation mode. Optional; the default is `accurate`.\n\nIn accurate mode, Jieba splits the sentence into the most precise words using dictionary matching. This mode focuses on precision, ensuring that the segmentation is as accurate as possible.\n\nIn full mode, Jieba tries to split the sentence into every possible word combination, aiming to include all potential words. This mode focuses on maximizing recall, meaning it identifies as many words as possible, even if some of them overlap or are less commonly used. It returns all the words found in its dictionary.\n\nIn search mode, Jieba breaks the text into both whole words and smaller parts, combining precise segmentation with extra detail by providing overlapping word fragments. This mode balances precision and recall, making it useful for search engines.\n\n`jieba_mode` should be used with `morphology = jieba_chinese`. See [Chinese, Japanese, Korean (CJK) and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_56\n\n<!-- request JSON -->\n\nCODE_BLOCK_57\n\n<!-- request PHP -->\n\nCODE_BLOCK_58\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_64\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_65\n\n<!-- end -->\n\n## jieba_user_dict_path\n\n<!-- example jieba_user_dict_path -->\n\nCODE_BLOCK_66\n\nPath to the Jieba user dictionary. Optional.\n\nJieba, a Chinese text segmentation library, uses dictionary files to assist with word segmentation. The format of these dictionary files is as follows: each line contains a word, split into three parts separated by spaces — the word itself, word frequency, and part of speech (POS) tag. The word frequency and POS tag are optional and can be omitted. The dictionary file must be UTF-8 encoded.\n\nExample:\n\nCODE_BLOCK_67\n\n`jieba_user_dict_path` should be used with `morphology = jieba_chinese`. For more details, see [Chinese, Japanese, Korean (CJK), and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_77\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_51\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_52\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_53\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_54\n\n<!-- end -->\n\n## jieba_mode\n\n<!-- example jieba_mode -->\n\nCODE_BLOCK_55\n\nJieba segmentation mode. Optional; the default is `accurate`.\n\nIn accurate mode, Jieba splits the sentence into the most precise words using dictionary matching. This mode focuses on precision, ensuring that the segmentation is as accurate as possible.\n\nIn full mode, Jieba tries to split the sentence into every possible word combination, aiming to include all potential words. This mode focuses on maximizing recall, meaning it identifies as many words as possible, even if some of them overlap or are less commonly used. It returns all the words found in its dictionary.\n\nIn search mode, Jieba breaks the text into both whole words and smaller parts, combining precise segmentation with extra detail by providing overlapping word fragments. This mode balances precision and recall, making it useful for search engines.\n\n`jieba_mode` should be used with `morphology = jieba_chinese`. See [Chinese, Japanese, Korean (CJK) and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_56\n\n<!-- request JSON -->\n\nCODE_BLOCK_57\n\n<!-- request PHP -->\n\nCODE_BLOCK_58\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_59\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_60\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_61\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_62\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_63\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_64\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_65\n\n<!-- end -->\n\n## jieba_user_dict_path\n\n<!-- example jieba_user_dict_path -->\n\nCODE_BLOCK_66\n\nPath to the Jieba user dictionary. Optional.\n\nJieba, a Chinese text segmentation library, uses dictionary files to assist with word segmentation. The format of these dictionary files is as follows: each line contains a word, split into three parts separated by spaces — the word itself, word frequency, and part of speech (POS) tag. The word frequency and POS tag are optional and can be omitted. The dictionary file must be UTF-8 encoded.\n\nExample:\n\nCODE_BLOCK_67\n\n`jieba_user_dict_path` should be used with `morphology = jieba_chinese`. For more details, see [Chinese, Japanese, Korean (CJK), and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_68\n\n<!-- request JSON -->\n\nCODE_BLOCK_69\n\n<!-- request PHP -->\n\nCODE_BLOCK_70\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_71\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_72\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_73\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_74\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_75\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_76\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_77\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": true
  },
  "dcdc816f31d72cddfc520716e431649aa3c5274d655800b80bf9b1c455c1de01": {
    "original": "# Advanced morphology\n\nMorphology preprocessors can be applied to words during indexing to normalize different forms of the same word and improve segmentation. For example, an English stemmer can normalize \"dogs\" and \"dog\" to \"dog\", resulting in identical search results for both keywords.\n\nManticore has four built-in morphology preprocessors:\n\n*   **Lemmatizer**: reduces a word to its root or lemma. For example, \"running\" can be reduced to \"run\" and \"octopi\" can be reduced to \"octopus\". Note that some words may have multiple corresponding root forms. For example, \"dove\" can be either the past tense of \"dive\" or a noun meaning a bird, as in \"A white dove flew over the cuckoo's nest.\" In this case, a lemmatizer can generate all the possible root forms.\n\n*   **Stemmer**: reduces a word to its stem by removing or replacing certain known suffixes. The resulting stem may not necessarily be a valid word. For example, the Porter English stemmer reduces \"running\" to \"run\", \"business\" to \"busi\" (not a valid word), and does not reduce \"octopi\" at all.\n\n*   **Phonetic algorithms**: replace words with phonetic codes that are the same even if the words are different but phonetically close.\n\n*   **Word breaking algorithms**: split text into words. Currently available only for Chinese.\n\n## morphology\n\nCODE_BLOCK_0\n\n<!-- example morphology -->\n\nThe morphology directive specifies a list of morphology preprocessors to apply to the words being indexed. This is an optional setting, with the default being no preprocessor applied.\n\nManticore comes with built-in morphological preprocessors for:\n\n* English, Russian, and German lemmatizers\n\n* English, Russian, Arabic, and Czech stemmers\n\n* SoundEx and MetaPhone phonetic algorithms\n\n* Chinese word breaking algorithm\n\n* Snowball (libstemmer) stemmers for more than [15 other languages](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md) are also available.\n\nLemmatizers require dictionary `.pak` files that can be installed using the `manticore-language-packs` packages or [downloaded from the Manticore website](https://manticoresearch.com/install/#other-downloads). In the latter case the dictionaries need to be put in the directory specified by [lemmatizer_base](../../Server_settings/Common.md#lemmatizer_base).\n\nAdditionally, the [lemmatizer_cache](../../Data_creation_and_modification/Adding_data_from_external_storages/Plain_tables_creation.md#lemmatizer_cache) setting can be used to speed up lemmatizing by spending more RAM for an uncompressed dictionary cache.\n\nThe Chinese language segmentation can be done using [ICU](http://site.icu-project.org/) or [Jieba](https://github.com/yanyiwu/cppjieba) (requires package `manticore-language-packs`). Both libraries provide more accurate segmentation than n-grams, but are slightly slower. The [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) must include all Chinese characters, which can be done using the `cont`, `cjk` or `chinese` character sets. When you set `morphology=icu_chinese` or `morphology=jieba_chinese`, the documents are first pre-processed by ICU or Jieba. Then, the tokenizer processes the result according to the charset_table, and finally, other morphology processors from the `morphology` option are applied. Only those parts of the text that contain Chinese are passed to ICU/Jieba for segmentation, while the other parts can be modified by different means such as different morphologies or `charset_table`.\n\nBuilt-in English and Russian stemmers are faster than their libstemmer counterparts but may produce slightly different results\n\nSoundex implementation matches that of MySQL. Metaphone implementation is based on Double Metaphone algorithm and indexes the primary code.\n\nTo use the `morphology` option, specify one or multiple of the built-in options, including:\n\n* none: do not perform any morphology processing\n\n* lemmatize_ru - apply Russian lemmatizer and pick a single root form\n\n* lemmatize_uk - apply Ukrainian lemmatizer and pick a single root form (install it first in [Centos](../../Installation/RHEL_and_Centos.md#Ukrainian-lemmatizer) or [Ubuntu/Debian](../../Installation/Debian_and_Ubuntu.md#Ukrainian-lemmatizer)). For correct work of the lemmatizer make sure specific Ukrainian characters are preserved in your `charset_table` since by default they are not. For that override them, like this: `charset_table='non_cont,U+0406->U+0456,U+0456,U+0407->U+0457,U+0457,U+0490->U+0491,U+0491'`. [Here](https://play.manticoresearch.com/ua-lemmatizer/) is an interactive course about how to install and use the urkainian lemmatizer.\n\n* lemmatize_en - apply English lemmatizer and pick a single root form\n\n* lemmatize_de - apply German lemmatizer and pick a single root form\n\n* lemmatize_ru_all - apply Russian lemmatizer and index all possible root forms\n\n* lemmatize_uk_all - apply Ukrainian lemmatizer and index all possible root forms. Find the installation links above and take care of the `charset_table`.\n\n* lemmatize_en_all - apply English lemmatizer and index all possible root forms\n\n* lemmatize_de_all - apply German lemmatizer and index all possible root forms\n\n* stem_en - apply Porter's English stemmer\n\n* stem_ru - apply Porter's Russian stemmer\n\n* stem_enru - apply Porter's English and Russian stemmers\n\n* stem_cz - apply Czech stemmer\n\n* stem_ar - apply Arabic stemmer\n\n* soundex - replace keywords with their SOUNDEX code\n\n* metaphone - replace keywords with their METAPHONE code\n\n* icu_chinese - apply Chinese text segmentation using ICU\n\n* jieba_chinese - apply Chinese text segmentation using Jieba (requires package `manticore-language-packs`)\n\n* libstemmer_* . Refer to the [list of supported languages](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md) for details",
    "translations": {
      "chinese": "# 高级形态学\n\n形态学预处理器可以在索引期间应用于单词，以规范同一单词的不同形式并改进分词。例如，英文词干提取器可以将 \"dogs\" 和 \"dog\" 规范为 \"dog\"，从而使这两个关键词的搜索结果相同。\n\nManticore 有四种内置的形态学预处理器：\n\n*   **词形还原器**：将单词还原为其词根或词元。例如，\"running\" 可以还原为 \"run\"，\"octopi\" 可以还原为 \"octopus\"。注意，有些单词可能有多个对应的词根形式。例如，\"dove\" 既可以是 \"dive\" 的过去式，也可以是表示鸟类的名词，如 \"A white dove flew over the cuckoo's nest.\" 在这种情况下，词形还原器可以生成所有可能的词根形式。\n\n*   **词干提取器**：通过移除或替换某些已知后缀，将单词还原为词干。得到的词干不一定是有效单词。例如，Porter 英文词干提取器将 \"running\" 还原为 \"run\"，将 \"business\" 还原为 \"busi\"（不是有效单词），并且不会对 \"octopi\" 进行还原。\n\n*   **语音算法**：用语音编码替换单词，即使单词不同但发音相近，编码也相同。\n\n*   **分词算法**：将文本拆分成单词。目前仅支持中文。\n\n## morphology\n\nCODE_BLOCK_0\n\n<!-- example morphology -->\n\nmorphology 指令指定要应用于被索引单词的形态学预处理器列表。这是一个可选设置，默认不应用任何预处理器。\n\nManticore 内置了以下形态学预处理器：\n\n* 英语、俄语和德语词形还原器\n\n* 英语、俄语、阿拉伯语和捷克语词干提取器\n\n* SoundEx 和 MetaPhone 语音算法\n\n* 中文分词算法\n\n* 还提供了 Snowball（libstemmer）词干提取器，支持超过 [15 种其他语言](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md)。\n\n词形还原器需要字典 `.pak` 文件，可以通过 `manticore-language-packs` 软件包安装，或从 [Manticore 网站下载](https://manticoresearch.com/install/#other-downloads)。在后者情况下，字典需要放置在由 [lemmatizer_base](../../Server_settings/Common.md#lemmatizer_base) 指定的目录中。\n\n此外，[lemmatizer_cache](../../Data_creation_and_modification/Adding_data_from_external_storages/Plain_tables_creation.md#lemmatizer_cache) 设置可以通过使用更多内存缓存未压缩字典来加速词形还原。\n\n中文分词可以使用 [ICU](http://site.icu-project.org/) 或 [Jieba](https://github.com/yanyiwu/cppjieba)（需要 `manticore-language-packs` 包）。这两个库提供比 n-gram 更准确的分词，但速度稍慢。[charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) 必须包含所有中文字符，可以使用 `cont`、`cjk` 或 `chinese` 字符集来实现。当设置 `morphology=icu_chinese` 或 `morphology=jieba_chinese` 时，文档首先由 ICU 或 Jieba 预处理。然后，分词器根据 charset_table 处理结果，最后应用 `morphology` 选项中的其他形态学处理器。只有包含中文的文本部分会传递给 ICU/Jieba 进行分词，其他部分可以通过不同方式修改，如不同的形态学处理或 `charset_table`。\n\n内置的英语和俄语词干提取器比 libstemmer 版本更快，但可能产生略有不同的结果。\n\nSoundex 实现与 MySQL 一致。Metaphone 实现基于 Double Metaphone 算法，索引主编码。\n\n要使用 `morphology` 选项，请指定一个或多个内置选项，包括：\n\n* none：不执行任何形态学处理\n\n* lemmatize_ru - 应用俄语词形还原器并选择单一词根形式\n\n* lemmatize_uk - 应用乌克兰语词形还原器并选择单一词根形式（先在 [Centos](../../Installation/RHEL_and_Centos.md#Ukrainian-lemmatizer) 或 [Ubuntu/Debian](../../Installation/Debian_and_Ubuntu.md#Ukrainian-lemmatizer) 安装）。为确保词形还原器正常工作，请确保在 `charset_table` 中保留特定的乌克兰字符，因为默认不包含。可通过如下方式覆盖：`charset_table='non_cont,U+0406->U+0456,U+0456,U+0407->U+0457,U+0457,U+0490->U+0491,U+0491'`。[这里](https://play.manticoresearch.com/ua-lemmatizer/) 有一个关于如何安装和使用乌克兰词形还原器的交互式课程。\n\n* lemmatize_en - 应用英语词形还原器并选择单一词根形式\n\n* lemmatize_de - 应用德语词形还原器并选择单一词根形式\n\n* lemmatize_ru_all - 应用俄语词形还原器并索引所有可能的词根形式\n\n* lemmatize_uk_all - 应用乌克兰语词形还原器并索引所有可能的词根形式。安装链接见上，并注意 `charset_table` 设置。\n\n* lemmatize_en_all - 应用英语词形还原器并索引所有可能的词根形式\n\n* lemmatize_de_all - 应用德语词形还原器并索引所有可能的词根形式\n\n* stem_en - 应用 Porter 英语词干提取器\n\n* stem_ru - 应用 Porter 俄语词干提取器\n\n* stem_enru - 同时应用 Porter 英语和俄语词干提取器\n\n* stem_cz - 应用捷克语词干提取器\n\n* stem_ar - 应用阿拉伯语词干提取器\n\n* soundex - 用 SOUNDEX 代码替换关键词\n\n* metaphone - 用 METAPHONE 代码替换关键词\n\n* icu_chinese - 使用 ICU 进行中文分词\n\n* jieba_chinese - 使用 Jieba 进行中文分词（需要 `manticore-language-packs` 包）\n\n* libstemmer_* 。详情请参阅 [支持语言列表](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md)",
      "russian": "# Расширенная морфология\n\nМорфологические препроцессоры могут применяться к словам во время индексации для нормализации различных форм одного и того же слова и улучшения сегментации. Например, английский стеммер может нормализовать \"dogs\" и \"dog\" до \"dog\", что приведет к одинаковым результатам поиска для обоих ключевых слов.\n\nВ Manticore есть четыре встроенных морфологических препроцессора:\n\n*   **Лемматизатор**: сводит слово к его корню или лемме. Например, \"running\" может быть приведено к \"run\", а \"octopi\" — к \"octopus\". Обратите внимание, что некоторые слова могут иметь несколько соответствующих корневых форм. Например, \"dove\" может быть либо прошедшим временем от \"dive\", либо существительным, означающим птицу, как в \"A white dove flew over the cuckoo's nest.\" В этом случае лемматизатор может сгенерировать все возможные корневые формы.\n\n*   **Стеммер**: сводит слово к основе, удаляя или заменяя определённые известные суффиксы. Полученная основа может не быть валидным словом. Например, английский стеммер Портера сводит \"running\" к \"run\", \"business\" к \"busi\" (невалидное слово) и не изменяет \"octopi\".\n\n*   **Фонетические алгоритмы**: заменяют слова на фонетические коды, которые совпадают, даже если слова разные, но фонетически близки.\n\n*   **Алгоритмы разбиения слов**: разбивают текст на слова. В настоящее время доступны только для китайского языка.\n\n## morphology\n\nCODE_BLOCK_0\n\n<!-- example morphology -->\n\nДиректива morphology задаёт список морфологических препроцессоров, которые применяются к индексируемым словам. Это необязательная настройка, по умолчанию препроцессор не применяется.\n\nManticore поставляется со встроенными морфологическими препроцессорами для:\n\n* английского, русского и немецкого лемматизаторов\n\n* английских, русских, арабских и чешских стеммеров\n\n* фонетических алгоритмов SoundEx и MetaPhone\n\n* алгоритма разбиения слов для китайского языка\n\n* стеммеров Snowball (libstemmer) для более чем [15 других языков](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md).\n\nЛемматизаторы требуют словарные файлы `.pak`, которые можно установить с помощью пакетов `manticore-language-packs` или [скачать с сайта Manticore](https://manticoresearch.com/install/#other-downloads). В последнем случае словари нужно поместить в директорию, указанную в [lemmatizer_base](../../Server_settings/Common.md#lemmatizer_base).\n\nКроме того, настройка [lemmatizer_cache](../../Data_creation_and_modification/Adding_data_from_external_storages/Plain_tables_creation.md#lemmatizer_cache) может использоваться для ускорения лемматизации за счёт использования большего объёма оперативной памяти для кэша распакованного словаря.\n\nСегментация китайского языка может выполняться с помощью [ICU](http://site.icu-project.org/) или [Jieba](https://github.com/yanyiwu/cppjieba) (требуется пакет `manticore-language-packs`). Обе библиотеки обеспечивают более точную сегментацию, чем n-граммы, но работают немного медленнее. В [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table) должны быть включены все китайские символы, что можно сделать с помощью наборов символов `cont`, `cjk` или `chinese`. При установке `morphology=icu_chinese` или `morphology=jieba_chinese` документы сначала предварительно обрабатываются ICU или Jieba. Затем токенизатор обрабатывает результат согласно charset_table, и, наконец, применяются другие морфологические процессоры из опции `morphology`. Только те части текста, которые содержат китайские символы, передаются ICU/Jieba для сегментации, в то время как остальные части могут обрабатываться другими способами, например, разными морфологиями или `charset_table`.\n\nВстроенные английский и русский стеммеры работают быстрее своих аналогов из libstemmer, но могут давать немного отличающиеся результаты.\n\nРеализация Soundex совпадает с MySQL. Реализация Metaphone основана на алгоритме Double Metaphone и индексирует основной код.\n\nДля использования опции `morphology` укажите одну или несколько встроенных опций, включая:\n\n* none: не выполнять морфологическую обработку\n\n* lemmatize_ru - применить русский лемматизатор и выбрать одну корневую форму\n\n* lemmatize_uk - применить украинский лемматизатор и выбрать одну корневую форму (сначала установите его в [Centos](../../Installation/RHEL_and_Centos.md#Ukrainian-lemmatizer) или [Ubuntu/Debian](../../Installation/Debian_and_Ubuntu.md#Ukrainian-lemmatizer)). Для корректной работы лемматизатора убедитесь, что специфические украинские символы сохранены в вашем `charset_table`, так как по умолчанию их нет. Для этого переопределите их, например: `charset_table='non_cont,U+0406->U+0456,U+0456,U+0407->U+0457,U+0457,U+0490->U+0491,U+0491'`. [Здесь](https://play.manticoresearch.com/ua-lemmatizer/) находится интерактивный курс по установке и использованию украинского лемматизатора.\n\n* lemmatize_en - применить английский лемматизатор и выбрать одну корневую форму\n\n* lemmatize_de - применить немецкий лемматизатор и выбрать одну корневую форму\n\n* lemmatize_ru_all - применить русский лемматизатор и индексировать все возможные корневые формы\n\n* lemmatize_uk_all - применить украинский лемматизатор и индексировать все возможные корневые формы. Ссылки на установку приведены выше, обратите внимание на `charset_table`.\n\n* lemmatize_en_all - применить английский лемматизатор и индексировать все возможные корневые формы\n\n* lemmatize_de_all - применить немецкий лемматизатор и индексировать все возможные корневые формы\n\n* stem_en - применить английский стеммер Портера\n\n* stem_ru - применить русский стеммер Портера\n\n* stem_enru - применить английский и русский стеммеры Портера\n\n* stem_cz - применить чешский стеммер\n\n* stem_ar - применить арабский стеммер\n\n* soundex - заменить ключевые слова на их SOUNDEX-код\n\n* metaphone - заменить ключевые слова на их METAPHONE-код\n\n* icu_chinese - применить сегментацию китайского текста с помощью ICU\n\n* jieba_chinese - применить сегментацию китайского текста с помощью Jieba (требуется пакет `manticore-language-packs`)\n\n* libstemmer_* . Смотрите [список поддерживаемых языков](../../Creating_a_table/NLP_and_tokenization/Supported_languages.md) для подробностей"
    },
    "is_code_or_comment": false
  },
  "41b829084cc894bfae046fb7bb060810c5f1f40c3dcb19ad81ce516fae12274c": {
    "original": "Multiple stemmers can be specified, separated by commas. They will be applied to incoming words in the order they are listed, and the processing will stop once one of the stemmers modifies the word. Additionally, when [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) feature is enabled, the word will be looked up in the word forms dictionary first. If there is a matching entry in the dictionary, stemmers will not be applied at all.  [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) сan be used to implement stemming exceptions.\n\n<!-- request SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- request PHP -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_4\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_5\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_9\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n## morphology_skip_fields\n\n<!-- example morphology_skip_fields -->\n\nCODE_BLOCK_11\n\nA list of fields to skip morphology preprocessing. Optional, default is empty (apply preprocessors to all fields).\n\n<!-- request SQL -->\n\nCODE_BLOCK_12\n\n<!-- request JSON -->\n\nCODE_BLOCK_13\n\n<!-- request PHP -->\n\nCODE_BLOCK_14\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_20\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n## min_stemming_len\n\n<!-- example min_stemming_len -->\n\nCODE_BLOCK_22\n\nMinimum word length at which to enable stemming. Optional, default is 1 (stem everything).\n\nStemmers are not perfect, and might sometimes produce undesired results. For instance, running \"gps\" keyword through Porter stemmer for English results in \"gp\", which is not really the intent. `min_stemming_len` feature lets you suppress stemming based on the source word length, ie. to avoid stemming too short words. Keywords that are shorter than the given threshold will not be stemmed. Note that keywords that are exactly as long as specified **will** be stemmed. So in order to avoid stemming 3-character keywords, you should specify 4 for the value. For more finely grained control, refer to [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) feature.\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- request PHP -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asycnio -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_31\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n## index_exact_words\n\n<!-- example index_exact_words -->\n\nCODE_BLOCK_33\n\nThis option allows for the indexing of original keywords along with their morphologically modified versions. However, original keywords that are remapped by the [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) and [exceptions](../../Creating_a_table/NLP_and_tokenization/Exceptions.md) cannot be indexed. The default value is 0, indicating that this feature is disabled by default.\n\nThis allows the use of the [exact form operator](../../Searching/Full_text_matching/Operators.md#Exact-form-modifier) in the query language. Enabling this feature will increase the full-text index size and indexing time, but will not impact search performance.\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- request JSON -->\n\nCODE_BLOCK_35\n\n<!-- request PHP -->\n\nCODE_BLOCK_36\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_42\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_43\n\n<!-- end -->\n\n## jieba_hmm\n\n<!-- example jieba_hmm -->\n\nCODE_BLOCK_44\n\nEnable or disable HMM in the Jieba segmentation tool. Optional; the default is 1.\n\nIn Jieba, the HMM (Hidden Markov Model) option refers to an algorithm used for word segmentation. Specifically, it allows Jieba to perform Chinese word segmentation by recognizing unknown words, especially those not present in its dictionary.\n\nJieba primarily uses a dictionary-based method for segmenting known words, but when the HMM option is enabled, it applies a statistical model to identify probable word boundaries for words or phrases that are not in its dictionary. This is particularly useful for segmenting new or rare words, names, and slang.\n\nIn summary, the `jieba_hmm` option helps improve segmentation accuracy at the expense of indexing performance. It must be used with `morphology = jieba_chinese`, see [Chinese, Japanese and Korean (CJK) and Thai languages](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_45\n\n<!-- request JSON -->\n\nCODE_BLOCK_46\n\n<!-- request PHP -->\n\nCODE_BLOCK_47\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_50",
    "translations": {
      "chinese": "可以指定多个词干提取器，用逗号分隔。它们将按照列出的顺序应用于输入的单词，并且一旦其中一个词干提取器修改了单词，处理将停止。此外，当启用[wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms)功能时，单词将首先在词形字典中查找。如果字典中有匹配的条目，则根本不会应用词干提取器。[wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms)可用于实现词干提取的例外情况。\n\n<!-- request SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- request PHP -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_4\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_5\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_9\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n## morphology_skip_fields\n\n<!-- example morphology_skip_fields -->\n\nCODE_BLOCK_11\n\n跳过形态学预处理的字段列表。可选，默认值为空（对所有字段应用预处理器）。\n\n<!-- request SQL -->\n\nCODE_BLOCK_12\n\n<!-- request JSON -->\n\nCODE_BLOCK_13\n\n<!-- request PHP -->\n\nCODE_BLOCK_14\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_20\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n## min_stemming_len\n\n<!-- example min_stemming_len -->\n\nCODE_BLOCK_22\n\n启用词干提取的最小单词长度。可选，默认值为1（对所有单词进行词干提取）。\n\n词干提取器并不完美，有时可能产生不理想的结果。例如，将“gps”关键词通过英语的Porter词干提取器处理，结果是“gp”，这并非真正的意图。`min_stemming_len`功能允许您根据源单词长度抑制词干提取，即避免对过短的单词进行词干提取。比给定阈值更短的关键词将不会被词干提取。注意，长度恰好等于指定值的关键词**会**被词干提取。因此，为了避免对3个字符的关键词进行词干提取，您应将值指定为4。有关更细粒度的控制，请参阅[wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms)功能。\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- request PHP -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asycnio -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_31\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n## index_exact_words\n\n<!-- example index_exact_words -->\n\nCODE_BLOCK_33\n\n此选项允许对原始关键词及其形态学修改版本进行索引。但是，被[wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms)和[exceptions](../../Creating_a_table/NLP_and_tokenization/Exceptions.md)重新映射的原始关键词无法被索引。默认值为0，表示默认情况下此功能被禁用。\n\n这允许在查询语言中使用[精确形式操作符](../../Searching/Full_text_matching/Operators.md#Exact-form-modifier)。启用此功能将增加全文索引的大小和索引时间，但不会影响搜索性能。\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- request JSON -->\n\nCODE_BLOCK_35\n\n<!-- request PHP -->\n\nCODE_BLOCK_36\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_42\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_43\n\n<!-- end -->\n\n## jieba_hmm\n\n<!-- example jieba_hmm -->\n\nCODE_BLOCK_44\n\n启用或禁用Jieba分词工具中的HMM。可选，默认值为1。\n\n在Jieba中，HMM（隐马尔可夫模型）选项指的是一种用于分词的算法。具体来说，它允许Jieba通过识别未知词，尤其是词典中不存在的词，来执行中文分词。\n\nJieba主要使用基于词典的方法来分割已知词，但当启用HMM选项时，它会应用统计模型来识别词典中不存在的词或短语的可能词边界。这对于分割新词、罕见词、姓名和俚语特别有用。\n\n总之，`jieba_hmm`选项有助于提高分词准确性，但会牺牲索引性能。它必须与`morphology = jieba_chinese`一起使用，详见[中文、日文和韩文（CJK）及泰语](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md)。\n\n<!-- request SQL -->\n\nCODE_BLOCK_45\n\n<!-- request JSON -->\n\nCODE_BLOCK_46\n\n<!-- request PHP -->\n\nCODE_BLOCK_47\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_50",
      "russian": "Несколько стеммеров могут быть указаны через запятую. Они будут применяться к входящим словам в том порядке, в котором перечислены, и обработка остановится, как только один из стеммеров изменит слово. Кроме того, когда включена функция [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms), слово сначала будет искаться в словаре словоформ. Если в словаре есть совпадающая запись, стеммеры применяться не будут.  [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) можно использовать для реализации исключений стемминга.\n\n<!-- request SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- request PHP -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_4\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_5\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_6\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_8\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_9\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n## morphology_skip_fields\n\n<!-- example morphology_skip_fields -->\n\nCODE_BLOCK_11\n\nСписок полей, для которых следует пропустить морфологическую предобработку. Необязательно, по умолчанию пусто (применять препроцессоры ко всем полям).\n\n<!-- request SQL -->\n\nCODE_BLOCK_12\n\n<!-- request JSON -->\n\nCODE_BLOCK_13\n\n<!-- request PHP -->\n\nCODE_BLOCK_14\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_17\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_20\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n## min_stemming_len\n\n<!-- example min_stemming_len -->\n\nCODE_BLOCK_22\n\nМинимальная длина слова, при которой включается стемминг. Необязательно, по умолчанию 1 (стеммить всё).\n\nСтеммеры не идеальны и иногда могут давать нежелательные результаты. Например, применение стеммера Porter для английского к ключевому слову \"gps\" приводит к \"gp\", что не соответствует намерению. Функция `min_stemming_len` позволяет подавить стемминг в зависимости от длины исходного слова, то есть избежать стемминга слишком коротких слов. Ключевые слова, короче заданного порога, стеммиться не будут. Обратите внимание, что ключевые слова, длина которых ровно равна указанному значению, **будут** стеммиться. Поэтому, чтобы избежать стемминга ключевых слов из 3 символов, следует указать значение 4. Для более тонкой настройки используйте функцию [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms).\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- request PHP -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asycnio -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_28\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_31\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n## index_exact_words\n\n<!-- example index_exact_words -->\n\nCODE_BLOCK_33\n\nЭта опция позволяет индексировать оригинальные ключевые слова вместе с их морфологически изменёнными версиями. Однако оригинальные ключевые слова, которые переназначены с помощью [wordforms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md#wordforms) и [exceptions](../../Creating_a_table/NLP_and_tokenization/Exceptions.md), индексироваться не могут. Значение по умолчанию — 0, что означает, что эта функция отключена по умолчанию.\n\nЭто позволяет использовать [оператор точной формы](../../Searching/Full_text_matching/Operators.md#Exact-form-modifier) в языке запросов. Включение этой функции увеличит размер полнотекстового индекса и время индексации, но не повлияет на производительность поиска.\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- request JSON -->\n\nCODE_BLOCK_35\n\n<!-- request PHP -->\n\nCODE_BLOCK_36\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_37\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_38\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### Java:\n\n<!-- request Java -->\n\nCODE_BLOCK_40\n\n<!-- intro -->\n\n##### C#:\n\n<!-- request C# -->\n\nCODE_BLOCK_41\n\n<!-- intro -->\n\n##### Rust:\n\n<!-- request Rust -->\n\nCODE_BLOCK_42\n\n<!-- request CONFIG -->\n\nCODE_BLOCK_43\n\n<!-- end -->\n\n## jieba_hmm\n\n<!-- example jieba_hmm -->\n\nCODE_BLOCK_44\n\nВключить или отключить HMM в инструменте сегментации Jieba. Необязательно; значение по умолчанию — 1.\n\nВ Jieba опция HMM (Hidden Markov Model — скрытая марковская модель) относится к алгоритму, используемому для сегментации слов. В частности, она позволяет Jieba выполнять сегментацию китайских слов, распознавая неизвестные слова, особенно те, которых нет в его словаре.\n\nJieba в основном использует метод на основе словаря для сегментации известных слов, но при включённой опции HMM применяется статистическая модель для определения вероятных границ слов для слов или фраз, отсутствующих в словаре. Это особенно полезно для сегментации новых или редких слов, имён и сленга.\n\nВ итоге опция `jieba_hmm` помогает улучшить точность сегментации за счёт производительности индексации. Она должна использоваться вместе с `morphology = jieba_chinese`, см. [Китайский, японский и корейский (CJK) и тайский языки](../../Creating_a_table/NLP_and_tokenization/Languages_with_continuous_scripts.md).\n\n<!-- request SQL -->\n\nCODE_BLOCK_45\n\n<!-- request JSON -->\n\nCODE_BLOCK_46\n\n<!-- request PHP -->\n\nCODE_BLOCK_47\n\n<!-- intro -->\n\n##### Python:\n\n<!-- request Python -->\n\nCODE_BLOCK_48\n\n<!-- intro -->\n\n##### Python-asyncio:\n\n<!-- request Python-asyncio -->\n\nCODE_BLOCK_49\n\n<!-- intro -->\n\n##### Javascript:\n\n<!-- request javascript -->\n\nCODE_BLOCK_50"
    },
    "is_code_or_comment": false
  }
}
