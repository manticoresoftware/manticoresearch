{
  "f8d214603912a97fe33b8464010380da42f046e8d069e7be94367fca57abf6e3": {
    "original": "# Token filter plugins\n\nToken filter plugins allow you to implement a custom tokenizer that creates tokens according to custom rules. There are two types:\n\n* Index-time tokenizer declared by [index_token_filter](../../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#index_token_filter) in table settings\n\n* Query-time tokenizer declared by [token_filter](../../../Searching/Options.md#token_filter) OPTION directive\n\nIn the text processing pipeline, token filters will run after the base tokenizer processing occurs (which processes the text from fields or queries and creates tokens out of them).\n\n## Index-time tokenizer\n\nIndex-time tokenizer is created by `indexer` when indexing source data into a table or by an RT table when processing `INSERT` or `REPLACE` statements.\n\nPlugin is declared as `library name:plugin name:optional string of settings`. The init functions of the plugin can accept arbitrary settings that can be passed as a string in the format `option1=value1;option2=value2;..`.\n\nExample:\n\nCODE_BLOCK_0\n\nThe call workflow for index-time token filter is as follows:\n\n1.  `XXX_init()` gets called right after `indexer` creates token filter with an empty fields list and then after indexer gets the table schema with the actual fields list. It must return zero for successful initialization or an error description otherwise.\n\n2.  `XXX_begin_document` gets called only for RT table `INSERT`/`REPLACE` for every document. It must return zero for a successful call or an error description otherwise. Using OPTION `token_filter_options`, additional parameters/settings can be passed to the function.\n\n    ```sql\n\n    INSERT INTO rt (id, title) VALUES (1, 'some text corp@space.io') OPTION token_filter_options='.io'\n\n    ```\n\n3.  `XXX_begin_field` gets called once for each field prior to processing the field with the base tokenizer, with the field number as its parameter.\n\n4.  `XXX_push_token` gets called once for each new token produced by the base tokenizer, with the source token as its parameter. It must return the token, count of extra tokens made by the token filter, and delta position for the token.\n\n5.  `XXX_get_extra_token` gets called multiple times in case `XXX_push_token` reports extra tokens. It must return the token and delta position for that extra token.\n\n6.  `XXX_end_field` gets called once right after the source tokens from the current field are processed.\n\n7.  `XXX_deinit` gets called at the very end of indexing.\n\nThe following functions are mandatory to be defined: `XXX_begin_document`, `XXX_push_token`, and `XXX_get_extra_token`.\n\n## query-time token filter\n\nQuery-time tokenizer gets created on search each time full-text is invoked by every table involved.\n\nThe call workflow for query-time token filter is as follows:\n\n1.  `XXX_init()` gets called once per table prior to parsing the query with parameters - max token length and a string set by the `token_filter` option\n\n    ```sql\n\n    SELECT * FROM index WHERE MATCH ('test') OPTION token_filter='my_lib.so:query_email_process:io'\n\n    ```\n\n    It must return zero for successful initialization or error description otherwise.\n\n2.  `XXX_push_token()` gets called once for each new token produced by the base tokenizer with parameters: token produced by the base tokenizer, pointer to raw token at source query string, and raw token length. It must return the token and delta position for the token.\n\n3.  `XXX_pre_morph()` gets called once for the token right before it gets passed to the morphology processor with a reference to the token and stopword flag. It might set the stopword flag to mark the token as a stopword.\n\n4.  `XXX_post_morph()` gets called once for the token after it is processed by the morphology processor with a reference to the token and stopword flag. It might set the stopword flag to mark the token as a stopword. It must return a flag, the non-zero value of which means to use the token prior to morphology processing.\n\n5.  `XXX_deinit()` gets called at the very end of query processing.\n\nAbsence of the functions is tolerated.\n\n<!-- proofread -->",
    "translations": {
      "chinese": "# 令牌过滤器插件\n\n令牌过滤器插件允许您实现一个自定义的分词器，根据自定义规则创建令牌。有两种类型：\n\n* 由表设置中的 [index_token_filter](../../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#index_token_filter) 声明的索引时分词器\n\n* 由 [token_filter](../../../Searching/Options.md#token_filter) OPTION 指令声明的查询时分词器\n\n在文本处理管道中，令牌过滤器将在基础分词器处理（处理字段或查询中的文本并从中创建令牌）之后运行。\n\n## 索引时分词器\n\n索引时分词器由 `indexer` 在将源数据索引到表中时创建，或者由 RT 表在处理 `INSERT` 或 `REPLACE` 语句时创建。\n\n插件声明格式为 `library name:plugin name:optional string of settings`。插件的初始化函数可以接受任意设置，这些设置可以作为字符串传递，格式为 `option1=value1;option2=value2;..`。\n\n示例：\n\nCODE_BLOCK_0\n\n索引时令牌过滤器的调用流程如下：\n\n1.  `XXX_init()` 在 `indexer` 创建令牌过滤器时调用，初始时字段列表为空，随后在 `indexer` 获取表模式及实际字段列表后再次调用。它必须返回零表示初始化成功，否则返回错误描述。\n\n2.  `XXX_begin_document` 仅在 RT 表的 `INSERT`/`REPLACE` 操作中为每个文档调用。它必须返回零表示调用成功，否则返回错误描述。通过 OPTION `token_filter_options`，可以向函数传递额外的参数/设置。\n\n    ```sql\n\n    INSERT INTO rt (id, title) VALUES (1, 'some text corp@space.io') OPTION token_filter_options='.io'\n\n    ```\n\n3.  `XXX_begin_field` 在处理字段的基础分词器之前为每个字段调用一次，参数为字段编号。\n\n4.  `XXX_push_token` 为基础分词器产生的每个新令牌调用一次，参数为源令牌。它必须返回令牌、令牌过滤器产生的额外令牌数量以及该令牌的位置增量。\n\n5.  `XXX_get_extra_token` 在 `XXX_push_token` 报告有额外令牌时调用多次。它必须返回该额外令牌及其位置增量。\n\n6.  `XXX_end_field` 在当前字段的源令牌处理完毕后调用一次。\n\n7.  `XXX_deinit` 在索引结束时调用。\n\n必须定义以下函数：`XXX_begin_document`、`XXX_push_token` 和 `XXX_get_extra_token`。\n\n## 查询时令牌过滤器\n\n查询时分词器在每次全文搜索时为每个涉及的表创建。\n\n查询时令牌过滤器的调用流程如下：\n\n1.  `XXX_init()` 在解析查询之前为每个表调用一次，参数为最大令牌长度和由 `token_filter` 选项设置的字符串\n\n    ```sql\n\n    SELECT * FROM index WHERE MATCH ('test') OPTION token_filter='my_lib.so:query_email_process:io'\n\n    ```\n\n    它必须返回零表示初始化成功，否则返回错误描述。\n\n2.  `XXX_push_token()` 为基础分词器产生的每个新令牌调用一次，参数为基础分词器产生的令牌、指向源查询字符串中原始令牌的指针以及原始令牌长度。它必须返回令牌及该令牌的位置增量。\n\n3.  `XXX_pre_morph()` 在令牌传递给形态学处理器之前调用一次，参数为令牌引用和停用词标志。它可以设置停用词标志以将令牌标记为停用词。\n\n4.  `XXX_post_morph()` 在令牌经过形态学处理器处理后调用一次，参数为令牌引用和停用词标志。它可以设置停用词标志以将令牌标记为停用词。它必须返回一个标志，非零值表示使用形态学处理之前的令牌。\n\n5.  `XXX_deinit()` 在查询处理结束时调用。\n\n缺少这些函数是允许的。\n\n<!-- proofread -->",
      "russian": "# Плагины фильтров токенов\n\nПлагины фильтров токенов позволяют реализовать пользовательский токенизатор, который создает токены согласно пользовательским правилам. Существует два типа:\n\n* Токенизатор во время индексации, объявляемый с помощью [index_token_filter](../../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#index_token_filter) в настройках таблицы\n\n* Токенизатор во время запроса, объявляемый с помощью директивы OPTION [token_filter](../../../Searching/Options.md#token_filter)\n\nВ конвейере обработки текста фильтры токенов запускаются после базовой обработки токенизатором (которая обрабатывает текст из полей или запросов и создает из них токены).\n\n## Токенизатор во время индексации\n\nТокенизатор во время индексации создается `indexer` при индексации исходных данных в таблицу или RT таблицей при обработке операторов `INSERT` или `REPLACE`.\n\nПлагин объявляется как `library name:plugin name:optional string of settings`. Функции инициализации плагина могут принимать произвольные настройки, которые передаются в виде строки в формате `option1=value1;option2=value2;..`.\n\nПример:\n\nCODE_BLOCK_0\n\nПоследовательность вызовов для фильтра токенов во время индексации следующая:\n\n1.  `XXX_init()` вызывается сразу после того, как `indexer` создает фильтр токенов с пустым списком полей, а затем после того, как indexer получает схему таблицы с актуальным списком полей. Должен возвращать ноль при успешной инициализации или описание ошибки в противном случае.\n\n2.  `XXX_begin_document` вызывается только для RT таблицы при `INSERT`/`REPLACE` для каждого документа. Должен возвращать ноль при успешном вызове или описание ошибки в противном случае. С помощью OPTION `token_filter_options` можно передать дополнительные параметры/настройки функции.\n\n    ```sql\n\n    INSERT INTO rt (id, title) VALUES (1, 'some text corp@space.io') OPTION token_filter_options='.io'\n\n    ```\n\n3.  `XXX_begin_field` вызывается один раз для каждого поля перед обработкой поля базовым токенизатором, с номером поля в качестве параметра.\n\n4.  `XXX_push_token` вызывается один раз для каждого нового токена, созданного базовым токенизатором, с исходным токеном в качестве параметра. Должен возвращать токен, количество дополнительных токенов, созданных фильтром токенов, и дельту позиции для токена.\n\n5.  `XXX_get_extra_token` вызывается несколько раз, если `XXX_push_token` сообщает о дополнительных токенах. Должен возвращать токен и дельту позиции для этого дополнительного токена.\n\n6.  `XXX_end_field` вызывается один раз сразу после обработки исходных токенов из текущего поля.\n\n7.  `XXX_deinit` вызывается в самом конце индексации.\n\nОбязательными для определения являются функции: `XXX_begin_document`, `XXX_push_token` и `XXX_get_extra_token`.\n\n## Фильтр токенов во время запроса\n\nТокенизатор во время запроса создается при каждом поиске полнотекстового поиска для каждой задействованной таблицы.\n\nПоследовательность вызовов для фильтра токенов во время запроса следующая:\n\n1.  `XXX_init()` вызывается один раз для каждой таблицы перед разбором запроса с параметрами — максимальная длина токена и строка, заданная опцией `token_filter`\n\n    ```sql\n\n    SELECT * FROM index WHERE MATCH ('test') OPTION token_filter='my_lib.so:query_email_process:io'\n\n    ```\n\n    Должен возвращать ноль при успешной инициализации или описание ошибки в противном случае.\n\n2.  `XXX_push_token()` вызывается один раз для каждого нового токена, созданного базовым токенизатором, с параметрами: токен, созданный базовым токенизатором, указатель на необработанный токен в исходной строке запроса и длина необработанного токена. Должен возвращать токен и дельту позиции для токена.\n\n3.  `XXX_pre_morph()` вызывается один раз для токена непосредственно перед передачей его морфологическому процессору с ссылкой на токен и флагом стоп-слова. Может установить флаг стоп-слова, чтобы пометить токен как стоп-слово.\n\n4.  `XXX_post_morph()` вызывается один раз для токена после обработки морфологическим процессором с ссылкой на токен и флагом стоп-слова. Может установить флаг стоп-слова, чтобы пометить токен как стоп-слово. Должен возвращать флаг, ненулевое значение которого означает использование токена до морфологической обработки.\n\n5.  `XXX_deinit()` вызывается в самом конце обработки запроса.\n\nОтсутствие функций допускается.\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  }
}
