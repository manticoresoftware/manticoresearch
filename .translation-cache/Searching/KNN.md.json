{
  "c14e77f862f465fc16e7d995e9a989b5cb1f759468502db27d3e2ecc26e60865": {
    "original": "Finding documents similar to a specific one based on its unique ID is a common task. For instance, when a user views a particular item, Manticore Search can efficiently identify and display a list of items that are most similar to it in the vector space. Here's how you can do it:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `document id`: Document ID for KNN similarity search.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Filtering KNN vector search results\n\nManticore also supports additional filtering of documents returned by the KNN search, either by full-text matching, attribute filters, or both.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "基于特定文档的唯一ID查找相似文档是一项常见任务。例如，当用户查看某个特定项目时，Manticore Search 可以高效地识别并显示在向量空间中与其最相似的项目列表。操作方法如下：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性的名称。\n\n* `k`：表示返回的文档数量，是分层可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。然而，最终结果中包含的文档实际数量可能会有所不同。例如，如果系统处理的是分割成磁盘块的实时表，每个块可能返回 `k` 个文档，导致总数超过指定的 `k`（因为累计数量为 `num_chunks * k`）。另一方面，如果在请求了 `k` 个文档后，根据特定属性过滤掉了一些文档，最终文档数量可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程不同，因此 `k` 参数对返回文档数量的影响不适用。\n\n* `document id`：用于 KNN 相似度搜索的文档ID。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### 过滤 KNN 向量搜索结果\n\nManticore 还支持通过全文匹配、属性过滤或两者结合，对 KNN 搜索返回的文档进行额外过滤。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "Нахождение документов, похожих на конкретный, на основе его уникального идентификатора — распространённая задача. Например, когда пользователь просматривает определённый элемент, Manticore Search может эффективно определить и отобразить список элементов, наиболее похожих на него в векторном пространстве. Вот как это можно сделать:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: Это имя атрибута с плавающей точкой, содержащего векторные данные.\n\n* `k`: Это количество документов для возврата и ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Он указывает количество документов, которые должен вернуть один индекс HNSW. Однако фактическое количество документов в итоговых результатах может варьироваться. Например, если система работает с таблицами в реальном времени, разделёнными на дисковые чанки, каждый чанк может вернуть `k` документов, что приведёт к общему числу, превышающему указанное `k` (так как суммарное количество будет `num_chunks * k`). С другой стороны, итоговое количество документов может быть меньше `k`, если после запроса `k` документов некоторые из них отфильтровываются по определённым атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс извлечения работает иначе, и поэтому параметр `k` не влияет на количество возвращаемых документов.\n\n* `document id`: Идентификатор документа для поиска по сходству KNN.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Фильтрация результатов KNN векторного поиска\n\nManticore также поддерживает дополнительную фильтрацию документов, возвращаемых поиском KNN, либо по полнотекстовому совпадению, фильтрам атрибутов, либо по обоим критериям.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  },
  "b76489316d062d2cff359f5cc6e5029ba2ab1fa7c44f128712086b53b0c9d562": {
    "original": "# K-nearest neighbor vector search\n\nManticore Search supports the ability to add embeddings generated by Machine Learning models to each document, and then doing a nearest-neighbor search on them. This lets you build features like similarity search, recommendations, semantic search, and relevance ranking based on NLP algorithms, among others, including image, video, and sound searches.\n\n## What is an embedding?\n\nAn embedding is a method of representing data - such as text, images, or sound - as vectors in a high-dimensional space. These vectors are crafted to ensure that the distance between them reflects the similarity of the data they represent. This process typically employs algorithms like word embeddings (e.g., Word2Vec, BERT) for text or neural networks for images. The high-dimensional nature of the vector space, with many components per vector, allows for the representation of complex and nuanced relationships between items. Their similarity is gauged by the distance between these vectors, often measured using methods like Euclidean distance or cosine similarity.\n\nManticore Search enables k-nearest neighbor (KNN) vector searches using the HNSW library. This functionality is part of the [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Configuring a table for KNN search\n\nTo run KNN searches, you must first configure your table. Float vectors and KNN search are only supported in real-time tables (not in plain tables). The table needs to have at least one [float_vector](../Creating_a_table/Data_types.md#Float-vector) attribute, which serves as a data vector. You need to specify the following properties:\n\n* `knn_type`: A mandatory setting; currently, only `hnsw` is supported.\n\n* `knn_dims`: A mandatory setting that specifies the dimensions of the vectors being indexed.\n\n* `hnsw_similarity`: A mandatory setting that specifies the distance function used by the HNSW index. Acceptable values are:\n\n  - `L2` - Squared L2\n\n  - `IP` - Inner product\n\n  - `COSINE` - Cosine similarity\n\n  \n\n  **Note:** When using `COSINE` similarity, vectors are automatically normalized upon insertion. This means the stored vector values may differ from the original input values, as they will be converted to unit vectors (vectors with a mathematical length/magnitude of 1.0) to enable efficient cosine similarity calculations. This normalization preserves the direction of the vector while standardizing its length.\n\n* `hnsw_m`: An optional setting that defines the maximum number of outgoing connections in the graph. The default is 16.\n\n* `hnsw_ef_construction`: An optional setting that defines a construction time/accuracy trade-off.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Plain mode (using configuration file):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Inserting vector data\n\n#### Auto Embeddings (Recommended)\n\nThe easiest way to work with vector data is using **auto embeddings**. With this feature, you create a table with `MODEL_NAME` and `FROM` parameters, then simply insert your text data - Manticore automatically generates embeddings for you.\n\n##### Creating a table with auto embeddings\n\nWhen creating a table for auto embeddings, specify:\n\n- `MODEL_NAME`: The embedding model to use\n\n- `FROM`: Which fields to use for embedding generation (empty means all text/string fields)\n\n**Supported embedding models:**\n\n- **Sentence Transformers**: Any [suitable BERT-based Hugging Face model](https://huggingface.co/sentence-transformers/models) (e.g., `sentence-transformers/all-MiniLM-L6-v2`) — no API key needed. Manticore downloads the model when you create the table.\n\n- **OpenAI**: OpenAI embedding models like `openai/text-embedding-ada-002` - requires `API_KEY='<OPENAI_API_KEY>'` parameter\n\n- **Voyage**: Voyage AI embedding models - requires `API_KEY='<VOYAGE_API_KEY>'` parameter\n\n- **Jina**: Jina AI embedding models - requires `API_KEY='<JINA_API_KEY>'` parameter\n\nMore information about setting up a `float_vector` attribute can be found [here](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nUsing sentence-transformers (no API key needed)\n\nCODE_BLOCK_3\n\nUsing OpenAI (requires API_KEY parameter)\n\nCODE_BLOCK_4\n\nUsing all text fields for embeddings (FROM is empty)\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### Inserting data with auto embeddings\n\n<!-- example inserting_embeddings -->\n\nWhen using auto embeddings, **do not specify the vector field** in your INSERT statement. The embeddings are generated automatically from the text fields specified in the `FROM` parameter.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nInsert text data only - embeddings generated automatically\n\nCODE_BLOCK_6\n\nInsert multiple fields - both used for embedding if FROM='title,description'  \n\nCODE_BLOCK_7\n\nInsert empty vector (document excluded from vector search)\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### Searching with auto embeddings\n\n<!-- example embeddings_search -->\n\nSearch works the same way - provide your query text and Manticore will generate embeddings and find similar documents:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nUsing text query with auto-embeddings\n\nCODE_BLOCK_11\n\nUsing vector query directly\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.",
    "translations": {
      "chinese": "# K-近邻向量搜索\n\nManticore Search 支持将由机器学习模型生成的嵌入向量添加到每个文档中，然后对它们进行最近邻搜索。这使您能够构建诸如相似性搜索、推荐、语义搜索和基于自然语言处理算法的相关性排序等功能，还包括图像、视频和声音搜索。\n\n## 什么是嵌入向量？\n\n嵌入向量是一种表示数据（如文本、图像或声音）的方法，将其表示为高维空间中的向量。这些向量被设计成使它们之间的距离反映所代表数据的相似性。该过程通常采用诸如词嵌入（例如 Word2Vec、BERT）用于文本，或神经网络用于图像的算法。向量空间的高维特性，每个向量包含多个分量，允许表示项目之间复杂且细微的关系。它们的相似性通过这些向量之间的距离来衡量，通常使用欧几里得距离或余弦相似度等方法。\n\nManticore Search 使用 HNSW 库实现 k-近邻（KNN）向量搜索。此功能是 [Manticore Columnar Library](https://github.com/manticoresoftware/columnar) 的一部分。\n\n<!-- example KNN -->\n\n### 配置用于 KNN 搜索的表\n\n要运行 KNN 搜索，您必须先配置表。浮点向量和 KNN 搜索仅支持实时表（不支持普通表）。表需要至少有一个 [float_vector](../Creating_a_table/Data_types.md#Float-vector) 属性，作为数据向量。您需要指定以下属性：\n\n* `knn_type`：必填设置；目前仅支持 `hnsw`。\n\n* `knn_dims`：必填设置，指定被索引向量的维度。\n\n* `hnsw_similarity`：必填设置，指定 HNSW 索引使用的距离函数。可接受的值有：\n\n  - `L2` - 平方 L2 距离\n\n  - `IP` - 内积\n\n  - `COSINE` - 余弦相似度\n\n  \n\n  **注意：** 使用 `COSINE` 相似度时，向量在插入时会自动归一化。这意味着存储的向量值可能与原始输入值不同，因为它们会被转换为单位向量（数学长度/模为 1.0 的向量），以实现高效的余弦相似度计算。此归一化保持向量的方向，同时标准化其长度。\n\n* `hnsw_m`：可选设置，定义图中最大出边连接数。默认值为 16。\n\n* `hnsw_ef_construction`：可选设置，定义构建时的时间/准确性权衡。\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### 普通模式（使用配置文件）：\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### 插入向量数据\n\n#### 自动嵌入（推荐）\n\n处理向量数据最简单的方法是使用**自动嵌入**。使用此功能，您创建一个带有 `MODEL_NAME` 和 `FROM` 参数的表，然后只需插入文本数据——Manticore 会自动为您生成嵌入向量。\n\n##### 创建带自动嵌入的表\n\n创建自动嵌入表时，指定：\n\n- `MODEL_NAME`：要使用的嵌入模型\n\n- `FROM`：用于生成嵌入的字段（为空表示所有文本/字符串字段）\n\n**支持的嵌入模型：**\n\n- **Sentence Transformers**：任何[合适的基于 BERT 的 Hugging Face 模型](https://huggingface.co/sentence-transformers/models)（例如 `sentence-transformers/all-MiniLM-L6-v2`）——无需 API 密钥。Manticore 在创建表时下载模型。\n\n- **OpenAI**：OpenAI 嵌入模型，如 `openai/text-embedding-ada-002` - 需要 `API_KEY='<OPENAI_API_KEY>'` 参数\n\n- **Voyage**：Voyage AI 嵌入模型 - 需要 `API_KEY='<VOYAGE_API_KEY>'` 参数\n\n- **Jina**：Jina AI 嵌入模型 - 需要 `API_KEY='<JINA_API_KEY>'` 参数\n\n有关设置 `float_vector` 属性的更多信息，请参见[这里](../Creating_a_table/Data_types.md#Float-vector)。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n使用 sentence-transformers（无需 API 密钥）\n\nCODE_BLOCK_3\n\n使用 OpenAI（需要 API_KEY 参数）\n\nCODE_BLOCK_4\n\n使用所有文本字段生成嵌入（FROM 为空）\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### 使用自动嵌入插入数据\n\n<!-- example inserting_embeddings -->\n\n使用自动嵌入时，**不要在 INSERT 语句中指定向量字段**。嵌入向量会自动从 `FROM` 参数指定的文本字段生成。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n仅插入文本数据 - 嵌入自动生成\n\nCODE_BLOCK_6\n\n插入多个字段 - 如果 FROM='title,description'，则两者都用于生成嵌入\n\nCODE_BLOCK_7\n\n插入空向量（文档将被排除在向量搜索之外）\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### 使用自动嵌入搜索\n\n<!-- example embeddings_search -->\n\n搜索方式相同——提供查询文本，Manticore 会生成嵌入并找到相似文档：\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON：\n\n<!-- request JSON -->\n\n使用文本查询和自动嵌入\n\nCODE_BLOCK_11\n\n直接使用向量查询\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### 手动插入向量\n\n<!-- example manual_vector -->\n\n或者，您可以手动插入预先计算好的向量数据，确保其维度与创建表时指定的维度匹配。您也可以插入空向量；这意味着该文档将被排除在向量搜索结果之外。",
      "russian": "# Поиск векторов методом k-ближайших соседей\n\nManticore Search поддерживает возможность добавления эмбеддингов, сгенерированных моделями машинного обучения, к каждому документу, а затем выполнения поиска ближайших соседей по ним. Это позволяет создавать такие функции, как поиск по схожести, рекомендации, семантический поиск и ранжирование релевантности на основе алгоритмов обработки естественного языка (NLP), а также, среди прочего, поиск по изображениям, видео и звуку.\n\n## Что такое эмбеддинг?\n\nЭмбеддинг — это метод представления данных — таких как текст, изображения или звук — в виде векторов в пространстве высокой размерности. Эти векторы создаются так, чтобы расстояние между ними отражало схожесть представляемых данных. Этот процесс обычно использует алгоритмы, такие как векторные представления слов (например, Word2Vec, BERT) для текста или нейронные сети для изображений. Высокая размерность векторного пространства, с большим количеством компонентов на вектор, позволяет представлять сложные и тонкие взаимосвязи между объектами. Их схожесть оценивается по расстоянию между этими векторами, часто измеряемому с помощью методов, таких как евклидово расстояние или косинусное сходство.\n\nManticore Search позволяет выполнять поиск k-ближайших соседей (KNN) векторов с использованием библиотеки HNSW. Эта функциональность является частью [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Настройка таблицы для поиска KNN\n\nДля выполнения поиска KNN необходимо сначала настроить таблицу. Векторы с плавающей точкой и поиск KNN поддерживаются только в таблицах реального времени (не в обычных таблицах). Таблица должна содержать как минимум один атрибут [float_vector](../Creating_a_table/Data_types.md#Float-vector), который служит вектором данных. Необходимо указать следующие свойства:\n\n* `knn_type`: обязательный параметр; в настоящее время поддерживается только `hnsw`.\n\n* `knn_dims`: обязательный параметр, указывающий размерность индексируемых векторов.\n\n* `hnsw_similarity`: обязательный параметр, указывающий функцию расстояния, используемую индексом HNSW. Допустимые значения:\n\n  - `L2` - квадрат евклидова расстояния\n\n  - `IP` - внутреннее произведение\n\n  - `COSINE` - косинусное сходство\n\n  \n\n  **Примечание:** При использовании косинусного сходства (`COSINE`) векторы автоматически нормализуются при вставке. Это означает, что сохранённые значения векторов могут отличаться от исходных, так как они будут преобразованы в единичные векторы (векторы с математической длиной/модулем 1.0) для эффективного вычисления косинусного сходства. Такая нормализация сохраняет направление вектора при стандартизации его длины.\n\n* `hnsw_m`: необязательный параметр, определяющий максимальное количество исходящих связей в графе. По умолчанию 16.\n\n* `hnsw_ef_construction`: необязательный параметр, определяющий компромисс между временем построения и точностью.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Plain mode (использование конфигурационного файла):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Вставка векторных данных\n\n#### Автоматические эмбеддинги (рекомендуется)\n\nСамый простой способ работать с векторными данными — использовать **автоматические эмбеддинги**. С этой функцией вы создаёте таблицу с параметрами `MODEL_NAME` и `FROM`, а затем просто вставляете текстовые данные — Manticore автоматически генерирует эмбеддинги для вас.\n\n##### Создание таблицы с автоматическими эмбеддингами\n\nПри создании таблицы для автоматических эмбеддингов укажите:\n\n- `MODEL_NAME`: модель эмбеддинга для использования\n\n- `FROM`: какие поля использовать для генерации эмбеддингов (пустое значение означает все текстовые/строковые поля)\n\n**Поддерживаемые модели эмбеддингов:**\n\n- **Sentence Transformers**: Любая [подходящая модель на базе BERT из Hugging Face](https://huggingface.co/sentence-transformers/models) (например, `sentence-transformers/all-MiniLM-L6-v2`) — не требует API-ключа. Manticore загружает модель при создании таблицы.\n\n- **OpenAI**: Модели эмбеддингов OpenAI, такие как `openai/text-embedding-ada-002` — требует параметр `API_KEY='<OPENAI_API_KEY>'`\n\n- **Voyage**: Модели эмбеддингов Voyage AI — требует параметр `API_KEY='<VOYAGE_API_KEY>'`\n\n- **Jina**: Модели эмбеддингов Jina AI — требует параметр `API_KEY='<JINA_API_KEY>'`\n\nБолее подробная информация о настройке атрибута `float_vector` доступна [здесь](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nИспользование sentence-transformers (API-ключ не требуется)\n\nCODE_BLOCK_3\n\nИспользование OpenAI (требуется параметр API_KEY)\n\nCODE_BLOCK_4\n\nИспользование всех текстовых полей для эмбеддингов (параметр FROM пуст)\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### Вставка данных с автоматическими эмбеддингами\n\n<!-- example inserting_embeddings -->\n\nПри использовании автоматических эмбеддингов **не указывайте поле вектора** в операторе INSERT. Эмбеддинги генерируются автоматически из текстовых полей, указанных в параметре `FROM`.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nВставка только текстовых данных — эмбеддинги генерируются автоматически\n\nCODE_BLOCK_6\n\nВставка нескольких полей — все используются для эмбеддинга, если FROM='title,description'  \n\nCODE_BLOCK_7\n\nВставка пустого вектора (документ исключается из поиска по векторам)\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### Поиск с автоматическими эмбеддингами\n\n<!-- example embeddings_search -->\n\nПоиск работает так же — укажите текст запроса, и Manticore сгенерирует эмбеддинги и найдёт похожие документы:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nИспользование текстового запроса с автоматическими эмбеддингами\n\nCODE_BLOCK_11\n\nИспользование векторного запроса напрямую\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### Ручная вставка векторов\n\n<!-- example manual_vector -->\n\nВ качестве альтернативы вы можете вручную вставлять заранее вычисленные векторные данные, убедившись, что они соответствуют размерности, указанной при создании таблицы. Также можно вставить пустой вектор; это означает, что документ будет исключён из результатов поиска по векторам."
    },
    "is_code_or_comment": false
  },
  "3499904d571b82d3031c8764d97a4443bad22f0fdc4b8eaf61ac49024a7538b5": {
    "original": "**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.\n\n* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Find similar docs by id\n\n> NOTE: Finding similar documents by id requires [Manticore Buddy](../Installation/Manticore_Buddy.md). If it doesn't work, make sure Buddy is installed.",
    "translations": {
      "russian": "**Важно:** При использовании `hnsw_similarity='cosine'` векторы автоматически нормализуются при вставке до единичных векторов (векторов с математической длиной/модулем 1.0). Эта нормализация сохраняет направление вектора при стандартизации его длины, что необходимо для эффективных вычислений косинусного сходства. Это означает, что сохранённые значения будут отличаться от ваших исходных значений.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### Поиск KNN векторов\n\nТеперь вы можете выполнять поиск KNN с помощью оператора `knn` в формате SQL или JSON. Оба интерфейса поддерживают одинаковые основные параметры, обеспечивая единообразный опыт независимо от выбранного формата:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: Имя атрибута с плавающей точкой, содержащего векторные данные.\n\n* `k`: Количество документов для возврата, ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Определяет количество документов, которые должен вернуть один индекс HNSW. Однако фактическое количество документов в итоговых результатах может варьироваться. Например, если система работает с таблицами в реальном времени, разбитыми на дисковые чанки, каждый чанк может вернуть `k` документов, что приведёт к общему числу, превышающему заданное `k` (так как суммарное количество будет `num_chunks * k`). С другой стороны, итоговое количество документов может быть меньше `k`, если после запроса `k` документов некоторые из них отфильтровываются по определённым атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс извлечения работает иначе, и поэтому параметр `k` не влияет на количество возвращаемых документов.\n\n* `query`: (Рекомендуемый параметр) Поисковый запрос, который может быть:\n\n  - Текстовой строкой: Автоматически преобразуется в эмбеддинги, если для поля настроены авто-эмбеддинги. Вернёт ошибку, если авто-эмбеддинги не настроены.\n\n  - Массивом векторов: Работает так же, как `query_vector`.\n\n* `query_vector`: (Устаревший параметр) Поисковый вектор в виде массива чисел. Поддерживается для обратной совместимости.\n\n  **Примечание:** Используйте либо `query`, либо `query_vector`, не оба одновременно.\n\n* `ef`: необязательный размер динамического списка, используемого во время поиска. Более высокий `ef` обеспечивает более точный, но более медленный поиск.\n\n* `rescore`: Включает повторный расчёт KNN (по умолчанию отключён). Установите `1` в SQL или `true` в JSON для включения повторного расчёта. После завершения поиска KNN с использованием квантизированных векторов (с возможным оверсемплингом) расстояния пересчитываются с использованием оригинальных (полной точности) векторов, и результаты пересортировываются для улучшения точности ранжирования.\n\n* `oversampling`: Устанавливает множитель (число с плавающей точкой), на который умножается `k` при выполнении поиска KNN, что приводит к выбору большего количества кандидатов, чем требуется, с использованием квантизированных векторов. По умолчанию оверсемплинг не применяется. Эти кандидаты могут быть повторно оценены, если включён повторный расчёт. Оверсемплинг также работает с неквантизированными векторами. Поскольку он увеличивает `k`, что влияет на работу индекса HNSW, это может вызвать небольшое изменение точности результатов.\n\nДокументы всегда сортируются по расстоянию до поискового вектора. Любые дополнительные критерии сортировки, которые вы укажете, применяются после этого основного условия сортировки. Для получения расстояния существует встроенная функция [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Квантование векторов\n\nИндексы HNSW должны быть полностью загружены в память для выполнения поиска KNN, что может привести к значительному потреблению памяти. Для уменьшения использования памяти можно применить скалярное квантование — метод сжатия высокоразмерных векторов путём представления каждого компонента (измерения) ограниченным числом дискретных значений. Manticore поддерживает 8-битное и 1-битное квантование, что означает, что каждый компонент вектора сжимается с 32-битного float до 8 бит или даже 1 бита, уменьшая использование памяти в 4 или 32 раза соответственно. Эти сжатые представления также позволяют быстрее вычислять расстояния, так как больше компонентов вектора можно обработать за одну SIMD-инструкцию. Хотя скалярное квантование вводит некоторую погрешность, это часто оправданный компромисс между точностью поиска и эффективностью ресурсов. Для ещё лучшей точности квантование можно сочетать с повторным расчётом и оверсемплингом: выбирается больше кандидатов, чем запрошено, и расстояния для этих кандидатов пересчитываются с использованием оригинальных 32-битных float-векторов.\n\nПоддерживаемые типы квантования:\n\n* `8bit`: Каждый компонент вектора квантован до 8 бит.\n\n* `1bit`: Каждый компонент вектора квантован до 1 бита. Используется асимметричное квантование: векторы запроса квантованы до 4 бит, а хранимые векторы — до 1 бита. Такой подход обеспечивает большую точность, чем более простые методы, хотя и с некоторыми потерями в производительности.\n\n* `1bitsimple`: Каждый компонент вектора квантован до 1 бита. Этот метод быстрее, чем `1bit`, но обычно менее точен.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Поиск похожих документов по id\n\n> ПРИМЕЧАНИЕ: Поиск похожих документов по id требует [Manticore Buddy](../Installation/Manticore_Buddy.md). Если не работает, убедитесь, что Buddy установлен.",
      "chinese": "**重要提示：** 当使用 `hnsw_similarity='cosine'` 时，向量在插入时会自动归一化为单位向量（数学长度/幅度为1.0的向量）。这种归一化保持了向量的方向，同时标准化了其长度，这是高效计算余弦相似度所必需的。这意味着存储的值将与您原始输入的值不同。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN 向量搜索\n\n现在，您可以使用 SQL 或 JSON 格式中的 `knn` 子句执行 KNN 搜索。两种接口都支持相同的基本参数，确保无论选择哪种格式，都能获得一致的体验：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性的名称。\n\n* `k`：表示返回的文档数量，是分层可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。然而，最终结果中包含的文档数量可能会有所不同。例如，如果系统处理的是分割成磁盘块的实时表，每个块可能返回 `k` 个文档，导致总数超过指定的 `k`（因为累计计数为 `num_chunks * k`）。另一方面，如果在请求了 `k` 个文档后，根据特定属性过滤掉了一些文档，最终文档数可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程不同，因此 `k` 参数对返回文档数量的影响不适用。\n\n* `query`：（推荐参数）搜索查询，可以是：\n\n  - 文本字符串：如果字段配置了自动嵌入，则自动转换为嵌入向量。如果字段没有自动嵌入，将返回错误。\n\n  - 向量数组：与 `query_vector` 功能相同。\n\n* `query_vector`：（遗留参数）作为数字数组的搜索向量。为向后兼容仍然支持。\n\n  **注意：** 在同一请求中使用 `query` 或 `query_vector` 中的一个，不要同时使用。\n\n* `ef`：搜索过程中使用的动态列表大小的可选参数。`ef` 越大，搜索越准确但越慢。\n\n* `rescore`：启用 KNN 重新评分（默认禁用）。在 SQL 中设置为 `1`，在 JSON 中设置为 `true` 以启用重新评分。KNN 搜索完成后，使用量化向量（可能带有过采样）进行距离计算，然后用原始（全精度）向量重新计算距离并重新排序结果，以提高排名准确性。\n\n* `oversampling`：设置一个因子（浮点值），在执行 KNN 搜索时将 `k` 乘以该因子，导致使用量化向量检索的候选项多于所需数量。默认不应用过采样。如果启用重新评分，这些候选项可以稍后重新评估。过采样也适用于非量化向量。由于它增加了 `k`，影响 HNSW 索引的工作方式，可能会导致结果准确性略有变化。\n\n文档始终按与搜索向量的距离排序。您指定的任何额外排序条件将在此主要排序条件之后应用。要获取距离，有一个内置函数叫做 [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29)。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### 向量量化\n\nHNSW 索引需要完全加载到内存中以执行 KNN 搜索，这可能导致显著的内存消耗。为了减少内存使用，可以应用标量量化——一种通过用有限数量的离散值表示每个分量（维度）来压缩高维向量的技术。Manticore 支持 8 位和 1 位量化，意味着每个向量分量从 32 位浮点压缩到 8 位甚至 1 位，分别减少了 4 倍或 32 倍的内存使用。这些压缩表示还允许更快的距离计算，因为可以在单个 SIMD 指令中处理更多的向量分量。虽然标量量化引入了一些近似误差，但通常是在搜索准确性和资源效率之间值得的权衡。为了获得更好的准确性，量化可以与重新评分和过采样结合使用：检索的候选项多于请求的数量，并使用原始 32 位浮点向量重新计算这些候选项的距离。\n\n支持的量化类型包括：\n\n* `8bit`：每个向量分量量化为 8 位。\n\n* `1bit`：每个向量分量量化为 1 位。使用非对称量化，查询向量量化为 4 位，存储向量量化为 1 位。这种方法比简单方法提供更高的精度，但性能有所折衷。\n\n* `1bitsimple`：每个向量分量量化为 1 位。此方法比 `1bit` 更快，但通常准确性较低。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### 通过 id 查找相似文档\n\n> 注意：通过 id 查找相似文档需要 [Manticore Buddy](../Installation/Manticore_Buddy.md)。如果无法使用，请确保已安装 Buddy。"
    },
    "is_code_or_comment": false
  },
  "0e558c9c8a9c7c4f86543972ec5b8ce376aa9e32c57345abb8e228702e986f5c": {
    "original": "<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.\n\n**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- response JSON -->\n\nCODE_BLOCK_25\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_26\n\n<!-- response SQL -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_28\n\n<!-- response JSON -->\n\nCODE_BLOCK_29\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.\n\n* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->",
    "translations": {
      "chinese": "<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.\n\n**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- response JSON -->\n\nCODE_BLOCK_25\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_26\n\n<!-- response SQL -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_28\n\n<!-- response JSON -->\n\nCODE_BLOCK_29\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.\n\n* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->",
      "russian": "<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.\n\n**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_24\n\n<!-- response JSON -->\n\nCODE_BLOCK_25\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_26\n\n<!-- response SQL -->\n\nCODE_BLOCK_27\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_28\n\n<!-- response JSON -->\n\nCODE_BLOCK_29\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.\n\n* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->"
    },
    "is_code_or_comment": true
  },
  "1bc252d34f3b4dffe798c8f817045999423c6c1db0e86fbbea60762bdbce9b72": {
    "original": "##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_30\n\n<!-- response SQL -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_32\n\n<!-- response JSON -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Find similar docs by id\n\n> NOTE: Finding similar documents by id requires [Manticore Buddy](../Installation/Manticore_Buddy.md). If it doesn't work, make sure Buddy is installed.\n\nFinding documents similar to a specific one based on its unique ID is a common task. For instance, when a user views a particular item, Manticore Search can efficiently identify and display a list of items that are most similar to it in the vector space. Here's how you can do it:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `document id`: Document ID for KNN similarity search.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- response SQL -->\n\nCODE_BLOCK_35\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- response JSON -->\n\nCODE_BLOCK_37\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Filtering KNN vector search results\n\nManticore also supports additional filtering of documents returned by the KNN search, either by full-text matching, attribute filters, or both.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_38\n\n<!-- response SQL -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_40\n\n<!-- response JSON -->\n\nCODE_BLOCK_41\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_30\n\n<!-- response SQL -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_32\n\n<!-- response JSON -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### 根据 id 查找相似文档\n\n> 注意：通过 id 查找相似文档需要 [Manticore Buddy](../Installation/Manticore_Buddy.md)。如果无法正常工作，请确保 Buddy 已安装。\n\n根据特定文档的唯一 ID 查找与之相似的文档是一个常见任务。例如，当用户查看某一特定项时，Manticore Search 可以高效地识别并展示一组在向量空间中与之最相似的项目。操作方法如下：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性的名称。\n\n* `k`：表示返回的文档数量，是层次可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。然而，最终结果中包含的文档数量可能会有所不同。例如，如果系统处理的是被分成磁盘块的实时表，每个块可能返回 `k` 个文档，导致总数超过指定的 `k`（因为总计为 `num_chunks * k`）。另一方面，如果在请求 `k` 个文档后，部分文档基于特定属性被过滤，最终的文档数量可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程有所不同，因此 `k` 参数对返回文档数量的影响不适用。\n\n* `document id`：用于 KNN 相似度搜索的文档 ID。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- response SQL -->\n\nCODE_BLOCK_35\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- response JSON -->\n\nCODE_BLOCK_37\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### 过滤 KNN 向量搜索结果\n\nManticore 还支持对 KNN 搜索返回的文档进行额外过滤，可以通过全文匹配、属性过滤，或两者结合。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_38\n\n<!-- response SQL -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_40\n\n<!-- response JSON -->\n\nCODE_BLOCK_41\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_30\n\n<!-- response SQL -->\n\nCODE_BLOCK_31\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_32\n\n<!-- response JSON -->\n\nCODE_BLOCK_33\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Поиск похожих документов по id\n\n> ПРИМЕЧАНИЕ: Для поиска похожих документов по id требуется [Manticore Buddy](../Installation/Manticore_Buddy.md). Если это не работает, убедитесь, что Buddy установлен.\n\nПоиск документов, похожих на конкретный, основываясь на его уникальном ID, является распространенной задачей. Например, когда пользователь просматривает определенный элемент, Manticore Search может эффективно определить и показать список элементов, наиболее похожих на него в векторном пространстве. Вот как это можно сделать:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: Это имя атрибута с плавающей точкой, содержащего векторные данные.\n\n* `k`: Это количество документов для возврата и ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Он определяет количество документов, которые должен вернуть один индекс HNSW. Однако фактическое количество документов в окончательных результатах может варьироваться. Например, если система работает с таблицами в реальном времени, разбитыми на дисковые чанки, каждый чанк может вернуть `k` документов, что приведет к общему числу, превышающему заданное `k` (поскольку суммарное количество будет `num_chunks * k`). С другой стороны, окончательное количество документов может быть меньше `k`, если после запроса `k` документов некоторые отфильтровываются по определенным атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс выборки осуществляется иначе, и потому параметр `k` не влияет на количество возвращаемых документов.\n\n* `document id`: ID документа для поиска по сходству KNN.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_34\n\n<!-- response SQL -->\n\nCODE_BLOCK_35\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_36\n\n<!-- response JSON -->\n\nCODE_BLOCK_37\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Фильтрация результатов KNN поиска векторов\n\nManticore также поддерживает дополнительную фильтрацию документов, возвращаемых KNN поиском, через полнотекстовое совпадение, фильтры атрибутов или оба способа одновременно.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_38\n\n<!-- response SQL -->\n\nCODE_BLOCK_39\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_40\n\n<!-- response JSON -->\n\nCODE_BLOCK_41\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  },
  "e526c98701271dc16d81657c644796a2ea12f6638425b2fbf889489638775cf9": {
    "original": "# K-nearest neighbor vector search\n\nManticore Search supports the ability to add embeddings generated by Machine Learning models to each document, and then doing a nearest-neighbor search on them. This lets you build features like similarity search, recommendations, semantic search, and relevance ranking based on NLP algorithms, among others, including image, video, and sound searches.\n\n## What is an embedding?\n\nAn embedding is a method of representing data - such as text, images, or sound - as vectors in a high-dimensional space. These vectors are crafted to ensure that the distance between them reflects the similarity of the data they represent. This process typically employs algorithms like word embeddings (e.g., Word2Vec, BERT) for text or neural networks for images. The high-dimensional nature of the vector space, with many components per vector, allows for the representation of complex and nuanced relationships between items. Their similarity is gauged by the distance between these vectors, often measured using methods like Euclidean distance or cosine similarity.\n\nManticore Search enables k-nearest neighbor (KNN) vector searches using the HNSW library. This functionality is part of the [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Configuring a table for KNN search\n\nTo run KNN searches, you must first configure your table. Float vectors and KNN search are only supported in real-time tables (not in plain tables). The table needs to have at least one [float_vector](../Creating_a_table/Data_types.md#Float-vector) attribute, which serves as a data vector. You need to specify the following properties:\n\n* `knn_type`: A mandatory setting; currently, only `hnsw` is supported.\n\n* `knn_dims`: A mandatory setting that specifies the dimensions of the vectors being indexed.\n\n* `hnsw_similarity`: A mandatory setting that specifies the distance function used by the HNSW index. Acceptable values are:\n\n  - `L2` - Squared L2\n\n  - `IP` - Inner product\n\n  - `COSINE` - Cosine similarity\n\n  \n\n  **Note:** When using `COSINE` similarity, vectors are automatically normalized upon insertion. This means the stored vector values may differ from the original input values, as they will be converted to unit vectors (vectors with a mathematical length/magnitude of 1.0) to enable efficient cosine similarity calculations. This normalization preserves the direction of the vector while standardizing its length.\n\n* `hnsw_m`: An optional setting that defines the maximum number of outgoing connections in the graph. The default is 16.\n\n* `hnsw_ef_construction`: An optional setting that defines a construction time/accuracy trade-off.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- response JSON -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### Plain mode (using configuration file):\n\n<!-- request Config -->\n\nCODE_BLOCK_4\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Inserting vector data\n\n#### Auto Embeddings (Recommended)\n\nThe easiest way to work with vector data is using **auto embeddings**. With this feature, you create a table with `MODEL_NAME` and `FROM` parameters, then simply insert your text data - Manticore automatically generates embeddings for you.\n\n##### Creating a table with auto embeddings\n\nWhen creating a table for auto embeddings, specify:\n\n- `MODEL_NAME`: The embedding model to use\n\n- `FROM`: Which fields to use for embedding generation (empty means all text/string fields)\n\n**Supported embedding models:**\n\n- **Sentence Transformers**: Any [suitable BERT-based Hugging Face model](https://huggingface.co/sentence-transformers/models) (e.g., `sentence-transformers/all-MiniLM-L6-v2`) — no API key needed. Manticore downloads the model when you create the table.\n\n- **OpenAI**: OpenAI embedding models like `openai/text-embedding-ada-002` - requires `API_KEY='<OPENAI_API_KEY>'` parameter\n\n- **Voyage**: Voyage AI embedding models - requires `API_KEY='<VOYAGE_API_KEY>'` parameter\n\n- **Jina**: Jina AI embedding models - requires `API_KEY='<JINA_API_KEY>'` parameter\n\nMore information about setting up a `float_vector` attribute can be found [here](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nUsing sentence-transformers (no API key needed)\n\nCODE_BLOCK_5\n\nUsing OpenAI (requires API_KEY parameter)\n\nCODE_BLOCK_6\n\nUsing all text fields for embeddings (FROM is empty)\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nUsing sentence-transformers (no API key needed)\n\nCODE_BLOCK_8\n\nUsing OpenAI (requires API_KEY parameter)\n\nCODE_BLOCK_9\n\nUsing all text fields for embeddings (FROM is empty)\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n##### Inserting data with auto embeddings\n\n<!-- example inserting_embeddings -->\n\nWhen using auto embeddings, **do not specify the vector field** in your INSERT statement. The embeddings are generated automatically from the text fields specified in the `FROM` parameter.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nInsert text data only - embeddings generated automatically\n\nCODE_BLOCK_11\n\nInsert multiple fields - both used for embedding if FROM='title,description'  \n\nCODE_BLOCK_12\n\nInsert empty vector (document excluded from vector search)\n\nCODE_BLOCK_13\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nInsert text data only - embeddings generated automatically\n\nCODE_BLOCK_14\n\nInsert multiple fields - both used for embedding if FROM='title,description'  \n\nCODE_BLOCK_15\n\nInsert empty vector (document excluded from vector search)\n\nCODE_BLOCK_16\n\n<!-- end -->\n\n##### Searching with auto embeddings\n\n<!-- example embeddings_search -->\n\nSearch works the same way - provide your query text and Manticore will generate embeddings and find similar documents:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_17\n\n<!-- response SQL -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nUsing text query with auto-embeddings\n\nCODE_BLOCK_19\n\nUsing vector query directly\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21",
    "translations": {
      "chinese": "# K近邻向量搜索\n\nManticore Search 支持将机器学习模型生成的嵌入向量添加到每个文档中，然后对其进行近邻搜索。这样可以构建诸如相似性搜索、推荐、语义搜索及基于NLP算法的相关度排序等功能，还包括图像、视频和声音搜索。\n\n## 什么是嵌入向量？\n\n嵌入向量是一种表示数据（如文本、图像或声音）的方法，将其表示为高维空间中的向量。这些向量被设计成使它们之间的距离反映出所代表数据的相似性。这个过程通常使用诸如词嵌入（如 Word2Vec、BERT）对文本，或神经网络对图像的算法。向量空间的高维特性（每个向量包含多个分量）允许表示项目之间复杂而细致的关系。它们的相似度通过向量间的距离进行衡量，常用的方法包括欧氏距离或余弦相似度。\n\nManticore Search 使用 HNSW 库实现 K 近邻（KNN）向量搜索。此功能是 [Manticore Columnar Library](https://github.com/manticoresoftware/columnar) 的一部分。\n\n<!-- example KNN -->\n\n### 配置用于KNN搜索的表\n\n要运行KNN搜索，首先必须配置表。浮点向量和KNN搜索仅支持实时表（不支持普通表）。表中需要至少有一个用作数据向量的 [float_vector](../Creating_a_table/Data_types.md#Float-vector) 属性。需要指定以下属性：\n\n* `knn_type`：必需设置；目前只支持 `hnsw`。\n\n* `knn_dims`：必需设置，指定被索引向量的维数。\n\n* `hnsw_similarity`：必需设置，指定HNSW索引使用的距离函数。可接受的值为：\n\n  - `L2` - 平方L2距离\n\n  - `IP` - 内积\n\n  - `COSINE` - 余弦相似度\n\n  \n\n  **注意：** 使用 `COSINE` 相似度时，向量在插入时会自动归一化。这意味着存储的向量值可能与原始输入值不同，因为它们将被转换为单位向量（长度为1.0的向量），以实现高效的余弦相似度计算。此归一化保留了向量方向，同时标准化了长度。\n\n* `hnsw_m`：可选设置，定义图中最大出边连接数。默认值为16。\n\n* `hnsw_ef_construction`：可选设置，定义构建时的速度与准确度权衡。\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- response JSON -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### 普通模式（使用配置文件）：\n\n<!-- request Config -->\n\nCODE_BLOCK_4\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### 插入向量数据\n\n#### 自动嵌入（推荐）\n\n处理向量数据最简单的方法是使用**自动嵌入**。使用此功能，创建包含 `MODEL_NAME` 和 `FROM` 参数的表，然后直接插入文本数据——Manticore 会自动为你生成嵌入向量。\n\n##### 创建支持自动嵌入的表\n\n创建支持自动嵌入的表时，需指定：\n\n- `MODEL_NAME`：所用的嵌入模型\n\n- `FROM`：用于生成嵌入的字段（为空表示使用所有文本/字符串字段）\n\n**支持的嵌入模型：**\n\n- **Sentence Transformers**：任何[合适的基于BERT的Hugging Face模型](https://huggingface.co/sentence-transformers/models)（例如 `sentence-transformers/all-MiniLM-L6-v2`）——无需API密钥。Manticore 在创建表时下载模型。\n\n- **OpenAI**：OpenAI 嵌入模型，如 `openai/text-embedding-ada-002` —— 需要 `API_KEY='<OPENAI_API_KEY>'` 参数\n\n- **Voyage**：Voyage AI 嵌入模型 —— 需要 `API_KEY='<VOYAGE_API_KEY>'` 参数\n\n- **Jina**：Jina AI 嵌入模型 —— 需要 `API_KEY='<JINA_API_KEY>'` 参数\n\n有关如何设置 `float_vector` 属性的更多信息，请参见[此处](../Creating_a_table/Data_types.md#Float-vector)。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n使用 sentence-transformers（无需API密钥）\n\nCODE_BLOCK_5\n\n使用 OpenAI（需要 API_KEY 参数）\n\nCODE_BLOCK_6\n\n使用所有文本字段进行嵌入（FROM 为空）\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### JSON：\n\n<!-- request JSON -->\n\n使用 sentence-transformers（无需API密钥）\n\nCODE_BLOCK_8\n\n使用 OpenAI（需要 API_KEY 参数）\n\nCODE_BLOCK_9\n\n使用所有文本字段进行嵌入（FROM 为空）\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n##### 使用自动嵌入插入数据\n\n<!-- example inserting_embeddings -->\n\n使用自动嵌入时，**不要在 INSERT 语句中指定向量字段**。嵌入向量会自动从 `FROM` 参数指定的文本字段生成。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n仅插入文本数据——嵌入自动生成\n\nCODE_BLOCK_11\n\n插入多个字段——如果 FROM='title,description'，这两个字段都会用于嵌入生成\n\nCODE_BLOCK_12\n\n插入空向量（该文档被排除在向量搜索之外）\n\nCODE_BLOCK_13\n\n<!-- intro -->\n\n##### JSON：\n\n<!-- request JSON -->\n\n仅插入文本数据——嵌入自动生成\n\nCODE_BLOCK_14\n\n插入多个字段——如果 FROM='title,description'，这两个字段都会用于嵌入生成\n\nCODE_BLOCK_15\n\n插入空向量（该文档被排除在向量搜索之外）\n\nCODE_BLOCK_16\n\n<!-- end -->\n\n##### 使用自动嵌入搜索\n\n<!-- example embeddings_search -->\n\n搜索方式相同——提供查询文本，Manticore 会自动生成嵌入并找到相似文档：\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\nCODE_BLOCK_17\n\n<!-- response SQL -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### JSON：\n\n<!-- request JSON -->\n\n使用带自动嵌入的文本查询\n\nCODE_BLOCK_19\n\n直接使用向量查询\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21",
      "russian": "# Поиск векторов методом ближайших соседей K\n\nManticore Search поддерживает возможность добавлять эмбеддинги, сгенерированные моделями машинного обучения, к каждому документу и выполнять поиск ближайших соседей по ним. Это позволяет создавать функции, такие как поиск по схожести, рекомендации, семантический поиск и ранжирование релевантности на основе алгоритмов обработки естественного языка (NLP), а также, среди прочего, поиск по изображениям, видео и звуку.\n\n## Что такое эмбеддинг?\n\nЭмбеддинг — это метод представления данных — таких как текст, изображения или звук — в виде векторов в пространстве высокой размерности. Эти векторы формируются так, чтобы расстояние между ними отражало степень схожести представленных данных. Этот процесс обычно использует алгоритмы, такие как встраивание слов (например, Word2Vec, BERT) для текста или нейронные сети для изображений. Высокая размерность векторного пространства, с множеством компонентов на вектор, позволяет представлять сложные и тонкие отношения между объектами. Их схожесть оценивается по расстоянию между этими векторами, часто измеряемому методами, такими как евклидово расстояние или косинусное сходство.\n\nManticore Search реализует поиск k-ближайших соседей (KNN) векторов с помощью библиотеки HNSW. Эта функциональность входит в состав [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Настройка таблицы для поиска KNN\n\nДля выполнения поиска KNN сначала нужно настроить таблицу. Векторы типа float и поиск KNN поддерживаются только в таблицах с поддержкой реального времени (real-time), а не в простых таблицах. Таблица должна содержать как минимум один атрибут типа [float_vector](../Creating_a_table/Data_types.md#Float-vector), который будет служить вектором данных. Необходимо указать следующие параметры:\n\n* `knn_type`: обязательный параметр; на данный момент поддерживается только `hnsw`.\n\n* `knn_dims`: обязательный параметр, указывающий размерность векторов, индексируемых в таблице.\n\n* `hnsw_similarity`: обязательный параметр, задающий функцию расстояния, используемую индексом HNSW. Допустимые значения:\n\n  - `L2` — квадратичное L2 расстояние\n\n  - `IP` — внутреннее произведение\n\n  - `COSINE` — косинусное сходство\n\n  \n\n  **Примечание:** При использовании сходства `COSINE` векторы автоматически нормализуются при вставке. Это означает, что хранящиеся значения векторов могут отличаться от исходных, так как они будут преобразованы в единичные векторы (векторы с длиной/модулем 1.0) для обеспечения эффективных вычислений косинусного сходства. Такая нормализация сохраняет направление вектора, одновременно стандартизируя его длину.\n\n* `hnsw_m`: необязательный параметр, определяющий максимальное число исходящих связей в графе. По умолчанию — 16.\n\n* `hnsw_ef_construction`: необязательный параметр, который регулирует компромисс между временем построения и точностью.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- request JSON -->\n\nCODE_BLOCK_2\n\n<!-- response JSON -->\n\nCODE_BLOCK_3\n\n<!-- intro -->\n\n##### Режим Plain (использование конфигурационного файла):\n\n<!-- request Config -->\n\nCODE_BLOCK_4\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Вставка векторных данных\n\n#### Автоматические эмбеддинги (рекомендуется)\n\nСамый простой способ работать с векторными данными — использовать **автоматические эмбеддинги**. С этой функцией вы создаёте таблицу с параметрами `MODEL_NAME` и `FROM`, затем просто вставляете текстовые данные — Manticore автоматически генерирует эмбеддинги за вас.\n\n##### Создание таблицы с автоматическими эмбеддингами\n\nПри создании таблицы для автоматических эмбеддингов укажите:\n\n- `MODEL_NAME`: модель для эмбеддинга\n\n- `FROM`: поля, используемые для генерации эмбеддингов (пустое значение означает использование всех текстовых/строковых полей)\n\n**Поддерживаемые модели эмбеддингов:**\n\n- **Sentence Transformers**: Любая [подходящая модель Hugging Face на базе BERT](https://huggingface.co/sentence-transformers/models) (например, `sentence-transformers/all-MiniLM-L6-v2`) — не требуется API-ключ. Manticore скачивает модель при создании таблицы.\n\n- **OpenAI**: Модели эмбеддингов OpenAI, например `openai/text-embedding-ada-002` — требует параметр `API_KEY='<OPENAI_API_KEY>'`\n\n- **Voyage**: Модели эмбеддингов Voyage AI — требует параметр `API_KEY='<VOYAGE_API_KEY>'`\n\n- **Jina**: Модели эмбеддингов Jina AI — требует параметр `API_KEY='<JINA_API_KEY>'`\n\nБольше информации о настройке атрибута `float_vector` можно найти [здесь](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nИспользование sentence-transformers (API-ключ не нужен)\n\nCODE_BLOCK_5\n\nИспользование OpenAI (требуется параметр API_KEY)\n\nCODE_BLOCK_6\n\nИспользование всех текстовых полей для эмбеддингов (FROM пустой)\n\nCODE_BLOCK_7\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nИспользование sentence-transformers (API-ключ не нужен)\n\nCODE_BLOCK_8\n\nИспользование OpenAI (требуется параметр API_KEY)\n\nCODE_BLOCK_9\n\nИспользование всех текстовых полей для эмбеддингов (FROM пустой)\n\nCODE_BLOCK_10\n\n<!-- end -->\n\n##### Вставка данных с автоматическими эмбеддингами\n\n<!-- example inserting_embeddings -->\n\nПри использовании автоматических эмбеддингов **не указывайте поле вектора** в вашем операторе INSERT. Эмбеддинги генерируются автоматически из текстовых полей, указанных в параметре `FROM`.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nВставка только текстовых данных — эмбеддинги создаются автоматически\n\nCODE_BLOCK_11\n\nВставка нескольких полей — используются оба для эмбеддинга, если FROM='title,description'  \n\nCODE_BLOCK_12\n\nВставка пустого вектора (документ исключается из векторного поиска)\n\nCODE_BLOCK_13\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nВставка только текстовых данных — эмбеддинги создаются автоматически\n\nCODE_BLOCK_14\n\nВставка нескольких полей — используются оба для эмбеддинга, если FROM='title,description'  \n\nCODE_BLOCK_15\n\nВставка пустого вектора (документ исключается из векторного поиска)\n\nCODE_BLOCK_16\n\n<!-- end -->\n\n##### Поиск с автоматическими эмбеддингами\n\n<!-- example embeddings_search -->\n\nПоиск работает так же — предоставьте текст запроса, и Manticore сгенерирует эмбеддинги и найдёт похожие документы:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_17\n\n<!-- response SQL -->\n\nCODE_BLOCK_18\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nИспользование текстового запроса с автоматическими эмбеддингами\n\nCODE_BLOCK_19\n\nПрямое использование векторного запроса\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21"
    },
    "is_code_or_comment": false
  }
}
