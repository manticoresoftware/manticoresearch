{
  "c14e77f862f465fc16e7d995e9a989b5cb1f759468502db27d3e2ecc26e60865": {
    "original": "Finding documents similar to a specific one based on its unique ID is a common task. For instance, when a user views a particular item, Manticore Search can efficiently identify and display a list of items that are most similar to it in the vector space. Here's how you can do it:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `document id`: Document ID for KNN similarity search.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Filtering KNN vector search results\n\nManticore also supports additional filtering of documents returned by the KNN search, either by full-text matching, attribute filters, or both.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "基于特定文档的唯一ID查找相似文档是一项常见任务。例如，当用户查看某个特定项目时，Manticore Search 可以高效地识别并显示在向量空间中与其最相似的项目列表。操作方法如下：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性的名称。\n\n* `k`：表示返回的文档数量，是分层可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。然而，最终结果中包含的文档实际数量可能会有所不同。例如，如果系统处理的是分割成磁盘块的实时表，每个块可能返回 `k` 个文档，导致总数超过指定的 `k`（因为累计数量为 `num_chunks * k`）。另一方面，如果在请求了 `k` 个文档后，根据特定属性过滤掉了一些文档，最终文档数量可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程不同，因此 `k` 参数对返回文档数量的影响不适用。\n\n* `document id`：用于 KNN 相似度搜索的文档ID。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### 过滤 KNN 向量搜索结果\n\nManticore 还支持通过全文匹配、属性过滤或两者结合，对 KNN 搜索返回的文档进行额外过滤。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "Нахождение документов, похожих на конкретный, на основе его уникального идентификатора — распространённая задача. Например, когда пользователь просматривает определённый элемент, Manticore Search может эффективно определить и отобразить список элементов, наиболее похожих на него в векторном пространстве. Вот как это можно сделать:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: Это имя атрибута с плавающей точкой, содержащего векторные данные.\n\n* `k`: Это количество документов для возврата и ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Он указывает количество документов, которые должен вернуть один индекс HNSW. Однако фактическое количество документов в итоговых результатах может варьироваться. Например, если система работает с таблицами в реальном времени, разделёнными на дисковые чанки, каждый чанк может вернуть `k` документов, что приведёт к общему числу, превышающему указанное `k` (так как суммарное количество будет `num_chunks * k`). С другой стороны, итоговое количество документов может быть меньше `k`, если после запроса `k` документов некоторые из них отфильтровываются по определённым атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс извлечения работает иначе, и поэтому параметр `k` не влияет на количество возвращаемых документов.\n\n* `document id`: Идентификатор документа для поиска по сходству KNN.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_24\n\n<!-- response SQL -->\n\nCODE_BLOCK_25\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_26\n\n<!-- response JSON -->\n\nCODE_BLOCK_27\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Фильтрация результатов KNN векторного поиска\n\nManticore также поддерживает дополнительную фильтрацию документов, возвращаемых поиском KNN, либо по полнотекстовому совпадению, фильтрам атрибутов, либо по обоим критериям.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_28\n\n<!-- response SQL -->\n\nCODE_BLOCK_29\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_30\n\n<!-- response JSON -->\n\nCODE_BLOCK_31\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  },
  "b76489316d062d2cff359f5cc6e5029ba2ab1fa7c44f128712086b53b0c9d562": {
    "original": "# K-nearest neighbor vector search\n\nManticore Search supports the ability to add embeddings generated by Machine Learning models to each document, and then doing a nearest-neighbor search on them. This lets you build features like similarity search, recommendations, semantic search, and relevance ranking based on NLP algorithms, among others, including image, video, and sound searches.\n\n## What is an embedding?\n\nAn embedding is a method of representing data - such as text, images, or sound - as vectors in a high-dimensional space. These vectors are crafted to ensure that the distance between them reflects the similarity of the data they represent. This process typically employs algorithms like word embeddings (e.g., Word2Vec, BERT) for text or neural networks for images. The high-dimensional nature of the vector space, with many components per vector, allows for the representation of complex and nuanced relationships between items. Their similarity is gauged by the distance between these vectors, often measured using methods like Euclidean distance or cosine similarity.\n\nManticore Search enables k-nearest neighbor (KNN) vector searches using the HNSW library. This functionality is part of the [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Configuring a table for KNN search\n\nTo run KNN searches, you must first configure your table. Float vectors and KNN search are only supported in real-time tables (not in plain tables). The table needs to have at least one [float_vector](../Creating_a_table/Data_types.md#Float-vector) attribute, which serves as a data vector. You need to specify the following properties:\n\n* `knn_type`: A mandatory setting; currently, only `hnsw` is supported.\n\n* `knn_dims`: A mandatory setting that specifies the dimensions of the vectors being indexed.\n\n* `hnsw_similarity`: A mandatory setting that specifies the distance function used by the HNSW index. Acceptable values are:\n\n  - `L2` - Squared L2\n\n  - `IP` - Inner product\n\n  - `COSINE` - Cosine similarity\n\n  \n\n  **Note:** When using `COSINE` similarity, vectors are automatically normalized upon insertion. This means the stored vector values may differ from the original input values, as they will be converted to unit vectors (vectors with a mathematical length/magnitude of 1.0) to enable efficient cosine similarity calculations. This normalization preserves the direction of the vector while standardizing its length.\n\n* `hnsw_m`: An optional setting that defines the maximum number of outgoing connections in the graph. The default is 16.\n\n* `hnsw_ef_construction`: An optional setting that defines a construction time/accuracy trade-off.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Plain mode (using configuration file):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Inserting vector data\n\n#### Auto Embeddings (Recommended)\n\nThe easiest way to work with vector data is using **auto embeddings**. With this feature, you create a table with `MODEL_NAME` and `FROM` parameters, then simply insert your text data - Manticore automatically generates embeddings for you.\n\n##### Creating a table with auto embeddings\n\nWhen creating a table for auto embeddings, specify:\n\n- `MODEL_NAME`: The embedding model to use\n\n- `FROM`: Which fields to use for embedding generation (empty means all text/string fields)\n\n**Supported embedding models:**\n\n- **Sentence Transformers**: Any [suitable BERT-based Hugging Face model](https://huggingface.co/sentence-transformers/models) (e.g., `sentence-transformers/all-MiniLM-L6-v2`) — no API key needed. Manticore downloads the model when you create the table.\n\n- **OpenAI**: OpenAI embedding models like `openai/text-embedding-ada-002` - requires `API_KEY='<OPENAI_API_KEY>'` parameter\n\n- **Voyage**: Voyage AI embedding models - requires `API_KEY='<VOYAGE_API_KEY>'` parameter\n\n- **Jina**: Jina AI embedding models - requires `API_KEY='<JINA_API_KEY>'` parameter\n\nMore information about setting up a `float_vector` attribute can be found [here](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nUsing sentence-transformers (no API key needed)\n\nCODE_BLOCK_3\n\nUsing OpenAI (requires API_KEY parameter)\n\nCODE_BLOCK_4\n\nUsing all text fields for embeddings (FROM is empty)\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### Inserting data with auto embeddings\n\n<!-- example inserting_embeddings -->\n\nWhen using auto embeddings, **do not specify the vector field** in your INSERT statement. The embeddings are generated automatically from the text fields specified in the `FROM` parameter.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nInsert text data only - embeddings generated automatically\n\nCODE_BLOCK_6\n\nInsert multiple fields - both used for embedding if FROM='title,description'  \n\nCODE_BLOCK_7\n\nInsert empty vector (document excluded from vector search)\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### Searching with auto embeddings\n\n<!-- example embeddings_search -->\n\nSearch works the same way - provide your query text and Manticore will generate embeddings and find similar documents:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nUsing text query with auto-embeddings\n\nCODE_BLOCK_11\n\nUsing vector query directly\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.",
    "translations": {
      "chinese": "# K-近邻向量搜索\n\nManticore Search 支持将由机器学习模型生成的嵌入向量添加到每个文档中，然后对它们进行最近邻搜索。这使您能够构建诸如相似性搜索、推荐、语义搜索和基于自然语言处理算法的相关性排序等功能，还包括图像、视频和声音搜索。\n\n## 什么是嵌入向量？\n\n嵌入向量是一种表示数据（如文本、图像或声音）的方法，将其表示为高维空间中的向量。这些向量被设计成使它们之间的距离反映所代表数据的相似性。该过程通常采用诸如词嵌入（例如 Word2Vec、BERT）用于文本，或神经网络用于图像的算法。向量空间的高维特性，每个向量包含多个分量，允许表示项目之间复杂且细微的关系。它们的相似性通过这些向量之间的距离来衡量，通常使用欧几里得距离或余弦相似度等方法。\n\nManticore Search 使用 HNSW 库实现 k-近邻（KNN）向量搜索。此功能是 [Manticore Columnar Library](https://github.com/manticoresoftware/columnar) 的一部分。\n\n<!-- example KNN -->\n\n### 配置用于 KNN 搜索的表\n\n要运行 KNN 搜索，您必须先配置表。浮点向量和 KNN 搜索仅支持实时表（不支持普通表）。表需要至少有一个 [float_vector](../Creating_a_table/Data_types.md#Float-vector) 属性，作为数据向量。您需要指定以下属性：\n\n* `knn_type`：必填设置；目前仅支持 `hnsw`。\n\n* `knn_dims`：必填设置，指定被索引向量的维度。\n\n* `hnsw_similarity`：必填设置，指定 HNSW 索引使用的距离函数。可接受的值有：\n\n  - `L2` - 平方 L2 距离\n\n  - `IP` - 内积\n\n  - `COSINE` - 余弦相似度\n\n  \n\n  **注意：** 使用 `COSINE` 相似度时，向量在插入时会自动归一化。这意味着存储的向量值可能与原始输入值不同，因为它们会被转换为单位向量（数学长度/模为 1.0 的向量），以实现高效的余弦相似度计算。此归一化保持向量的方向，同时标准化其长度。\n\n* `hnsw_m`：可选设置，定义图中最大出边连接数。默认值为 16。\n\n* `hnsw_ef_construction`：可选设置，定义构建时的时间/准确性权衡。\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### 普通模式（使用配置文件）：\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### 插入向量数据\n\n#### 自动嵌入（推荐）\n\n处理向量数据最简单的方法是使用**自动嵌入**。使用此功能，您创建一个带有 `MODEL_NAME` 和 `FROM` 参数的表，然后只需插入文本数据——Manticore 会自动为您生成嵌入向量。\n\n##### 创建带自动嵌入的表\n\n创建自动嵌入表时，指定：\n\n- `MODEL_NAME`：要使用的嵌入模型\n\n- `FROM`：用于生成嵌入的字段（为空表示所有文本/字符串字段）\n\n**支持的嵌入模型：**\n\n- **Sentence Transformers**：任何[合适的基于 BERT 的 Hugging Face 模型](https://huggingface.co/sentence-transformers/models)（例如 `sentence-transformers/all-MiniLM-L6-v2`）——无需 API 密钥。Manticore 在创建表时下载模型。\n\n- **OpenAI**：OpenAI 嵌入模型，如 `openai/text-embedding-ada-002` - 需要 `API_KEY='<OPENAI_API_KEY>'` 参数\n\n- **Voyage**：Voyage AI 嵌入模型 - 需要 `API_KEY='<VOYAGE_API_KEY>'` 参数\n\n- **Jina**：Jina AI 嵌入模型 - 需要 `API_KEY='<JINA_API_KEY>'` 参数\n\n有关设置 `float_vector` 属性的更多信息，请参见[这里](../Creating_a_table/Data_types.md#Float-vector)。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n使用 sentence-transformers（无需 API 密钥）\n\nCODE_BLOCK_3\n\n使用 OpenAI（需要 API_KEY 参数）\n\nCODE_BLOCK_4\n\n使用所有文本字段生成嵌入（FROM 为空）\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### 使用自动嵌入插入数据\n\n<!-- example inserting_embeddings -->\n\n使用自动嵌入时，**不要在 INSERT 语句中指定向量字段**。嵌入向量会自动从 `FROM` 参数指定的文本字段生成。\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\n仅插入文本数据 - 嵌入自动生成\n\nCODE_BLOCK_6\n\n插入多个字段 - 如果 FROM='title,description'，则两者都用于生成嵌入\n\nCODE_BLOCK_7\n\n插入空向量（文档将被排除在向量搜索之外）\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### 使用自动嵌入搜索\n\n<!-- example embeddings_search -->\n\n搜索方式相同——提供查询文本，Manticore 会生成嵌入并找到相似文档：\n\n<!-- intro -->\n\n##### SQL：\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON：\n\n<!-- request JSON -->\n\n使用文本查询和自动嵌入\n\nCODE_BLOCK_11\n\n直接使用向量查询\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### 手动插入向量\n\n<!-- example manual_vector -->\n\n或者，您可以手动插入预先计算好的向量数据，确保其维度与创建表时指定的维度匹配。您也可以插入空向量；这意味着该文档将被排除在向量搜索结果之外。",
      "russian": "# Поиск векторов методом k-ближайших соседей\n\nManticore Search поддерживает возможность добавления эмбеддингов, сгенерированных моделями машинного обучения, к каждому документу, а затем выполнения поиска ближайших соседей по ним. Это позволяет создавать такие функции, как поиск по схожести, рекомендации, семантический поиск и ранжирование релевантности на основе алгоритмов обработки естественного языка (NLP), а также, среди прочего, поиск по изображениям, видео и звуку.\n\n## Что такое эмбеддинг?\n\nЭмбеддинг — это метод представления данных — таких как текст, изображения или звук — в виде векторов в пространстве высокой размерности. Эти векторы создаются так, чтобы расстояние между ними отражало схожесть представляемых данных. Этот процесс обычно использует алгоритмы, такие как векторные представления слов (например, Word2Vec, BERT) для текста или нейронные сети для изображений. Высокая размерность векторного пространства, с большим количеством компонентов на вектор, позволяет представлять сложные и тонкие взаимосвязи между объектами. Их схожесть оценивается по расстоянию между этими векторами, часто измеряемому с помощью методов, таких как евклидово расстояние или косинусное сходство.\n\nManticore Search позволяет выполнять поиск k-ближайших соседей (KNN) векторов с использованием библиотеки HNSW. Эта функциональность является частью [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Настройка таблицы для поиска KNN\n\nДля выполнения поиска KNN необходимо сначала настроить таблицу. Векторы с плавающей точкой и поиск KNN поддерживаются только в таблицах реального времени (не в обычных таблицах). Таблица должна содержать как минимум один атрибут [float_vector](../Creating_a_table/Data_types.md#Float-vector), который служит вектором данных. Необходимо указать следующие свойства:\n\n* `knn_type`: обязательный параметр; в настоящее время поддерживается только `hnsw`.\n\n* `knn_dims`: обязательный параметр, указывающий размерность индексируемых векторов.\n\n* `hnsw_similarity`: обязательный параметр, указывающий функцию расстояния, используемую индексом HNSW. Допустимые значения:\n\n  - `L2` - квадрат евклидова расстояния\n\n  - `IP` - внутреннее произведение\n\n  - `COSINE` - косинусное сходство\n\n  \n\n  **Примечание:** При использовании косинусного сходства (`COSINE`) векторы автоматически нормализуются при вставке. Это означает, что сохранённые значения векторов могут отличаться от исходных, так как они будут преобразованы в единичные векторы (векторы с математической длиной/модулем 1.0) для эффективного вычисления косинусного сходства. Такая нормализация сохраняет направление вектора при стандартизации его длины.\n\n* `hnsw_m`: необязательный параметр, определяющий максимальное количество исходящих связей в графе. По умолчанию 16.\n\n* `hnsw_ef_construction`: необязательный параметр, определяющий компромисс между временем построения и точностью.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Plain mode (использование конфигурационного файла):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Вставка векторных данных\n\n#### Автоматические эмбеддинги (рекомендуется)\n\nСамый простой способ работать с векторными данными — использовать **автоматические эмбеддинги**. С этой функцией вы создаёте таблицу с параметрами `MODEL_NAME` и `FROM`, а затем просто вставляете текстовые данные — Manticore автоматически генерирует эмбеддинги для вас.\n\n##### Создание таблицы с автоматическими эмбеддингами\n\nПри создании таблицы для автоматических эмбеддингов укажите:\n\n- `MODEL_NAME`: модель эмбеддинга для использования\n\n- `FROM`: какие поля использовать для генерации эмбеддингов (пустое значение означает все текстовые/строковые поля)\n\n**Поддерживаемые модели эмбеддингов:**\n\n- **Sentence Transformers**: Любая [подходящая модель на базе BERT из Hugging Face](https://huggingface.co/sentence-transformers/models) (например, `sentence-transformers/all-MiniLM-L6-v2`) — не требует API-ключа. Manticore загружает модель при создании таблицы.\n\n- **OpenAI**: Модели эмбеддингов OpenAI, такие как `openai/text-embedding-ada-002` — требует параметр `API_KEY='<OPENAI_API_KEY>'`\n\n- **Voyage**: Модели эмбеддингов Voyage AI — требует параметр `API_KEY='<VOYAGE_API_KEY>'`\n\n- **Jina**: Модели эмбеддингов Jina AI — требует параметр `API_KEY='<JINA_API_KEY>'`\n\nБолее подробная информация о настройке атрибута `float_vector` доступна [здесь](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nИспользование sentence-transformers (API-ключ не требуется)\n\nCODE_BLOCK_3\n\nИспользование OpenAI (требуется параметр API_KEY)\n\nCODE_BLOCK_4\n\nИспользование всех текстовых полей для эмбеддингов (параметр FROM пуст)\n\nCODE_BLOCK_5\n\n<!-- end -->\n\n##### Вставка данных с автоматическими эмбеддингами\n\n<!-- example inserting_embeddings -->\n\nПри использовании автоматических эмбеддингов **не указывайте поле вектора** в операторе INSERT. Эмбеддинги генерируются автоматически из текстовых полей, указанных в параметре `FROM`.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nВставка только текстовых данных — эмбеддинги генерируются автоматически\n\nCODE_BLOCK_6\n\nВставка нескольких полей — все используются для эмбеддинга, если FROM='title,description'  \n\nCODE_BLOCK_7\n\nВставка пустого вектора (документ исключается из поиска по векторам)\n\nCODE_BLOCK_8\n\n<!-- end -->\n\n##### Поиск с автоматическими эмбеддингами\n\n<!-- example embeddings_search -->\n\nПоиск работает так же — укажите текст запроса, и Manticore сгенерирует эмбеддинги и найдёт похожие документы:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_9\n\n<!-- response SQL -->\n\nCODE_BLOCK_10\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nИспользование текстового запроса с автоматическими эмбеддингами\n\nCODE_BLOCK_11\n\nИспользование векторного запроса напрямую\n\nCODE_BLOCK_12\n\n<!-- response JSON -->\n\nCODE_BLOCK_13\n\n<!-- end -->\n\n#### Ручная вставка векторов\n\n<!-- example manual_vector -->\n\nВ качестве альтернативы вы можете вручную вставлять заранее вычисленные векторные данные, убедившись, что они соответствуют размерности, указанной при создании таблицы. Также можно вставить пустой вектор; это означает, что документ будет исключён из результатов поиска по векторам."
    },
    "is_code_or_comment": false
  },
  "3499904d571b82d3031c8764d97a4443bad22f0fdc4b8eaf61ac49024a7538b5": {
    "original": "**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.\n\n* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Find similar docs by id\n\n> NOTE: Finding similar documents by id requires [Manticore Buddy](../Installation/Manticore_Buddy.md). If it doesn't work, make sure Buddy is installed.",
    "translations": {
      "russian": "**Важно:** При использовании `hnsw_similarity='cosine'` векторы автоматически нормализуются при вставке до единичных векторов (векторов с математической длиной/модулем 1.0). Эта нормализация сохраняет направление вектора при стандартизации его длины, что необходимо для эффективных вычислений косинусного сходства. Это означает, что сохранённые значения будут отличаться от ваших исходных значений.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### Поиск KNN векторов\n\nТеперь вы можете выполнять поиск KNN с помощью оператора `knn` в формате SQL или JSON. Оба интерфейса поддерживают одинаковые основные параметры, обеспечивая единообразный опыт независимо от выбранного формата:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: Имя атрибута с плавающей точкой, содержащего векторные данные.\n\n* `k`: Количество документов для возврата, ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Определяет количество документов, которые должен вернуть один индекс HNSW. Однако фактическое количество документов в итоговых результатах может варьироваться. Например, если система работает с таблицами в реальном времени, разбитыми на дисковые чанки, каждый чанк может вернуть `k` документов, что приведёт к общему числу, превышающему заданное `k` (так как суммарное количество будет `num_chunks * k`). С другой стороны, итоговое количество документов может быть меньше `k`, если после запроса `k` документов некоторые из них отфильтровываются по определённым атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс извлечения работает иначе, и поэтому параметр `k` не влияет на количество возвращаемых документов.\n\n* `query`: (Рекомендуемый параметр) Поисковый запрос, который может быть:\n\n  - Текстовой строкой: Автоматически преобразуется в эмбеддинги, если для поля настроены авто-эмбеддинги. Вернёт ошибку, если авто-эмбеддинги не настроены.\n\n  - Массивом векторов: Работает так же, как `query_vector`.\n\n* `query_vector`: (Устаревший параметр) Поисковый вектор в виде массива чисел. Поддерживается для обратной совместимости.\n\n  **Примечание:** Используйте либо `query`, либо `query_vector`, не оба одновременно.\n\n* `ef`: необязательный размер динамического списка, используемого во время поиска. Более высокий `ef` обеспечивает более точный, но более медленный поиск.\n\n* `rescore`: Включает повторный расчёт KNN (по умолчанию отключён). Установите `1` в SQL или `true` в JSON для включения повторного расчёта. После завершения поиска KNN с использованием квантизированных векторов (с возможным оверсемплингом) расстояния пересчитываются с использованием оригинальных (полной точности) векторов, и результаты пересортировываются для улучшения точности ранжирования.\n\n* `oversampling`: Устанавливает множитель (число с плавающей точкой), на который умножается `k` при выполнении поиска KNN, что приводит к выбору большего количества кандидатов, чем требуется, с использованием квантизированных векторов. По умолчанию оверсемплинг не применяется. Эти кандидаты могут быть повторно оценены, если включён повторный расчёт. Оверсемплинг также работает с неквантизированными векторами. Поскольку он увеличивает `k`, что влияет на работу индекса HNSW, это может вызвать небольшое изменение точности результатов.\n\nДокументы всегда сортируются по расстоянию до поискового вектора. Любые дополнительные критерии сортировки, которые вы укажете, применяются после этого основного условия сортировки. Для получения расстояния существует встроенная функция [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Квантование векторов\n\nИндексы HNSW должны быть полностью загружены в память для выполнения поиска KNN, что может привести к значительному потреблению памяти. Для уменьшения использования памяти можно применить скалярное квантование — метод сжатия высокоразмерных векторов путём представления каждого компонента (измерения) ограниченным числом дискретных значений. Manticore поддерживает 8-битное и 1-битное квантование, что означает, что каждый компонент вектора сжимается с 32-битного float до 8 бит или даже 1 бита, уменьшая использование памяти в 4 или 32 раза соответственно. Эти сжатые представления также позволяют быстрее вычислять расстояния, так как больше компонентов вектора можно обработать за одну SIMD-инструкцию. Хотя скалярное квантование вводит некоторую погрешность, это часто оправданный компромисс между точностью поиска и эффективностью ресурсов. Для ещё лучшей точности квантование можно сочетать с повторным расчётом и оверсемплингом: выбирается больше кандидатов, чем запрошено, и расстояния для этих кандидатов пересчитываются с использованием оригинальных 32-битных float-векторов.\n\nПоддерживаемые типы квантования:\n\n* `8bit`: Каждый компонент вектора квантован до 8 бит.\n\n* `1bit`: Каждый компонент вектора квантован до 1 бита. Используется асимметричное квантование: векторы запроса квантованы до 4 бит, а хранимые векторы — до 1 бита. Такой подход обеспечивает большую точность, чем более простые методы, хотя и с некоторыми потерями в производительности.\n\n* `1bitsimple`: Каждый компонент вектора квантован до 1 бита. Этот метод быстрее, чем `1bit`, но обычно менее точен.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Поиск похожих документов по id\n\n> ПРИМЕЧАНИЕ: Поиск похожих документов по id требует [Manticore Buddy](../Installation/Manticore_Buddy.md). Если не работает, убедитесь, что Buddy установлен.",
      "chinese": "**重要提示：** 当使用 `hnsw_similarity='cosine'` 时，向量在插入时会自动归一化为单位向量（数学长度/幅度为1.0的向量）。这种归一化保持了向量的方向，同时标准化了其长度，这是高效计算余弦相似度所必需的。这意味着存储的值将与您原始输入的值不同。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_14\n\n<!-- response SQL -->\n\nCODE_BLOCK_15\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_16\n\n<!-- response JSON -->\n\nCODE_BLOCK_17\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN 向量搜索\n\n现在，您可以使用 SQL 或 JSON 格式中的 `knn` 子句执行 KNN 搜索。两种接口都支持相同的基本参数，确保无论选择哪种格式，都能获得一致的体验：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性的名称。\n\n* `k`：表示返回的文档数量，是分层可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。然而，最终结果中包含的文档数量可能会有所不同。例如，如果系统处理的是分割成磁盘块的实时表，每个块可能返回 `k` 个文档，导致总数超过指定的 `k`（因为累计计数为 `num_chunks * k`）。另一方面，如果在请求了 `k` 个文档后，根据特定属性过滤掉了一些文档，最终文档数可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程不同，因此 `k` 参数对返回文档数量的影响不适用。\n\n* `query`：（推荐参数）搜索查询，可以是：\n\n  - 文本字符串：如果字段配置了自动嵌入，则自动转换为嵌入向量。如果字段没有自动嵌入，将返回错误。\n\n  - 向量数组：与 `query_vector` 功能相同。\n\n* `query_vector`：（遗留参数）作为数字数组的搜索向量。为向后兼容仍然支持。\n\n  **注意：** 在同一请求中使用 `query` 或 `query_vector` 中的一个，不要同时使用。\n\n* `ef`：搜索过程中使用的动态列表大小的可选参数。`ef` 越大，搜索越准确但越慢。\n\n* `rescore`：启用 KNN 重新评分（默认禁用）。在 SQL 中设置为 `1`，在 JSON 中设置为 `true` 以启用重新评分。KNN 搜索完成后，使用量化向量（可能带有过采样）进行距离计算，然后用原始（全精度）向量重新计算距离并重新排序结果，以提高排名准确性。\n\n* `oversampling`：设置一个因子（浮点值），在执行 KNN 搜索时将 `k` 乘以该因子，导致使用量化向量检索的候选项多于所需数量。默认不应用过采样。如果启用重新评分，这些候选项可以稍后重新评估。过采样也适用于非量化向量。由于它增加了 `k`，影响 HNSW 索引的工作方式，可能会导致结果准确性略有变化。\n\n文档始终按与搜索向量的距离排序。您指定的任何额外排序条件将在此主要排序条件之后应用。要获取距离，有一个内置函数叫做 [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29)。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_18\n\n<!-- response SQL -->\n\nCODE_BLOCK_19\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_20\n\n<!-- response JSON -->\n\nCODE_BLOCK_21\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### 向量量化\n\nHNSW 索引需要完全加载到内存中以执行 KNN 搜索，这可能导致显著的内存消耗。为了减少内存使用，可以应用标量量化——一种通过用有限数量的离散值表示每个分量（维度）来压缩高维向量的技术。Manticore 支持 8 位和 1 位量化，意味着每个向量分量从 32 位浮点压缩到 8 位甚至 1 位，分别减少了 4 倍或 32 倍的内存使用。这些压缩表示还允许更快的距离计算，因为可以在单个 SIMD 指令中处理更多的向量分量。虽然标量量化引入了一些近似误差，但通常是在搜索准确性和资源效率之间值得的权衡。为了获得更好的准确性，量化可以与重新评分和过采样结合使用：检索的候选项多于请求的数量，并使用原始 32 位浮点向量重新计算这些候选项的距离。\n\n支持的量化类型包括：\n\n* `8bit`：每个向量分量量化为 8 位。\n\n* `1bit`：每个向量分量量化为 1 位。使用非对称量化，查询向量量化为 4 位，存储向量量化为 1 位。这种方法比简单方法提供更高的精度，但性能有所折衷。\n\n* `1bitsimple`：每个向量分量量化为 1 位。此方法比 `1bit` 更快，但通常准确性较低。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_22\n\n<!-- response SQL -->\n\nCODE_BLOCK_23\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### 通过 id 查找相似文档\n\n> 注意：通过 id 查找相似文档需要 [Manticore Buddy](../Installation/Manticore_Buddy.md)。如果无法使用，请确保已安装 Buddy。"
    },
    "is_code_or_comment": false
  },
  "6a964a4264ca12821ffabae20a32dfdcf24b327cffc3b3d8f33b8213eabd1e4d": {
    "original": "* `1bitsimple`: Each vector component is quantized to 1 bit. This method is faster than `1bit`, but typically less accurate.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- response SQL -->\n\nCODE_BLOCK_24\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Find similar docs by id\n\n> NOTE: Finding similar documents by id requires [Manticore Buddy](../Installation/Manticore_Buddy.md). If it doesn't work, make sure Buddy is installed.\n\nFinding documents similar to a specific one based on its unique ID is a common task. For instance, when a user views a particular item, Manticore Search can efficiently identify and display a list of items that are most similar to it in the vector space. Here's how you can do it:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `document id`: Document ID for KNN similarity search.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_25\n\n<!-- response SQL -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_27\n\n<!-- response JSON -->\n\nCODE_BLOCK_28\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Filtering KNN vector search results\n\nManticore also supports additional filtering of documents returned by the KNN search, either by full-text matching, attribute filters, or both.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_29\n\n<!-- response SQL -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_31\n\n<!-- response JSON -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n<!-- proofread -->",
    "translations": {
      "chinese": "* `1bitsimple`：每个向量分量被量化为1位。该方法比 `1bit` 更快，但通常精度较低。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- response SQL -->\n\nCODE_BLOCK_24\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### 通过id查找相似文档\n\n> 注意：通过id查找相似文档需要 [Manticore Buddy](../Installation/Manticore_Buddy.md)。如果不起作用，请确保已安装Buddy。\n\n基于唯一ID查找与特定文档相似的文档是一个常见任务。例如，当用户查看某个特定条目时，Manticore Search可以高效地识别并显示在向量空间中与其最相似的条目列表。操作方法如下：\n\n- SQL：`select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON：\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性名称。\n\n* `k`：表示返回的文档数量，是分层可导航小世界图（HNSW）索引的重要参数。它指定单个HNSW索引应返回的文档数量。不过，最终结果中包含的文档实际数量可能有所不同。例如，若系统处理被划分为多个磁盘块的实时表，每个块可以返回 `k` 个文档，导致总数超过指定的 `k`（累计数为 `num_chunks * k`）。另一方面，若请求 `k` 个文档后基于特定属性过滤掉部分文档，最终文档数可能少于 `k`。需要注意的是，参数 `k` 不适用于ramchunks。在ramchunks的上下文中，检索过程不同，因此 `k` 参数对返回文档数的影响不适用。\n\n* `document id`：用于KNN相似性搜索的文档ID。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_25\n\n<!-- response SQL -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_27\n\n<!-- response JSON -->\n\nCODE_BLOCK_28\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### 筛选KNN向量搜索结果\n\nManticore还支持对KNN搜索返回的文档进行额外筛选，可通过全文匹配、属性过滤，或两者结合实现。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_29\n\n<!-- response SQL -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_31\n\n<!-- response JSON -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n<!-- proofread -->",
      "russian": "* `1bitsimple`: Каждый компонент вектора квантован до 1 бита. Этот метод быстрее, чем `1bit`, но обычно менее точен.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_23\n\n<!-- response SQL -->\n\nCODE_BLOCK_24\n\n<!-- end -->\n\n<!-- Example knn_similar_docs -->\n\n### Найти похожие документы по id\n\n> ПРИМЕЧАНИЕ: Поиск похожих документов по id требует [Manticore Buddy](../Installation/Manticore_Buddy.md). Если это не работает, убедитесь, что Buddy установлен.\n\nПоиск документов, похожих на конкретный, основанный на его уникальном ID, — распространённая задача. Например, когда пользователь просматривает определённый элемент, Manticore Search может эффективно определить и вывести список элементов, наиболее похожих на него в векторном пространстве. Вот как это сделать:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <document id> )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"doc_id\": <document id>,\n\n          \"k\": <k>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: это имя атрибута с плавающим вектором, содержащим векторные данные.\n\n* `k`: это количество документов, которые нужно вернуть, и ключевой параметр для иерархических навигируемых мирового малого мира (HNSW) индексов. Он определяет, сколько документов должен вернуть один индекс HNSW. Однако фактическое количество документов в итоговых результатах может отличаться. Например, если система работает с таблицами в реальном времени, разбитыми на дисковые чанкы, каждый из которых может вернуть `k` документов, итоговое число может превысить заданное `k` (так как сумма будет равна `num_chunks * k`). С другой стороны, окончательное количество документов может быть меньше `k`, если после запроса `k` документов некоторые отфильтровываются по определённым атрибутам. Важно отметить, что параметр `k` не применяется к ramchunks. В случае ramchunks процесс извлечения работает иначе, и воздействие параметра `k` на количество возвращаемых документов не применяется.\n\n* `document id`: ID документа для поиска по сходству KNN.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_25\n\n<!-- response SQL -->\n\nCODE_BLOCK_26\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_27\n\n<!-- response JSON -->\n\nCODE_BLOCK_28\n\n<!-- end -->\n\n<!-- Example knn_filtering -->\n\n### Фильтрация результатов поиска KNN вектора\n\nManticore также поддерживает дополнительную фильтрацию документов, возвращаемых по KNN-поиску, либо по полнотекстовому совпадению, фильтрам атрибутов, либо по обоим сразу.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_29\n\n<!-- response SQL -->\n\nCODE_BLOCK_30\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_31\n\n<!-- response JSON -->\n\nCODE_BLOCK_32\n\n<!-- end -->\n\n<!-- proofread -->"
    },
    "is_code_or_comment": false
  },
  "1fca19ec99317992ebc653389ecb2de6a3e03bc661486d4f6cfad467eab41f86": {
    "original": "# K-nearest neighbor vector search\n\nManticore Search supports the ability to add embeddings generated by Machine Learning models to each document, and then doing a nearest-neighbor search on them. This lets you build features like similarity search, recommendations, semantic search, and relevance ranking based on NLP algorithms, among others, including image, video, and sound searches.\n\n## What is an embedding?\n\nAn embedding is a method of representing data - such as text, images, or sound - as vectors in a high-dimensional space. These vectors are crafted to ensure that the distance between them reflects the similarity of the data they represent. This process typically employs algorithms like word embeddings (e.g., Word2Vec, BERT) for text or neural networks for images. The high-dimensional nature of the vector space, with many components per vector, allows for the representation of complex and nuanced relationships between items. Their similarity is gauged by the distance between these vectors, often measured using methods like Euclidean distance or cosine similarity.\n\nManticore Search enables k-nearest neighbor (KNN) vector searches using the HNSW library. This functionality is part of the [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Configuring a table for KNN search\n\nTo run KNN searches, you must first configure your table. Float vectors and KNN search are only supported in real-time tables (not in plain tables). The table needs to have at least one [float_vector](../Creating_a_table/Data_types.md#Float-vector) attribute, which serves as a data vector. You need to specify the following properties:\n\n* `knn_type`: A mandatory setting; currently, only `hnsw` is supported.\n\n* `knn_dims`: A mandatory setting that specifies the dimensions of the vectors being indexed.\n\n* `hnsw_similarity`: A mandatory setting that specifies the distance function used by the HNSW index. Acceptable values are:\n\n  - `L2` - Squared L2\n\n  - `IP` - Inner product\n\n  - `COSINE` - Cosine similarity\n\n  \n\n  **Note:** When using `COSINE` similarity, vectors are automatically normalized upon insertion. This means the stored vector values may differ from the original input values, as they will be converted to unit vectors (vectors with a mathematical length/magnitude of 1.0) to enable efficient cosine similarity calculations. This normalization preserves the direction of the vector while standardizing its length.\n\n* `hnsw_m`: An optional setting that defines the maximum number of outgoing connections in the graph. The default is 16.\n\n* `hnsw_ef_construction`: An optional setting that defines a construction time/accuracy trade-off.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Plain mode (using configuration file):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Inserting vector data\n\n#### Auto Embeddings (Recommended)\n\nThe easiest way to work with vector data is using **auto embeddings**. With this feature, you create a table with `MODEL_NAME` and `FROM` parameters, then simply insert your text data - Manticore automatically generates embeddings for you.\n\n##### Creating a table with auto embeddings\n\nWhen creating a table for auto embeddings, specify:\n\n- `MODEL_NAME`: The embedding model to use\n\n- `FROM`: Which fields to use for embedding generation (empty means all text/string fields)\n\n- `API_KEY`: Required for remote models (OpenAI, Voyage, Jina). The API key is validated during table creation by making a real API request.\n\n- `API_URL`: Optional. Custom API endpoint URL. If not specified, uses the default provider endpoint (e.g., `https://api.openai.com/v1/embeddings` for OpenAI).\n\n- `API_TIMEOUT`: Optional. HTTP timeout in seconds for API requests. Default is 10 seconds. Set to `'0'` to use the default timeout. Applies to both validation requests during table creation and embedding generation during INSERT operations.\n\n**Supported embedding models:**\n\n- **Sentence Transformers**: Any [suitable BERT-based Hugging Face model](https://huggingface.co/sentence-transformers/models) (e.g., `sentence-transformers/all-MiniLM-L6-v2`) — no API key needed. Manticore downloads the model when you create the table.\n\n- **OpenAI, Voyage, Jina**: Remote embedding models (e.g., `openai/text-embedding-ada-002`, `voyage/voyage-3.5-lite`, `jina/jina-embeddings-v2-base-en`) - require `API_KEY='<API_KEY>'` parameter. Optionally specify `API_URL='<CUSTOM_URL>'` to use a custom API endpoint, and `API_TIMEOUT='<SECONDS>'` to configure HTTP timeout (default is 10 seconds).\n\nMore information about setting up a `float_vector` attribute can be found [here](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nUsing sentence-transformers (no API key needed)\n\nCODE_BLOCK_3\n\nUsing OpenAI (requires API_KEY parameter)\n\nCODE_BLOCK_4\n\nUsing OpenAI with custom API URL and timeout (optional)\n\nCODE_BLOCK_5\n\nUsing all text fields for embeddings (FROM is empty)\n\nCODE_BLOCK_6\n\n<!-- end -->\n\n##### Inserting data with auto embeddings\n\n<!-- example inserting_embeddings -->\n\nWhen using auto embeddings, **do not specify the vector field** in your INSERT statement. The embeddings are generated automatically from the text fields specified in the `FROM` parameter.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nInsert text data only - embeddings generated automatically\n\nCODE_BLOCK_7\n\nInsert multiple fields - both used for embedding if FROM='title,description'  \n\nCODE_BLOCK_8\n\nInsert empty vector (document excluded from vector search)\n\nCODE_BLOCK_9\n\n<!-- end -->\n\n##### Searching with auto embeddings\n\n<!-- example embeddings_search -->\n\nSearch works the same way - provide your query text and Manticore will generate embeddings and find similar documents:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_10\n\n<!-- response SQL -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nUsing text query with auto-embeddings\n\nCODE_BLOCK_12\n\nUsing vector query directly",
    "translations": {
      "chinese": "# K-近邻向量搜索\n\nManticore Search 支持为每个文档添加由机器学习模型生成的嵌入向量，然后基于这些向量进行近邻搜索。这样可以构建类似相似度搜索、推荐、语义搜索以及基于自然语言处理算法的相关性排序等功能，还包括图像、视频和声音搜索。\n\n## 什么是嵌入向量？\n\n嵌入向量是一种表示数据（例如文本、图像或声音）的方法，将其表示为高维空间中的向量。这些向量被设计为使得它们之间的距离能够反映所代表数据的相似度。此过程通常使用诸如词嵌入（例如 Word2Vec、BERT）用于文本，或用于图像的神经网络等算法。向量空间的高维性质，每个向量包含多个分量，允许表示项目之间复杂且细微的关系。它们的相似度通过向量之间的距离来衡量，常用的度量方法包括欧氏距离或余弦相似度。\n\nManticore Search 使用 HNSW 库支持 k-近邻（KNN）向量搜索。该功能是 [Manticore 列存库](https://github.com/manticoresoftware/columnar) 的一部分。\n\n<!-- example KNN -->\n\n### 配置用于 KNN 搜索的表\n\n要进行 KNN 搜索，必须先配置表。浮点向量和 KNN 搜索仅支持实时表（不支持普通表）。表需要至少包含一个 [float_vector](../Creating_a_table/Data_types.md#Float-vector) 属性，作为数据向量。需要指定以下属性：\n\n* `knn_type`：必填设置；目前只支持 `hnsw`。\n\n* `knn_dims`：必填设置，指定被索引向量的维度。\n\n* `hnsw_similarity`：必填设置，指定 HNSW 索引所用的距离函数。可接受的值为：\n\n  - `L2` - 平方 L2\n\n  - `IP` - 内积\n\n  - `COSINE` - 余弦相似度\n\n  \n\n  **注意：** 使用 `COSINE` 相似度时，向量在插入时会自动归一化。这意味着存储的向量值可能与原始输入值不同，因为它们会被转换为单位向量（数学长度/幅度为 1.0 的向量），以便高效计算余弦相似度。归一化过程中向量的方向被保留，而长度被标准化。\n\n* `hnsw_m`：可选设置，定义图中最大出边连接数，默认值为 16。\n\n* `hnsw_ef_construction`：可选设置，定义构建时的准确度/时间权衡。\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### 纯模式（使用配置文件）：\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### 插入向量数据\n\n#### 自动嵌入（推荐）\n\n处理向量数据的最简单方式是使用**自动嵌入**功能。借助此功能，您只需创建带有 `MODEL_NAME` 和 `FROM` 参数的表，然后直接插入文本数据，Manticore 会自动为您生成嵌入。\n\n##### 创建带自动嵌入的表\n\n创建自动嵌入用表时，需要指定：\n\n- `MODEL_NAME`：使用的嵌入模型\n\n- `FROM`：用于生成嵌入的字段（为空则表示所有文本/字符串字段）\n\n- `API_KEY`：远程模型（OpenAI、Voyage、Jina）要求必填。通过发起实际 API 请求验证 API 密钥。\n\n- `API_URL`：可选。自定义 API 端点 URL。未指定则使用默认提供商端点（例如 OpenAI 使用 `https://api.openai.com/v1/embeddings`）。\n\n- `API_TIMEOUT`：可选。API 请求的 HTTP 超时时间（秒），默认为 10 秒。设置为 `'0'` 时使用默认超时。该配置同时适用于建表时的验证请求和插入操作期间的嵌入生成。\n\n**支持的嵌入模型：**\n\n- **Sentence Transformers**：任何[适合的基于 BERT 的 Hugging Face 模型](https://huggingface.co/sentence-transformers/models)（例如 `sentence-transformers/all-MiniLM-L6-v2`）——无需 API 密钥。Manticore 在建表时会下载模型。\n\n- **OpenAI、Voyage、Jina**：远程嵌入模型（例如 `openai/text-embedding-ada-002`，`voyage/voyage-3.5-lite`，`jina/jina-embeddings-v2-base-en`）——需要 `API_KEY='<API_KEY>'` 参数。可选指定 `API_URL='<CUSTOM_URL>'` 以使用自定义 API 端点，及 `API_TIMEOUT='<SECONDS>'` 以配置 HTTP 超时（默认 10 秒）。\n\n关于 float_vector 属性的更多设置细节，可参见 [这里](../Creating_a_table/Data_types.md#Float-vector)。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\n使用 sentence-transformers（无需 API Key）\n\nCODE_BLOCK_3\n\n使用 OpenAI（需 API_KEY 参数）\n\nCODE_BLOCK_4\n\n使用带自定义 API URL 和超时的 OpenAI（可选）\n\nCODE_BLOCK_5\n\n使用所有文本字段生成嵌入（FROM 为空）\n\nCODE_BLOCK_6\n\n<!-- end -->\n\n##### 使用自动嵌入插入数据\n\n<!-- example inserting_embeddings -->\n\n使用自动嵌入时，**插入语句中不需指定向量字段**。嵌入会自动从 `FROM` 参数指定的文本字段生成。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\n只插入文本数据 — 嵌入会自动生成\n\nCODE_BLOCK_7\n\n插入多个字段 — 如果 FROM='title,description'，这两个字段都会参与嵌入生成\n\nCODE_BLOCK_8\n\n插入空向量（文档将不参与向量搜索）\n\nCODE_BLOCK_9\n\n<!-- end -->\n\n##### 使用自动嵌入搜索\n\n<!-- example embeddings_search -->\n\n搜索方式相同——输入查询文本，Manticore 会自动生成嵌入并查找相似文档：\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_10\n\n<!-- response SQL -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\n使用文本查询的自动嵌入\n\nCODE_BLOCK_12\n\n使用直接向量查询",
      "russian": "# Поиск векторов методом k-ближайших соседей\n\nManticore Search поддерживает возможность добавлять эмбеддинги, создаваемые моделями машинного обучения, к каждому документу и затем выполнять поиск ближайших соседей по ним. Это позволяет создавать такие функции, как поиск по схожести, рекомендации, семантический поиск и ранжирование релевантности на основе алгоритмов обработки естественного языка, а также другие, включая поиск изображений, видео и звука.\n\n## Что такое эмбеддинг?\n\nЭмбеддинг — это способ представления данных, таких как текст, изображения или звук, в виде векторов в пространстве высокой размерности. Эти векторы создаются таким образом, чтобы расстояние между ними отражало схожесть представленных данных. Этот процесс обычно использует алгоритмы, например, векторные представления слов (Word2Vec, BERT) для текста или нейронные сети для изображений. Высокая размерность пространства векторов, с большим числом компонентов на один вектор, позволяет представлять сложные и тонкие взаимосвязи между элементами. Их схожесть измеряется расстоянием между векторами, часто с помощью таких метрик, как евклидово расстояние или косинусное сходство.\n\nManticore Search позволяет выполнять поиск k-ближайших соседей (KNN) с помощью библиотеки HNSW. Эта функциональность является частью [Manticore Columnar Library](https://github.com/manticoresoftware/columnar).\n\n<!-- example KNN -->\n\n### Конфигурация таблицы для поиска KNN\n\nДля выполнения поиска KNN необходимо сначала настроить таблицу. Векторы с плавающей точкой и поиск KNN поддерживаются только в таблицах реального времени (не в обычных таблицах). В таблице должен быть хотя бы один атрибут типа [float_vector](../Creating_a_table/Data_types.md#Float-vector), который служит вектором данных. Нужно указать следующие параметры:\n\n* `knn_type`: обязательный параметр; в настоящее время поддерживается только `hnsw`.\n\n* `knn_dims`: обязательный параметр, указывающий размерность индексируемых векторов.\n\n* `hnsw_similarity`: обязательный параметр, указывающий функцию расстояния, используемую индексом HNSW. Допустимые значения:\n\n  - `L2` - квадрат евклидова расстояния\n\n  - `IP` - скалярное произведение\n\n  - `COSINE` - косинусное сходство\n\n  \n\n  **Примечание:** При использовании косинусного сходства (`COSINE`) векторы автоматически нормализуются при вставке. Это означает, что сохраняемые значения могут отличаться от исходных, так как они преобразуются к единичным векторам (векторами с длиной 1.0) для эффективного вычисления косинусного сходства. Такая нормализация сохраняет направление вектора при стандартизации его длины.\n\n* `hnsw_m`: необязательный параметр, определяющий максимальное число исходящих связей в графе. Значение по умолчанию — 16.\n\n* `hnsw_ef_construction`: необязательный параметр, задающий компромисс между скоростью построения и точностью.\n\n<!-- intro -->\n\n##### SQL\n\n<!-- request SQL -->\n\nCODE_BLOCK_0\n\n<!-- response SQL -->\n\nCODE_BLOCK_1\n\n<!-- intro -->\n\n##### Режим Plain (используя конфигурационный файл):\n\n<!-- request Config -->\n\nCODE_BLOCK_2\n\n<!-- end -->\n\n<!-- example knn_insert -->\n\n### Вставка векторных данных\n\n#### Автоэмбеддинги (рекомендуется)\n\nСамый простой способ работы с векторными данными — использовать **автоэмбеддинги**. С этой функцией вы создаёте таблицу с параметрами `MODEL_NAME` и `FROM`, затем просто вставляете текстовые данные — Manticore автоматически сгенерирует эмбеддинги для вас.\n\n##### Создание таблицы с автоэмбеддингами\n\nПри создании таблицы для автоэмбеддингов укажите:\n\n- `MODEL_NAME`: Модель для создания эмбеддингов\n\n- `FROM`: Какие поля использовать для генерации эмбеддингов (пусто означает использовать все текстовые/строковые поля)\n\n- `API_KEY`: Обязательный параметр для удалённых моделей (OpenAI, Voyage, Jina). Ключ API проверяется при создании таблицы посредством реального запроса к API.\n\n- `API_URL`: Необязательный. Пользовательский URL точки доступа API. Если не указан, используется стандартный URL провайдера (например, `https://api.openai.com/v1/embeddings` для OpenAI).\n\n- `API_TIMEOUT`: Необязательный. Таймаут HTTP-запросов к API в секундах. Значение по умолчанию — 10 секунд. Установите `'0'` для использования таймаута по умолчанию. Применяется как к валидационным запросам при создании таблицы, так и к генерации эмбеддингов при операциях INSERT.\n\n**Поддерживаемые модели эмбеддингов:**\n\n- **Sentence Transformers**: Любая [соответствующая модель на базе BERT из Hugging Face](https://huggingface.co/sentence-transformers/models) (например, `sentence-transformers/all-MiniLM-L6-v2`) — API ключ не требуется. Manticore загружает модель при создании таблицы.\n\n- **OpenAI, Voyage, Jina**: Удалённые модели эмбеддингов (например, `openai/text-embedding-ada-002`, `voyage/voyage-3.5-lite`, `jina/jina-embeddings-v2-base-en`) — требуют параметр `API_KEY='<API_KEY>'`. Опционально можно указать `API_URL='<CUSTOM_URL>'` для пользовательской точки доступа API и `API_TIMEOUT='<SECONDS>'` для настройки таймаута HTTP (по умолчанию 10 секунд).\n\nПодробности о настройке атрибута `float_vector` доступны [здесь](../Creating_a_table/Data_types.md#Float-vector).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nИспользование sentence-transformers (без API ключа)\n\nCODE_BLOCK_3\n\nИспользование OpenAI (требуется параметр API_KEY)\n\nCODE_BLOCK_4\n\nИспользование OpenAI с пользовательским URL и таймаутом (опционально)\n\nCODE_BLOCK_5\n\nИспользование всех текстовых полей для генерации эмбеддингов (параметр FROM пуст)\n\nCODE_BLOCK_6\n\n<!-- end -->\n\n##### Вставка данных с автоэмбеддингами\n\n<!-- example inserting_embeddings -->\n\nПри использовании автоэмбеддингов **не указывайте векторное поле** в операторе INSERT. Эмбеддинги автоматически генерируются из текстовых полей, указанных в параметре `FROM`.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nВставка только текстовых данных — эмбеддинги сгенерированы автоматически\n\nCODE_BLOCK_7\n\nВставка нескольких полей — используются оба для генерации эмбеддингов, если FROM='title,description'  \n\nCODE_BLOCK_8\n\nВставка пустого вектора (документ исключён из векторного поиска)\n\nCODE_BLOCK_9\n\n<!-- end -->\n\n##### Поиск с автоэмбеддингами\n\n<!-- example embeddings_search -->\n\nПоиск работает аналогично — укажите текст вашего запроса, и Manticore сгенерирует эмбеддинги и найдёт похожие документы:\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_10\n\n<!-- response SQL -->\n\nCODE_BLOCK_11\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nИспользование текстового запроса с автоэмбеддингами\n\nCODE_BLOCK_12\n\nИспользование запроса с вектором напрямую"
    },
    "is_code_or_comment": false
  },
  "0707fe81365b9c080f74ed00a6120bf0a37a8a8a358efade125dd5c6b7015804": {
    "original": "CODE_BLOCK_13\n\n<!-- response JSON -->\n\nCODE_BLOCK_14\n\n<!-- end -->\n\n#### Manual Vector Insertion\n\n<!-- example manual_vector -->\n\nAlternatively, you can manually insert pre-computed vector data, ensuring it matches the dimensions you specified when creating the table. You can also insert an empty vector; this means that the document will be excluded from vector search results.\n\n**Important:** When using `hnsw_similarity='cosine'`, vectors are automatically normalized upon insertion to unit vectors (vectors with a mathematical length/magnitude of 1.0). This normalization preserves the direction of the vector while standardizing its length, which is required for efficient cosine similarity calculations. This means the stored values will differ from your original input values.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_15\n\n<!-- response SQL -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_17\n\n<!-- response JSON -->\n\nCODE_BLOCK_18\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN vector search\n\nNow, you can perform a KNN search using the `knn` clause in either SQL or JSON format. Both interfaces support the same essential parameters, ensuring a consistent experience regardless of the format you choose:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nThe parameters are:\n\n* `field`: This is the name of the float vector attribute containing vector data.\n\n* `k`: This represents the number of documents to return and is a key parameter for Hierarchical Navigable Small World (HNSW) indexes. It specifies the quantity of documents that a single HNSW index should return. However, the actual number of documents included in the final results may vary. For instance, if the system is dealing with real-time tables divided into disk chunks, each chunk could return `k` documents, leading to a total that exceeds the specified `k` (as the cumulative count would be `num_chunks * k`). On the other hand, the final document count might be less than `k` if, after requesting `k` documents, some are filtered out based on specific attributes. It's important to note that the parameter `k` does not apply to ramchunks. In the context of ramchunks, the retrieval process operates differently, and thus, the `k` parameter's effect on the number of documents returned is not applicable.\n\n* `query`: (Recommended parameter) The search query, which can be either:\n\n  - Text string: Automatically converted to embeddings if the field has auto-embeddings configured. Will return an error if the field doesn't have auto-embeddings.\n\n  - Vector array: Works the same as `query_vector`.\n\n* `query_vector`: (Legacy parameter) The search vector as an array of numbers. Still supported for backward compatibility.\n\n  **Note:** Use either `query` or `query_vector`, not both in the same request.\n\n* `ef`: optional size of the dynamic list used during the search. A higher `ef` leads to more accurate but slower search.\n\n* `rescore`: Enables KNN rescoring (disabled by default). Set to `1` in SQL or `true` in JSON to enable rescoring. After the KNN search is completed using quantized vectors (with possible oversampling), distances are recalculated with the original (full-precision) vectors and results are re-sorted to improve ranking accuracy.\n\n* `oversampling`: Sets a factor (float value) by which `k` is multiplied when executing the KNN search, causing more candidates to be retrieved than needed using quantized vectors. No oversampling is applied by default. These candidates can be re-evaluated later if rescoring is enabled. Oversampling also works with non-quantized vectors. Since it increases `k`, which affects how the HNSW index works, it may cause a small change in result accuracy.\n\nDocuments are always sorted by their distance to the search vector. Any additional sorting criteria you specify will be applied after this primary sort condition. For retrieving the distance, there is a built-in function called [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_19\n\n<!-- response SQL -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_21\n\n<!-- response JSON -->\n\nCODE_BLOCK_22\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Vector quantization\n\nHNSW indexes need to be fully loaded into memory to perform KNN search, which can lead to significant memory consumption. To reduce memory usage, scalar quantization can be applied - a technique that compresses high-dimensional vectors by representing each component (dimension) with a limited number of discrete values. Manticore supports 8-bit and 1-bit quantization, meaning each vector component is compressed from a 32-bit float to 8 bits or even 1 bit, reducing memory usage by 4x or 32x, respectively. These compressed representations also allow for faster distance calculations, as more vector components can be processed in a single SIMD instruction. Although scalar quantization introduces some approximation error, it is often a worthwhile trade-off between search accuracy and resource efficiency. For even better accuracy, quantization can be combined with rescoring and oversampling: more candidates are retrieved than requested, and distances for these candidates are recalculated using the original 32-bit float vectors.\n\nSupported quantization types include:\n\n* `8bit`: Each vector component is quantized to 8 bits.\n\n* `1bit`: Each vector component is quantized to 1 bit. Asymmetric quantization is used, with query vectors quantized to 4 bits and stored vectors to 1 bit. This approach offers greater precision than simpler methods, though with some performance trade-off.",
    "translations": {
      "chinese": "CODE_BLOCK_13\n\n<!-- response JSON -->\n\nCODE_BLOCK_14\n\n<!-- end -->\n\n#### 手动向量插入\n\n<!-- example manual_vector -->\n\n或者，您可以手动插入预先计算好的向量数据，确保其与您创建表时指定的维度相匹配。您也可以插入一个空向量；这意味着该文档将被排除在向量搜索结果之外。\n\n**重要提示：** 当使用 `hnsw_similarity='cosine'` 时，向量在插入时会自动被归一化为单位向量（数学长度/幅度为1.0的向量）。这种归一化保留了向量的方向，同时统一了其长度，这是高效余弦相似度计算所必需的。这意味着存储的值将与您原始输入的值有所不同。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_15\n\n<!-- response SQL -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_17\n\n<!-- response JSON -->\n\nCODE_BLOCK_18\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN 向量搜索\n\n现在，您可以使用 SQL 或 JSON 格式中的 `knn` 子句执行 KNN 搜索。两种接口支持相同的关键参数，确保无论选择哪种格式体验一致：\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\n参数说明：\n\n* `field`：这是包含向量数据的浮点向量属性名称。\n\n* `k`：表示要返回的文档数量，是分层可导航小世界（HNSW）索引的关键参数。它指定单个 HNSW 索引应返回的文档数量。但最终结果中包含的文档数可能有所不同。例如，如果系统处理分成多个磁盘块的实时表，每个块都可能返回 `k` 个文档，总数可能超过指定的 `k`（因为总数为 `num_chunks * k`）。另一方面，如果请求 `k` 个文档后基于特定属性过滤掉了一些文档，最终文档数可能少于 `k`。需要注意的是，参数 `k` 不适用于 ramchunks。在 ramchunks 的上下文中，检索过程不同，因此 `k` 参数对返回文档数的影响不适用。\n\n* `query`：（推荐参数）搜索查询，可以是：\n\n  - 文本字符串：如果字段配置了自动嵌入，则会自动转换为嵌入。若字段没有自动嵌入，则会报错。\n\n  - 向量数组：作用与 `query_vector` 相同。\n\n* `query_vector`：（遗留参数）作为数字数组的搜索向量。仍然支持以保持向后兼容。\n\n  **注意：** 请求中只能使用 `query` 或 `query_vector` 中的一个，不能同时使用。\n\n* `ef`：搜索时使用的动态列表大小（可选）。`ef`越大搜索越准确但越慢。\n\n* `rescore`：启用KNN重评分（默认禁用）。SQL中设置为 `1` 或 JSON 中设置为 `true` 以启用。KNN搜索使用量化后的向量完成（可能带有过采样）后，距离会用原始（全精度）向量重新计算，结果重新排序以提高排名准确度。\n\n* `oversampling`：设置乘以 `k` 的因子（浮点值），执行 KNN 搜索时检索比需求更多的候选项，基于量化向量的检索默认不使用过采样。启用重评分时，这些候选将被重新评估。过采样也适用于非量化向量。由于增加了 `k`，这会影响 HNSW 指数的工作，可能会导致结果准确度出现小幅变化。\n\n文档始终按照与搜索向量的距离排序。您指定的其他排序条件将应用于此主要排序条件之后。要检索距离，可以使用内置函数 [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29)。\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_19\n\n<!-- response SQL -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_21\n\n<!-- response JSON -->\n\nCODE_BLOCK_22\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### 向量量化\n\nHNSW 索引需要完全加载到内存中以执行 KNN 搜索，这可能导致大量内存消耗。为了减少内存使用，可以应用标量量化——一种通过将每个分量（维度）表示为有限个离散值来压缩高维向量的技术。Manticore 支持 8 位和 1 位量化，意味着每个向量分量从32位浮点数压缩到8位甚至1位，分别减少4倍或32倍内存使用。这些压缩表示还允许更快的距离计算，因为可以在单条 SIMD 指令中处理更多的向量分量。虽然标量量化引入了一些近似误差，但这通常是在搜索准确度和资源效率之间的可取权衡。为了获得更好的准确度，量化可以与重评分和过采样结合：检索的候选数超过请求数，并使用原始32位浮点向量重新计算这些候选的距离。\n\n支持的量化类型包括：\n\n* `8bit`：每个向量分量量化为8位。\n\n* `1bit`：每个向量分量量化为1位。采用非对称量化，查询向量量化为4位，存储向量量化为1位。这种方法比更简单的方式提供更高精度，但带来一定的性能折衷。",
      "russian": "CODE_BLOCK_13\n\n<!-- response JSON -->\n\nCODE_BLOCK_14\n\n<!-- end -->\n\n#### Ручная вставка векторов\n\n<!-- example manual_vector -->\n\nВ качестве альтернативы вы можете вручную вставлять заранее вычисленные данные векторов, убедившись, что они соответствуют размерности, которую вы указали при создании таблицы. Также можно вставить пустой вектор; это означает, что документ будет исключен из результатов поиска по векторам.\n\n**Важно:** При использовании `hnsw_similarity='cosine'` векторы автоматически нормализуются при вставке до единичных векторов (векторов с математической длиной/модулем 1.0). Эта нормализация сохраняет направление вектора, одновременно стандартизируя его длину, что требуется для эффективных вычислений косинусного сходства. Это означает, что сохранённые значения будут отличаться от ваших исходных данных.\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_15\n\n<!-- response SQL -->\n\nCODE_BLOCK_16\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_17\n\n<!-- response JSON -->\n\nCODE_BLOCK_18\n\n<!-- end -->\n\n<!-- example knn_search -->\n\n### KNN поиск по векторам\n\nТеперь вы можете выполнять поиск KNN с использованием параметра `knn` в формате SQL или JSON. Обе эти формы поддерживают одинаковые основные параметры, обеспечивая единый опыт работы независимо от выбранного формата:\n\n- SQL: `select ... from <table name> where knn ( <field>, <k>, <query vector> [,<options>] )`\n\n- JSON:\n\n  ```\n\n  POST /search\n\n  {\n\n      \"table\": \"<table name>\",\n\n      \"knn\":\n\n      {\n\n          \"field\": \"<field>\",\n\n          \"query\": \"<text or vector>\",\n\n          \"k\": <k>,\n\n          \"ef\": <ef>,\n\n\t\t  \"rescore\": <rescore>,\n\n\t\t  \"oversampling\": <oversampling>\n\n      }\n\n  }\n\n  ```\n\nПараметры:\n\n* `field`: имя атрибута типа float vector, содержащего векторные данные.\n\n* `k`: количество документов для возврата; ключевой параметр для индексов Hierarchical Navigable Small World (HNSW). Он определяет, сколько документов должен вернуть один HNSW-индекс. Однако фактическое количество документов в итоговых результатах может варьироваться. Например, если система работает с таблицами реального времени, разбитыми на дисковые чанки, каждый чанк может вернуть `k` документов, что приведёт к суммарному количеству, превышающему заданное `k` (так как итоговый подсчёт будет `num_chunks * k`). С другой стороны, итоговое число документов может быть меньше `k`, если после запроса `k` документов некоторые из них фильтруются по определённым атрибутам. Важно учесть, что параметр `k` не применяется к ramchunks. В контексте ramchunks процесс выборки работает иначе, поэтому эффект параметра `k` на число возвращаемых документов не актуален.\n\n* `query`: (рекомендуемый параметр) Поисковый запрос, который может быть:\n\n  - Текстовой строкой: автоматически конвертируется в эмбеддинги, если для поля настроены авто-эмбеддинги. Если авто-эмбеддинги не настроены, выдаётся ошибка.\n\n  - Массивом векторов: работает так же, как `query_vector`.\n\n* `query_vector`: (устаревший параметр) Поисковый вектор в виде массива чисел. Поддерживается для обратной совместимости.\n\n  **Примечание:** Используйте либо `query`, либо `query_vector`, но не оба параметра в одном запросе.\n\n* `ef`: дополнительный размер динамического списка, используемого во время поиска. Более высокое значение `ef` даёт более точный, но более медленный поиск.\n\n* `rescore`: включает пересчёт результатов KNN (по умолчанию отключён). Установите `1` в SQL или `true` в JSON, чтобы включить пересчёт. После выполнения KNN-поиска с квантованными векторами (с возможным сверхвыбором) расстояния пересчитываются с использованием исходных (полноточных) векторов, и результаты пересортировываются для повышения точности ранжирования.\n\n* `oversampling`: задаёт множитель (число с плавающей точкой), на который умножается `k` при выполнении KNN-поиска, что приводит к извлечению большего количества кандидатов, чем необходимо, используя квантованные вектора. По умолчанию сверхвыбор не применяется. Эти кандидаты могут быть переоценены позже, если включён пересчёт (rescore). Сверхвыбор также работает с неквантованными векторами. Поскольку он увеличивает `k`, что влияет на работу индекса HNSW, это может вызвать небольшое изменение точности результатов.\n\nДокументы всегда сортируются по их расстоянию от поискового вектора. Любые дополнительные критерии сортировки, которые вы укажете, будут применены после этой основной сортировки. Для получения расстояния существует встроенная функция [knn_dist()](../Functions/Other_functions.md#KNN_DIST%28%29).\n\n<!-- intro -->\n\n##### SQL:\n\n<!-- request SQL -->\n\nCODE_BLOCK_19\n\n<!-- response SQL -->\n\nCODE_BLOCK_20\n\n<!-- intro -->\n\n##### JSON:\n\n<!-- request JSON -->\n\nCODE_BLOCK_21\n\n<!-- response JSON -->\n\nCODE_BLOCK_22\n\n<!-- end -->\n\n<!-- example knn_quantization -->\n\n### Векторная квантование\n\nИндексы HNSW должны быть полностью загружены в память для выполнения KNN-поиска, что может вести к значительному потреблению памяти. Чтобы сократить использование памяти, можно применить скалярное квантование — метод сжатия векторов высокой размерности путём представления каждого компонента (измерения) ограниченным набором дискретных значений. Manticore поддерживает 8-битное и 1-битное квантование, то есть каждый компонент вектора сжимается с 32-битного float до 8 бит или даже 1 бита, соответственно уменьшая использование памяти в 4 или 32 раза. Такие сжатые представления также позволяют быстрее вычислять расстояния, так как большее число компонентов векторов может быть обработано в одной SIMD-инструкции. Хотя скалярное квантование вводит некоторую ошибку аппроксимации, оно часто является разумным компромиссом между точностью поиска и эффективным использованием ресурсов. Для ещё большей точности квантование можно сочетать с пересчётом результатов и сверхвыбором: извлекается больше кандидатов, чем запрошено, и для них расстояния пересчитываются с использованием исходных 32-битных float-векторов.\n\nПоддерживаемые типы квантования включают:\n\n* `8bit`: каждый компонент вектора квантуется до 8 бит.\n\n* `1bit`: каждый компонент вектора квантуется до 1 бита. Используется асимметричное квантование: запросные векторы квантуются до 4 бит, а сохраняемые — до 1 бита. Такой подход даёт большую точность по сравнению с более простыми методами, но с некоторой потерей производительности."
    },
    "is_code_or_comment": false
  }
}
