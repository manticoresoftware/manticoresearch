# Восстановление кластера

В случае если демон поиска Manticore останавливается, и в кластере не остается узлов для обслуживания запросов, необходима процедура восстановления. Благодаря мульти-мастерной природе библиотеки Galera, используемой для репликации, кластер репликации Manticore представляет собой единый логический объект, который поддерживает согласованность своих узлов и данных, а также статус всего кластера. Это позволяет безопасно выполнять записи на нескольких узлах одновременно и обеспечивает целостность кластера.

Однако это также создает определённые сложности. Рассмотрим несколько сценариев, используя кластер из узлов A, B и C, чтобы понять, что необходимо сделать, если некоторые или все узлы становятся недоступными.

### Случай 1

Когда узел A остановлен, остальные узлы получают сообщение о "нормальном завершении работы". Размер кластера уменьшается, и происходит перерасчет кворума.

При запуске узел A присоединяется к кластеру и не будет обслуживать транзакции записи до полной синхронизации с кластером. Если в кеше записей (writeset cache) на донорских узлах B или C (который можно контролировать с помощью параметра [gcache.size](https://galeracluster.com/library/documentation/galera-parameters.html#gcache-size) Galera кластера) по-прежнему содержатся все транзакции, пропущенные узлом A, узел A получит быстрый инкрементальный перенос состояния ([IST](https://galeracluster.com/library/documentation/state-transfer.html#state-transfer-ist)), то есть перенос только пропущенных транзакций. В противном случае произойдет передача состояния с полной копией ([SST](https://galeracluster.com/library/documentation/state-transfer.html#state-transfer-sst)), включающая передачу файлов таблиц.

### Случай 2

В ситуации, когда узлы A и B остановлены, размер кластера уменьшается до одного узла, узел C формирует основную компоненту для обработки транзакций записи.

Узлы A и B затем могут быть запущены как обычно и присоединятся к кластеру после запуска. Узел C выступает донором, предоставляя перенос состояния узлам A и B.

### Случай 3

Все узлы остановлены как обычно, и кластер выключен.

Проблема теперь заключается в том, как инициализировать кластер. Важно, чтобы при корректном завершении работы searchd узлы записывали номер последней выполненной транзакции в файл [grastate.dat](../../Creating_a_cluster/Setting_up_replication/Restarting_a_cluster.md) в каталоге кластера вместе с флагом `safe_to_bootstrap`. Узел, остановленный последним, будет иметь опцию `safe_to_bootstrap: 1` и наиболее продвинутый номер `seqno`.

Важно, чтобы этот узел запускался первым для формирования кластера. Для запуска кластера сервер должен стартовать на этом узле с ключом [--new-cluster](../../Creating_a_cluster/Setting_up_replication/Restarting_a_cluster.md). В Linux можно также использовать `manticore_new_cluster`, который запустит Manticore в режиме `--new-cluster` через systemd.

Если другой узел запускается первым и инициирует кластер, тогда наиболее продвинутый узел присоединится к этому кластеру, выполнит полную SST и получит файл таблицы, в котором некоторые транзакции отсутствуют по сравнению с файлами таблиц, которые были у него ранее. Поэтому важно запускать первым тот узел, который был выключен последним — у него должен быть флаг `safe_to_bootstrap: 1` в файле [grastate.dat](../../Creating_a_cluster/Setting_up_replication/Restarting_a_cluster.md).

### Случай 4

В случае аварийного сбоя или сетевого сбоя, из-за которого узел A пропадает из кластера, узлы B и C попытаются переподключиться к узлу A. При неудаче они удалят узел A из кластера. Поскольку два из трех узлов продолжают работать, кластер сохраняет кворум и продолжает функционировать нормально.

При повторном запуске узел A автоматически присоединится к кластеру, как описано в [Случае 1](../../Creating_a_cluster/Setting_up_replication/Cluster_recovery.md#Case-1).

### Случай 5

Узлы A и B отключены. Узел C не может сформировать кворум самостоятельно, так как один узел — это менее половины от общего количества узлов (3). В результате кластер на узле C переходит в состояние non-primary и отклоняет любые транзакции записи с сообщением об ошибке.

Тем временем узел C ожидает подключения других узлов и также пытается с ними соединиться. Если это происходит и сеть восстанавливается, а узлы A и B возвращаются в онлайн, кластер автоматически восстановится. Если узлы A и B временно отключены от узла C, но могут общаться между собой, они продолжат работать нормально, поскольку по-прежнему формируют кворум.

<!-- example case 5 -->
Однако, если оба узла A и B аварийно завершились или перезапустились из-за отключения питания, кто-то должен активировать основную компоненту на узле C, используя следующую команду:

<!-- intro -->
##### SQL:

<!-- request SQL -->

```sql
SET CLUSTER posts GLOBAL 'pc.bootstrap' = 1
```
<!-- request JSON -->

```json
POST /cli -d "
SET CLUSTER posts GLOBAL 'pc.bootstrap' = 1
"
```
<!-- end -->

Важно отметить, что перед выполнением этой команды необходимо убедиться, что остальные узлы действительно недоступны. В противном случае может возникнуть ситуация разделенного мозга (split-brain) и сформируются отдельные кластеры.

### Случай 6

Все узлы аварийно завершились. В этой ситуации файл [grastate.dat](../../Creating_a_cluster/Setting_up_replication/Restarting_a_cluster.md) в каталоге кластера не был обновлен и не содержит действительного номера последовательности `seqno`.

Если это произошло, необходимо найти узел с самыми свежими данными и запустить сервер на нем с использованием ключа командной строки [--new-cluster-force](../../Creating_a_cluster/Setting_up_replication/Restarting_a_cluster.md). Все остальные узлы будут запускаться как обычно, как описано в [Случае 3](../../Creating_a_cluster/Setting_up_replication/Cluster_recovery.md#Case-3)).
В Linux можно также использовать команду `manticore_new_cluster --force`, которая запустит Manticore в режиме `--new-cluster-force` через systemd.

### Случай 7

Split-brain может привести к переходу кластера в непервичное состояние. Например, рассмотрим кластер, состоящий из четного количества узлов (четыре), таких как две пары узлов, расположенных в разных дата-центрах. Если сетевая ошибка прерывает соединение между дата-центрами, возникает split-brain, поскольку каждая группа узлов удерживает ровно половину кворума. В результате обе группы прекращают обработку транзакций на запись, так как модель репликации Galera придает приоритет согласованности данных, и кластер не может принимать транзакции на запись без кворума. Тем не менее, узлы в обеих группах пытаются переподключиться к узлам из другой группы в попытке восстановить кластер.

<!-- example case 7 -->
Если кто-то хочет восстановить кластер до восстановления сети, следует выполнить те же шаги, описанные в [Случае 5](../../Creating_a_cluster/Setting_up_replication/Cluster_recovery.md#Case-5), но только для одной группы узлов.

После выполнения оператора группа с узлом, на котором он был запущен, снова сможет обрабатывать транзакции на запись.


<!-- intro -->
##### SQL:

<!-- request SQL -->

```sql
SET CLUSTER posts GLOBAL 'pc.bootstrap' = 1
```
<!-- request JSON -->

```json
POST /cli -d "
SET CLUSTER posts GLOBAL 'pc.bootstrap' = 1
"
```
<!-- end -->

Однако важно отметить, что если оператор будет выполнен в обеих группах, это приведет к формированию двух отдельных кластеров, и последующее восстановление сети не приведет к объединению групп.
<!-- proofread -->

