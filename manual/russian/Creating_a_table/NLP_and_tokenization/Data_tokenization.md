# Токенизация данных

Manticore не хранит текст как есть для выполнения полнотекстового поиска по нему. Вместо этого он извлекает слова и создает несколько структур, которые позволяют выполнять быстрый полнотекстовый поиск. Из найденных слов строится словарь, который позволяет быстро проверить наличие слова в индексе. Кроме того, другие структуры записывают документы и поля, в которых было найдено слово (а также позицию этого слова внутри поля). Все это используется при выполнении полнотекстового поиска.

Процесс разделения и классификации слов называется токенизацией. Токенизация применяется как при индексировании, так и при поиске, и работает на уровне символов и слов.

На уровне символов движок пропускает только определенные символы. Это определяется с помощью [charset_table](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#charset_table). Все остальное заменяется пробелом (который считается стандартным разделителем слов). charset_table также позволяет задавать замены, например преобразование к нижнему регистру или простую замену одного символа на другой. Помимо этого, символы могут быть [игнорированы](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#ignore_chars), [объединены](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#blend_chars), определены как [граница фразы](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#phrase_boundary).

На уровне слов базовой настройкой является [min_word_len](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#min_word_len), которая задает минимальную длину слова в символах, принимаемую в индекс. Часто требуется сопоставлять единственное и множественное число слов. Для этого могут использоваться [морфологические процессоры](../../Creating_a_table/NLP_and_tokenization/Morphology.md#morphology).

Далее, мы можем захотеть, чтобы слово сопоставлялось с другим, поскольку они являются синонимами. Для этого используется функция [word forms](../../Creating_a_table/NLP_and_tokenization/Wordforms.md), которая позволяет сопоставлять одно или несколько слов с другим словом.

Очень распространенные слова могут оказывать нежелательное влияние на поиск, в основном из-за их частоты они требуют много вычислительных ресурсов для обработки списков документов/хитов. Они могут быть внесены в черный список с помощью функциональности [стоп-слов](../../Creating_a_table/NLP_and_tokenization/Ignoring_stop-words.md#stopwords). Это помогает не только ускорить запросы, но и уменьшить размер индекса.

Более продвинутый способ черного списка – [биграммы](../../Creating_a_table/NLP_and_tokenization/Low-level_tokenization.md#bigram_index), который позволяет создавать специальный токен между «биграммным» (частым) словом и редким словом. Это может значительно ускорить поиск, когда в поисковых фразах используются распространенные слова.

В случае индексирования HTML-контента важно не индексировать HTML-теги, так как они могут внести много "шума" в индекс. Для этого может использоваться [удаление HTML-тегов](../../Creating_a_table/NLP_and_tokenization/Advanced_HTML_tokenization.md#Stripping-HTML-tags), которое можно настроить так, чтобы удалять, но индексировать определенные атрибуты тегов или полностью игнорировать содержимое определенных HTML-элементов.
<!-- proofread -->

